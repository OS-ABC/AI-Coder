{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import codecs\n",
    "import random\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU设置\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95 #占用95%显存\n",
    "session = tf.Session(config=config)\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(input_data,min_word_freq):\n",
    "    counter = collections.Counter()\n",
    "    with codecs.open(input_data,\"r\",\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = ' '.join(re.split(' |\\t|\\v|\\n',line))        #将数据中的空格符统一，便于后期处理(原始数据中空格符包含\\t、\\v等)   \n",
    "            line = re.split('([: ,.(){}\\[\\]=])',line)        #将字符串数据按照括号中的符号进行分割，分割成列表格式，并且在列表中保留分隔符\n",
    "            line = list(filter(lambda x: x!=' 'and x!='',line))\n",
    "            for word in line:\n",
    "                counter[word] += 1\n",
    "                \n",
    "        counter = filter(lambda x: x[1] > min_word_freq, counter.items())\n",
    "        sorted_word_to_cnt = sorted(counter,key=itemgetter(1),reverse=True)\n",
    "        sorted_words = [x[0] for x in sorted_word_to_cnt]\n",
    "\n",
    "    sorted_words = [\"<unk>\",\"<start>\",\"<end>\"] + sorted_words\n",
    "\n",
    "    print(\"vocab_len: \" + str(len(sorted_words)))\n",
    "\n",
    "    return sorted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_len: 11928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['in_channels',\n",
       " 'zeros',\n",
       " 'class',\n",
       " 'plt',\n",
       " 'session',\n",
       " 'variable_scope',\n",
       " 'join',\n",
       " 'size',\n",
       " 'Session',\n",
       " 'zip']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = \"../00-data/tf_data.txt\"\n",
    "min_word_freq = 0\n",
    "vocab = get_vocab(input_data,min_word_freq)\n",
    "vocab[70:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {word:index for index,word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict[\"Session\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【随机】前k句预测下一句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_split(line):\n",
    "    line = ' '.join(re.split(' |\\t|\\v|\\n',line))        \n",
    "    line = re.split('([: ,.(){}\\[\\]=])',line)        \n",
    "    line = list(filter(lambda x: x!=' 'and x!='',line))\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_newdata_by_random(raw_data,vocab,enc_vec_data,enc_text_data,dec_vec_data,dec_text_data,rand_max=10,duplicate=2):\n",
    "    import codecs\n",
    "    import sys\n",
    "    import re\n",
    "\n",
    "    word_to_id = {k:v for(k,v) in zip(vocab,range(len(vocab)))}\n",
    "\n",
    "    def get_id(word):\n",
    "        return word_to_id[word] if word in word_to_id else word_to_id[\"<unk>\"]\n",
    "\n",
    "    fout_vec_enc = codecs.open(enc_vec_data,\"w\",\"utf-8\")\n",
    "    fout_vec_dec = codecs.open(dec_vec_data,\"w\",\"utf-8\")\n",
    "    fout_text_enc = codecs.open(enc_text_data,\"w\",\"utf-8\")\n",
    "    fout_text_dec = codecs.open(dec_text_data,\"w\",\"utf-8\")\n",
    "    \n",
    "    data_length = 0\n",
    "    with open(raw_data,\"r\") as fin:\n",
    "        lines = fin.readlines()\n",
    "        for i in range(rand_max,len(lines)):\n",
    "            rand_nums = set(random.randint(1,rand_max) for _ in range(duplicate))\n",
    "            for rand_num in rand_nums:\n",
    "                #构造enc_data\n",
    "                words = []\n",
    "                for j in range(i - rand_num,i):\n",
    "                    line = clean_and_split(lines[j])\n",
    "                    words += [\"<start>\"] + line + [\"<end>\"]\n",
    "                out_line = ' '.join([str(get_id(w)) for w in words]) + '\\n'\n",
    "                fout_text_enc.write(' '.join(words) + '\\n')\n",
    "                fout_vec_enc.write(out_line)\n",
    "\n",
    "                #构造dec_data\n",
    "                words = []\n",
    "                line = clean_and_split(lines[i])\n",
    "                words = line + [\"<end>\"]\n",
    "                out_line = ' '.join([str(get_id(w)) for w in words]) + '\\n'\n",
    "                fout_text_dec.write(' '.join(words) + '\\n')\n",
    "                fout_vec_dec.write(out_line)\n",
    "            \n",
    "                data_length += 1\n",
    "    fout_vec_enc.close()\n",
    "    fout_vec_dec.close()\n",
    "    fout_text_enc.close()\n",
    "    fout_text_dec.close()\n",
    "    \n",
    "    return data_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vec_data = \"../00-data/train.enc\"\n",
    "dec_vec_data = \"../00-data/train.dec\"\n",
    "enc_text_data = \"../00-data/train_text.enc\"\n",
    "dec_text_data = \"../00-data/train_text.dec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_len: 74052\n"
     ]
    }
   ],
   "source": [
    "data_length = get_newdata_by_random(input_data,vocab,enc_vec_data,enc_text_data,dec_vec_data,dec_text_data)\n",
    "print(\"data_len: \" + str(data_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原始数据向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeDataset(file_path):\n",
    "    dataset = tf.data.TextLineDataset(file_path)\n",
    "    dataset = dataset.map(lambda string:tf.string_split([string]).values)\n",
    "    dataset = dataset.map(\n",
    "            lambda string: tf.string_to_number(string,tf.int32))\n",
    "    dataset = dataset.map(lambda x:(x,tf.size(x)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeSrcTrgDataset(src_path,trg_path,batch_size,shuffle_size=3000,start_id=1):\n",
    "    src_data = MakeDataset(src_path)\n",
    "    trg_data = MakeDataset(trg_path)\n",
    "    \n",
    "    dataset = tf.data.Dataset.zip((src_data,trg_data))\n",
    "    \n",
    "    #删除内容为空or长度过长的句子\n",
    "    #不需要执行\n",
    "    def FilterLength(src_tuple,trg_tuple):\n",
    "        ((src_input,src_len),(trg_label,trg_len)) = (src_tuple,trg_tuple)\n",
    "        src_len_ok = tf.logical_and(\n",
    "            tf.greater(src_len,1),tf.less_equal(src_len,MAX_LEN))\n",
    "        trg_len_ok = tf.logical_and(\n",
    "            tf.greater(trg_len,1),tf.less_equal(trg_len,MAX_LEN))\n",
    "        return tf.logical_and(src_len_ok,trg_len_ok)\n",
    "    #dataset = dataset.filter(FilterLength)\n",
    "    \n",
    "    #生成<start> X Y Z 作为解码器的输入\n",
    "    def MakeTrgInput(src_tuple,trg_tuple):\n",
    "        ((src_input,src_len),(trg_label,trg_len)) = (src_tuple,trg_tuple)\n",
    "        trg_input = tf.concat([[start_id],trg_label[:-1]],axis=0)\n",
    "        return ((src_input,src_len),(trg_input,trg_label,trg_len))\n",
    "    dataset = dataset.map(MakeTrgInput)\n",
    "    \n",
    "    #随机打乱训练数据\n",
    "    dataset = dataset.shuffle(shuffle_size)\n",
    "    \n",
    "    #规定填充后输出的数据维度\n",
    "    padded_shapes = (\n",
    "        (tf.TensorShape([None]),\n",
    "         tf.TensorShape([])),\n",
    "        (tf.TensorShape([None]),\n",
    "         tf.TensorShape([None]),\n",
    "         tf.TensorShape([])))\n",
    "    #调用padded_batch方法进行batching操作\n",
    "    batched_dataset = dataset.padded_batch(batch_size,padded_shapes)\n",
    "    return  batched_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq + attention模型\n",
    "encoder:2层单向lstm\n",
    "decoder:1层单向lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"../02-checkpoints/\"\n",
    "HIDDEN_SIZE = 256\n",
    "ENCODER_LAYERS = 2\n",
    "DECODER_LAYERS = 1\n",
    "SRC_VOCAB_SIZE = len(vocab)\n",
    "TRG_VOCAB_SIZE = len(vocab)\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCH = 10\n",
    "KEEP_PROB = 0.9\n",
    "MAX_GRAD_NORM = 5\n",
    "LEARNING_RATE_BASE = 1.0\n",
    "LEARNING_RATE_DECAY = 0.7\n",
    "SHARE_EMB_AND_SOFTMAX = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义NMTModel类来描述模型。\n",
    "class NMTModel(object):\n",
    "    # 在模型的初始化函数中定义模型要用到的变量。\n",
    "    def __init__(self):\n",
    "        # 定义编码器和解码器所使用的LSTM结构。\n",
    "        self.enc_cell_fw = tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE)\n",
    "        self.enc_cell_bw = tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE)\n",
    "        \n",
    "        self.enc_cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "          [tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE) \n",
    "           for _ in range(ENCODER_LAYERS)])\n",
    "        \n",
    "        self.dec_cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "          [tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE) \n",
    "           for _ in range(DECODER_LAYERS)])\n",
    "\n",
    "        # 为源语言和目标语言分别定义词向量。   \n",
    "        self.src_embedding = tf.get_variable(\n",
    "            \"src_emb\", [SRC_VOCAB_SIZE, HIDDEN_SIZE])\n",
    "        self.trg_embedding = tf.get_variable(\n",
    "            \"trg_emb\", [TRG_VOCAB_SIZE, HIDDEN_SIZE])\n",
    "\n",
    "        # 定义softmax层的变量\n",
    "        if SHARE_EMB_AND_SOFTMAX:\n",
    "            self.softmax_weight = tf.transpose(self.trg_embedding)\n",
    "        else:\n",
    "            self.softmax_weight = tf.get_variable(\n",
    "               \"weight\", [HIDDEN_SIZE, TRG_VOCAB_SIZE])\n",
    "        self.softmax_bias = tf.get_variable(\n",
    "            \"softmax_bias\", [TRG_VOCAB_SIZE])\n",
    "\n",
    "    # 在forward函数中定义模型的前向计算图。\n",
    "    # src_input, src_size, trg_input, trg_label, trg_size分别是上面\n",
    "    # MakeSrcTrgDataset函数产生的五种张量。\n",
    "    def forward(self, src_input, src_size, trg_input, trg_label, trg_size,data_length):\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        batch_size = tf.shape(src_input)[0]\n",
    "    \n",
    "        # 将输入和输出单词编号转为词向量。\n",
    "        src_emb = tf.nn.embedding_lookup(self.src_embedding, src_input)\n",
    "        trg_emb = tf.nn.embedding_lookup(self.trg_embedding, trg_input)\n",
    "        \n",
    "        # 在词向量上进行dropout。\n",
    "        src_emb = tf.nn.dropout(src_emb, KEEP_PROB)\n",
    "        trg_emb = tf.nn.dropout(trg_emb, KEEP_PROB)\n",
    "\n",
    "        # 使用dynamic_rnn构造编码器。\n",
    "        # 编码器读取源句子每个位置的词向量，输出最后一步的隐藏状态enc_state。\n",
    "        # 因为编码器是一个双层LSTM，因此enc_state是一个包含两个LSTMStateTuple类\n",
    "        # 张量的tuple，每个LSTMStateTuple对应编码器中的一层。\n",
    "        # 张量的维度是 [batch_size, HIDDEN_SIZE]。\n",
    "        # enc_outputs是顶层LSTM在每一步的输出，它的维度是[batch_size, \n",
    "        # max_time, HIDDEN_SIZE]。Seq2Seq模型中不需要用到enc_outputs，而\n",
    "        # 后面介绍的attention模型会用到它。\n",
    "        # 下面的代码取代了Seq2Seq样例代码中forward函数里的相应部分。\n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "            # 构造编码器时，使用dynamic_rnn构造单向循环网络。\n",
    "            # 单向循环网络的顶层输出enc_outputs是一个包含两个张量的tuple，每个张量的\n",
    "            # 维度都是[batch_size, max_time, HIDDEN_SIZE]，代表两个LSTM在每一步的输出。\n",
    "            enc_outputs,enc_state = tf.nn.dynamic_rnn(\n",
    "                self.enc_cell,src_emb,src_size,dtype=tf.float32)    \n",
    "\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            # 选择注意力权重的计算模型。BahdanauAttention是使用一个隐藏层的前馈神经网络。\n",
    "            # memory_sequence_length是一个维度为[batch_size]的张量，代表batch\n",
    "            # 中每个句子的长度，Attention需要根据这个信息把填充位置的注意力权重设置为0。\n",
    "            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                HIDDEN_SIZE, enc_outputs,\n",
    "                memory_sequence_length=src_size)\n",
    "\n",
    "            # 将解码器的循环神经网络self.dec_cell和注意力一起封装成更高层的循环神经网络。\n",
    "            attention_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                self.dec_cell, attention_mechanism,\n",
    "                attention_layer_size=HIDDEN_SIZE)\n",
    "\n",
    "            # 使用attention_cell和dynamic_rnn构造编码器。\n",
    "            # 这里没有指定init_state，也就是没有使用编码器的输出来初始化输入，而完全依赖\n",
    "            # 注意力作为信息来源。\n",
    "            dec_outputs, _ = tf.nn.dynamic_rnn(\n",
    "                attention_cell, trg_emb, trg_size, dtype=tf.float32)\n",
    "\n",
    "        # 计算解码器每一步的log perplexity。这一步与语言模型代码相同。\n",
    "        output = tf.reshape(dec_outputs, [-1, HIDDEN_SIZE])\n",
    "        logits = tf.matmul(output, self.softmax_weight) + self.softmax_bias\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=tf.reshape(trg_label, [-1]), logits=logits)\n",
    "\n",
    "        # 在计算平均损失时，需要将填充位置的权重设置为0，以避免无效位置的预测干扰\n",
    "        # 模型的训练。\n",
    "        label_weights = tf.sequence_mask(\n",
    "            trg_size, maxlen=tf.shape(trg_label)[1], dtype=tf.float32)\n",
    "        label_weights = tf.reshape(label_weights, [-1])\n",
    "        cost = tf.reduce_sum(loss * label_weights)\n",
    "        cost_op = cost / tf.reduce_sum(label_weights)\n",
    "        \n",
    "        # 定义反向传播操作。反向操作的实现与语言模型代码相同。\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "\n",
    "        # 控制梯度大小，定义优化方法和训练步骤。\n",
    "        grads = tf.gradients(cost / tf.to_float(batch_size),\n",
    "                             trainable_variables)\n",
    "        grads, _ = tf.clip_by_global_norm(grads, MAX_GRAD_NORM)\n",
    "        \n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            LEARNING_RATE_BASE,\n",
    "            global_step,\n",
    "            data_length / batch_size, \n",
    "            LEARNING_RATE_DECAY,\n",
    "            staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_variables))\n",
    "        return cost_op, train_op,learning_rate,global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(session,cost_op,train_op,learning_rate_op,global_step_op,step,saver,epoch):\n",
    "    while True:\n",
    "        try:\n",
    "            cost,_,learning_rate,global_step = session.run([cost_op,train_op,learning_rate_op,global_step_op])\n",
    "            if global_step % 50 == 0:\n",
    "                print(\"After %d global_steps,per token cost is %.3f,learning_rate is %.5f\" %(global_step,cost,learning_rate))\n",
    "            session.run(tf.assign(global_step_op,step))\n",
    "            step += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            if epoch % 2 == 0:\n",
    "                saver.save(session,CHECKPOINT_PATH,global_step=global_step)\n",
    "            break\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-8ad012290bcb>:6: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-12-8ad012290bcb>:11: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /root/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-12-8ad012290bcb>:44: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-12-8ad012290bcb>:61: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /root/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-12-8ad012290bcb>:100: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /root/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ../02-checkpoints/02-0318/-4635\n",
      "In EPOCH: 1\n",
      "After 50 global_steps,per token cost is 2.616,learning_rate is 1.00000\n",
      "After 100 global_steps,per token cost is 2.445,learning_rate is 1.00000\n",
      "After 150 global_steps,per token cost is 1.941,learning_rate is 1.00000\n",
      "After 200 global_steps,per token cost is 1.367,learning_rate is 1.00000\n",
      "After 250 global_steps,per token cost is 1.796,learning_rate is 1.00000\n",
      "After 300 global_steps,per token cost is 2.015,learning_rate is 1.00000\n",
      "After 350 global_steps,per token cost is 2.002,learning_rate is 1.00000\n",
      "After 400 global_steps,per token cost is 1.729,learning_rate is 1.00000\n",
      "After 450 global_steps,per token cost is 1.414,learning_rate is 1.00000\n",
      "After 500 global_steps,per token cost is 1.769,learning_rate is 1.00000\n",
      "After 550 global_steps,per token cost is 1.915,learning_rate is 1.00000\n",
      "After 600 global_steps,per token cost is 2.256,learning_rate is 1.00000\n",
      "After 650 global_steps,per token cost is 2.122,learning_rate is 1.00000\n",
      "After 700 global_steps,per token cost is 2.302,learning_rate is 1.00000\n",
      "After 750 global_steps,per token cost is 2.152,learning_rate is 1.00000\n",
      "After 800 global_steps,per token cost is 1.761,learning_rate is 1.00000\n",
      "After 850 global_steps,per token cost is 1.629,learning_rate is 1.00000\n",
      "After 900 global_steps,per token cost is 1.359,learning_rate is 1.00000\n",
      "After 950 global_steps,per token cost is 1.678,learning_rate is 1.00000\n",
      "After 1000 global_steps,per token cost is 2.382,learning_rate is 1.00000\n",
      "After 1050 global_steps,per token cost is 1.874,learning_rate is 1.00000\n",
      "After 1100 global_steps,per token cost is 1.742,learning_rate is 1.00000\n",
      "After 1150 global_steps,per token cost is 1.647,learning_rate is 1.00000\n",
      "In EPOCH: 2\n",
      "After 1200 global_steps,per token cost is 1.876,learning_rate is 0.70000\n",
      "After 1250 global_steps,per token cost is 1.677,learning_rate is 0.70000\n",
      "After 1300 global_steps,per token cost is 1.445,learning_rate is 0.70000\n",
      "After 1350 global_steps,per token cost is 0.987,learning_rate is 0.70000\n",
      "After 1400 global_steps,per token cost is 1.265,learning_rate is 0.70000\n",
      "After 1450 global_steps,per token cost is 1.468,learning_rate is 0.70000\n",
      "After 1500 global_steps,per token cost is 1.557,learning_rate is 0.70000\n",
      "After 1550 global_steps,per token cost is 1.438,learning_rate is 0.70000\n",
      "After 1600 global_steps,per token cost is 1.204,learning_rate is 0.70000\n",
      "After 1650 global_steps,per token cost is 1.480,learning_rate is 0.70000\n",
      "After 1700 global_steps,per token cost is 1.611,learning_rate is 0.70000\n",
      "After 1750 global_steps,per token cost is 1.511,learning_rate is 0.70000\n",
      "After 1800 global_steps,per token cost is 1.696,learning_rate is 0.70000\n",
      "After 1850 global_steps,per token cost is 1.664,learning_rate is 0.70000\n",
      "After 1900 global_steps,per token cost is 1.624,learning_rate is 0.70000\n",
      "After 1950 global_steps,per token cost is 1.385,learning_rate is 0.70000\n",
      "After 2000 global_steps,per token cost is 1.141,learning_rate is 0.70000\n",
      "After 2050 global_steps,per token cost is 0.869,learning_rate is 0.70000\n",
      "After 2100 global_steps,per token cost is 1.091,learning_rate is 0.70000\n",
      "After 2150 global_steps,per token cost is 1.086,learning_rate is 0.70000\n",
      "After 2200 global_steps,per token cost is 1.226,learning_rate is 0.70000\n",
      "After 2250 global_steps,per token cost is 1.461,learning_rate is 0.70000\n",
      "After 2300 global_steps,per token cost is 1.344,learning_rate is 0.70000\n",
      "In EPOCH: 3\n",
      "After 2350 global_steps,per token cost is 1.346,learning_rate is 0.49000\n",
      "After 2400 global_steps,per token cost is 1.525,learning_rate is 0.49000\n",
      "After 2450 global_steps,per token cost is 1.384,learning_rate is 0.49000\n",
      "After 2500 global_steps,per token cost is 0.892,learning_rate is 0.49000\n",
      "After 2550 global_steps,per token cost is 0.815,learning_rate is 0.49000\n",
      "After 2600 global_steps,per token cost is 1.259,learning_rate is 0.49000\n",
      "After 2650 global_steps,per token cost is 1.180,learning_rate is 0.49000\n",
      "After 2700 global_steps,per token cost is 1.178,learning_rate is 0.49000\n",
      "After 2750 global_steps,per token cost is 1.122,learning_rate is 0.49000\n",
      "After 2800 global_steps,per token cost is 1.074,learning_rate is 0.49000\n",
      "After 2850 global_steps,per token cost is 1.415,learning_rate is 0.49000\n",
      "After 2900 global_steps,per token cost is 1.409,learning_rate is 0.49000\n",
      "After 2950 global_steps,per token cost is 1.326,learning_rate is 0.49000\n",
      "After 3000 global_steps,per token cost is 1.502,learning_rate is 0.49000\n",
      "After 3050 global_steps,per token cost is 1.608,learning_rate is 0.49000\n",
      "After 3100 global_steps,per token cost is 1.347,learning_rate is 0.49000\n",
      "After 3150 global_steps,per token cost is 1.076,learning_rate is 0.49000\n",
      "After 3200 global_steps,per token cost is 0.790,learning_rate is 0.49000\n",
      "After 3250 global_steps,per token cost is 0.996,learning_rate is 0.49000\n",
      "After 3300 global_steps,per token cost is 1.269,learning_rate is 0.49000\n",
      "After 3350 global_steps,per token cost is 1.299,learning_rate is 0.49000\n",
      "After 3400 global_steps,per token cost is 1.397,learning_rate is 0.49000\n",
      "After 3450 global_steps,per token cost is 1.084,learning_rate is 0.49000\n",
      "In EPOCH: 4\n",
      "After 3500 global_steps,per token cost is 1.322,learning_rate is 0.34300\n",
      "After 3550 global_steps,per token cost is 1.174,learning_rate is 0.34300\n",
      "After 3600 global_steps,per token cost is 1.244,learning_rate is 0.34300\n",
      "After 3650 global_steps,per token cost is 0.905,learning_rate is 0.34300\n",
      "After 3700 global_steps,per token cost is 0.956,learning_rate is 0.34300\n",
      "After 3750 global_steps,per token cost is 1.027,learning_rate is 0.34300\n",
      "After 3800 global_steps,per token cost is 1.224,learning_rate is 0.34300\n",
      "After 3850 global_steps,per token cost is 1.405,learning_rate is 0.34300\n",
      "After 3900 global_steps,per token cost is 0.747,learning_rate is 0.34300\n",
      "After 3950 global_steps,per token cost is 0.739,learning_rate is 0.34300\n",
      "After 4000 global_steps,per token cost is 1.245,learning_rate is 0.34300\n",
      "After 4050 global_steps,per token cost is 1.008,learning_rate is 0.34300\n",
      "After 4100 global_steps,per token cost is 1.243,learning_rate is 0.34300\n",
      "After 4150 global_steps,per token cost is 1.303,learning_rate is 0.34300\n",
      "After 4200 global_steps,per token cost is 1.521,learning_rate is 0.34300\n",
      "After 4250 global_steps,per token cost is 1.167,learning_rate is 0.34300\n",
      "After 4300 global_steps,per token cost is 1.118,learning_rate is 0.34300\n",
      "After 4350 global_steps,per token cost is 0.657,learning_rate is 0.34300\n",
      "After 4400 global_steps,per token cost is 0.544,learning_rate is 0.34300\n",
      "After 4450 global_steps,per token cost is 0.950,learning_rate is 0.34300\n",
      "After 4500 global_steps,per token cost is 1.181,learning_rate is 0.34300\n",
      "After 4550 global_steps,per token cost is 1.271,learning_rate is 0.34300\n",
      "After 4600 global_steps,per token cost is 0.746,learning_rate is 0.34300\n",
      "In EPOCH: 5\n",
      "After 4650 global_steps,per token cost is 1.303,learning_rate is 0.24010\n",
      "After 4700 global_steps,per token cost is 1.117,learning_rate is 0.24010\n",
      "After 4750 global_steps,per token cost is 1.203,learning_rate is 0.24010\n",
      "After 4800 global_steps,per token cost is 0.825,learning_rate is 0.24010\n",
      "After 4850 global_steps,per token cost is 0.638,learning_rate is 0.24010\n",
      "After 4900 global_steps,per token cost is 0.877,learning_rate is 0.24010\n",
      "After 4950 global_steps,per token cost is 1.058,learning_rate is 0.24010\n",
      "After 5000 global_steps,per token cost is 0.943,learning_rate is 0.24010\n",
      "After 5050 global_steps,per token cost is 0.828,learning_rate is 0.24010\n",
      "After 5100 global_steps,per token cost is 0.860,learning_rate is 0.24010\n",
      "After 5150 global_steps,per token cost is 0.915,learning_rate is 0.24010\n",
      "After 5200 global_steps,per token cost is 1.021,learning_rate is 0.24010\n",
      "After 5250 global_steps,per token cost is 1.344,learning_rate is 0.24010\n",
      "After 5300 global_steps,per token cost is 1.096,learning_rate is 0.24010\n",
      "After 5350 global_steps,per token cost is 1.425,learning_rate is 0.24010\n",
      "After 5400 global_steps,per token cost is 0.972,learning_rate is 0.24010\n",
      "After 5450 global_steps,per token cost is 1.101,learning_rate is 0.24010\n",
      "After 5500 global_steps,per token cost is 0.593,learning_rate is 0.24010\n",
      "After 5550 global_steps,per token cost is 0.569,learning_rate is 0.24010\n",
      "After 5600 global_steps,per token cost is 0.745,learning_rate is 0.24010\n",
      "After 5650 global_steps,per token cost is 0.944,learning_rate is 0.24010\n",
      "After 5700 global_steps,per token cost is 1.116,learning_rate is 0.24010\n",
      "After 5750 global_steps,per token cost is 0.812,learning_rate is 0.24010\n",
      "In EPOCH: 6\n",
      "After 5800 global_steps,per token cost is 1.252,learning_rate is 0.16807\n",
      "After 5850 global_steps,per token cost is 0.931,learning_rate is 0.16807\n",
      "After 5900 global_steps,per token cost is 1.007,learning_rate is 0.16807\n",
      "After 5950 global_steps,per token cost is 1.001,learning_rate is 0.16807\n",
      "After 6000 global_steps,per token cost is 0.770,learning_rate is 0.16807\n",
      "After 6050 global_steps,per token cost is 0.859,learning_rate is 0.16807\n",
      "After 6100 global_steps,per token cost is 1.054,learning_rate is 0.16807\n",
      "After 6150 global_steps,per token cost is 1.127,learning_rate is 0.16807\n",
      "After 6200 global_steps,per token cost is 0.683,learning_rate is 0.16807\n",
      "After 6250 global_steps,per token cost is 0.933,learning_rate is 0.16807\n",
      "After 6300 global_steps,per token cost is 0.823,learning_rate is 0.16807\n",
      "After 6350 global_steps,per token cost is 0.927,learning_rate is 0.16807\n",
      "After 6400 global_steps,per token cost is 1.110,learning_rate is 0.16807\n",
      "After 6450 global_steps,per token cost is 0.911,learning_rate is 0.16807\n",
      "After 6500 global_steps,per token cost is 1.243,learning_rate is 0.16807\n",
      "After 6550 global_steps,per token cost is 0.856,learning_rate is 0.16807\n",
      "After 6600 global_steps,per token cost is 0.755,learning_rate is 0.16807\n",
      "After 6650 global_steps,per token cost is 0.660,learning_rate is 0.16807\n",
      "After 6700 global_steps,per token cost is 0.566,learning_rate is 0.16807\n",
      "After 6750 global_steps,per token cost is 0.749,learning_rate is 0.16807\n",
      "After 6800 global_steps,per token cost is 0.912,learning_rate is 0.16807\n",
      "After 6850 global_steps,per token cost is 0.848,learning_rate is 0.16807\n",
      "After 6900 global_steps,per token cost is 0.909,learning_rate is 0.16807\n",
      "In EPOCH: 7\n",
      "After 6950 global_steps,per token cost is 1.221,learning_rate is 0.11765\n",
      "After 7000 global_steps,per token cost is 0.989,learning_rate is 0.11765\n",
      "After 7050 global_steps,per token cost is 1.083,learning_rate is 0.11765\n",
      "After 7100 global_steps,per token cost is 0.834,learning_rate is 0.11765\n",
      "After 7150 global_steps,per token cost is 0.461,learning_rate is 0.11765\n",
      "After 7200 global_steps,per token cost is 0.692,learning_rate is 0.11765\n",
      "After 7250 global_steps,per token cost is 0.884,learning_rate is 0.11765\n",
      "After 7300 global_steps,per token cost is 0.838,learning_rate is 0.11765\n",
      "After 7350 global_steps,per token cost is 0.584,learning_rate is 0.11765\n",
      "After 7400 global_steps,per token cost is 0.714,learning_rate is 0.11765\n",
      "After 7450 global_steps,per token cost is 0.888,learning_rate is 0.11765\n",
      "After 7500 global_steps,per token cost is 0.859,learning_rate is 0.11765\n",
      "After 7550 global_steps,per token cost is 1.101,learning_rate is 0.11765\n",
      "After 7600 global_steps,per token cost is 0.969,learning_rate is 0.11765\n",
      "After 7650 global_steps,per token cost is 1.108,learning_rate is 0.11765\n",
      "After 7700 global_steps,per token cost is 0.907,learning_rate is 0.11765\n",
      "After 7750 global_steps,per token cost is 0.851,learning_rate is 0.11765\n",
      "After 7800 global_steps,per token cost is 0.567,learning_rate is 0.11765\n",
      "After 7850 global_steps,per token cost is 0.395,learning_rate is 0.11765\n",
      "After 7900 global_steps,per token cost is 0.666,learning_rate is 0.11765\n",
      "After 7950 global_steps,per token cost is 0.874,learning_rate is 0.11765\n",
      "After 8000 global_steps,per token cost is 0.945,learning_rate is 0.11765\n",
      "After 8050 global_steps,per token cost is 0.636,learning_rate is 0.11765\n",
      "After 8100 global_steps,per token cost is 0.598,learning_rate is 0.08235\n",
      "In EPOCH: 8\n",
      "After 8150 global_steps,per token cost is 1.055,learning_rate is 0.08235\n",
      "After 8200 global_steps,per token cost is 0.970,learning_rate is 0.08235\n",
      "After 8250 global_steps,per token cost is 0.936,learning_rate is 0.08235\n",
      "After 8300 global_steps,per token cost is 0.468,learning_rate is 0.08235\n",
      "After 8350 global_steps,per token cost is 0.610,learning_rate is 0.08235\n",
      "After 8400 global_steps,per token cost is 0.903,learning_rate is 0.08235\n",
      "After 8450 global_steps,per token cost is 0.718,learning_rate is 0.08235\n",
      "After 8500 global_steps,per token cost is 0.577,learning_rate is 0.08235\n",
      "After 8550 global_steps,per token cost is 0.633,learning_rate is 0.08235\n",
      "After 8600 global_steps,per token cost is 0.596,learning_rate is 0.08235\n",
      "After 8650 global_steps,per token cost is 0.829,learning_rate is 0.08235\n",
      "After 8700 global_steps,per token cost is 0.812,learning_rate is 0.08235\n",
      "After 8750 global_steps,per token cost is 0.826,learning_rate is 0.08235\n",
      "After 8800 global_steps,per token cost is 1.072,learning_rate is 0.08235\n",
      "After 8850 global_steps,per token cost is 0.958,learning_rate is 0.08235\n",
      "After 8900 global_steps,per token cost is 0.766,learning_rate is 0.08235\n",
      "After 8950 global_steps,per token cost is 0.510,learning_rate is 0.08235\n",
      "After 9000 global_steps,per token cost is 0.433,learning_rate is 0.08235\n",
      "After 9050 global_steps,per token cost is 0.442,learning_rate is 0.08235\n",
      "After 9100 global_steps,per token cost is 0.723,learning_rate is 0.08235\n",
      "After 9150 global_steps,per token cost is 0.803,learning_rate is 0.08235\n",
      "After 9200 global_steps,per token cost is 0.767,learning_rate is 0.08235\n",
      "After 9250 global_steps,per token cost is 0.646,learning_rate is 0.08235\n",
      "In EPOCH: 9\n",
      "After 9300 global_steps,per token cost is 0.939,learning_rate is 0.05765\n",
      "After 9350 global_steps,per token cost is 0.802,learning_rate is 0.05765\n",
      "After 9400 global_steps,per token cost is 0.766,learning_rate is 0.05765\n",
      "After 9450 global_steps,per token cost is 0.540,learning_rate is 0.05765\n",
      "After 9500 global_steps,per token cost is 0.510,learning_rate is 0.05765\n",
      "After 9550 global_steps,per token cost is 0.829,learning_rate is 0.05765\n",
      "After 9600 global_steps,per token cost is 0.697,learning_rate is 0.05765\n",
      "After 9650 global_steps,per token cost is 0.651,learning_rate is 0.05765\n",
      "After 9700 global_steps,per token cost is 0.564,learning_rate is 0.05765\n",
      "After 9750 global_steps,per token cost is 0.703,learning_rate is 0.05765\n",
      "After 9800 global_steps,per token cost is 0.867,learning_rate is 0.05765\n",
      "After 9850 global_steps,per token cost is 0.830,learning_rate is 0.05765\n",
      "After 9900 global_steps,per token cost is 0.817,learning_rate is 0.05765\n",
      "After 9950 global_steps,per token cost is 0.729,learning_rate is 0.05765\n",
      "After 10000 global_steps,per token cost is 0.932,learning_rate is 0.05765\n",
      "After 10050 global_steps,per token cost is 0.769,learning_rate is 0.05765\n",
      "After 10100 global_steps,per token cost is 0.541,learning_rate is 0.05765\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    tf.reset_default_graph()\n",
    "    initializer = tf.random_uniform_initializer(-0.05,0.05)\n",
    "    with tf.variable_scope(\"nmt_model\",reuse=None,initializer=initializer):\n",
    "        train_model = NMTModel()\n",
    "    \n",
    "    data = MakeSrcTrgDataset(enc_vec_data,dec_vec_data,BATCH_SIZE)\n",
    "    iterator = data.make_initializable_iterator()\n",
    "    (src,src_size),(trg_input,trg_label,trg_size) = iterator.get_next()\n",
    "    \n",
    "    cost_op,train_op,learning_rate_op,global_step_op = train_model.forward(src,src_size,trg_input,trg_label,trg_size,data_length)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    step = 1\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        ckpt = tf.train.get_checkpoint_state(CHECKPOINT_PATH) #获取checkpoints对象  \n",
    "        if ckpt and ckpt.model_checkpoint_path:##判断ckpt是否为空，若不为空，才进行模型的加载，否则从头开始训练  \n",
    "            saver.restore(sess,ckpt.model_checkpoint_path)#恢复保存的神经网络结构，实现断点续训 \n",
    "        for i in range(NUM_EPOCH):\n",
    "            print(\"In EPOCH: %d\" %(i + 1))\n",
    "            sess.run(iterator.initializer)\n",
    "            step = run_epoch(sess,cost_op,train_op,learning_rate_op,global_step_op,step,saver,i + 1)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
