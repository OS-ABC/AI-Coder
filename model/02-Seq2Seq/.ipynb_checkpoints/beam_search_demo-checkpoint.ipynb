{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "associate-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import codecs\n",
    "import collections\n",
    "import random\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "outside-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU设置\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95 #占用95%显存\n",
    "session = tf.Session(config=config)\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-myanmar",
   "metadata": {},
   "source": [
    "## 数据构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "operating-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(input_data,min_word_freq):\n",
    "    counter = collections.Counter()\n",
    "    with codecs.open(input_data,\"r\",\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = ' '.join(re.split(' |\\t|\\v|\\n',line))        #将数据中的空格符统一，便于后期处理(原始数据中空格符包含\\t、\\v等)   \n",
    "            line = re.split('([: ,.(){}\\[\\]=])',line)        #将字符串数据按照括号中的符号进行分割，分割成列表格式，并且在列表中保留分隔符\n",
    "            line = list(filter(lambda x: x!=' 'and x!='',line))\n",
    "            for word in line:\n",
    "                counter[word] += 1\n",
    "                \n",
    "        counter = filter(lambda x: x[1] > min_word_freq, counter.items())\n",
    "        sorted_word_to_cnt = sorted(counter,key=itemgetter(1),reverse=True)\n",
    "        sorted_words = [x[0] for x in sorted_word_to_cnt]\n",
    "\n",
    "    sorted_words = [\"<UNK>\",\"<GO>\",\"<EOS>\",\"<PAD>\"] + sorted_words\n",
    "\n",
    "    print(\"vocab_len: \" + str(len(sorted_words)))\n",
    "\n",
    "    return sorted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "satisfied-nitrogen",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_len: 4036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<UNK>',\n",
       " '<GO>',\n",
       " '<EOS>',\n",
       " '<PAD>',\n",
       " '=',\n",
       " '.',\n",
       " ')',\n",
       " '(',\n",
       " ',',\n",
       " ':',\n",
       " '[',\n",
       " ']',\n",
       " 'tf',\n",
       " 'self',\n",
       " 'def',\n",
       " 'if',\n",
       " 'import',\n",
       " 'return',\n",
       " 'np',\n",
       " 'in',\n",
       " '\"\"',\n",
       " 'name',\n",
       " 'for',\n",
       " 'x',\n",
       " 'shape',\n",
       " 'sess',\n",
       " 'with',\n",
       " '+',\n",
       " 'as',\n",
       " 'run',\n",
       " '0',\n",
       " '}',\n",
       " '{',\n",
       " 'from',\n",
       " 'else',\n",
       " '1',\n",
       " 'train',\n",
       " 'placeholder',\n",
       " 'nn',\n",
       " 'os',\n",
       " 'feed_dict',\n",
       " '*',\n",
       " 'dtype',\n",
       " 'range',\n",
       " 'random',\n",
       " 'model',\n",
       " 'append',\n",
       " 'not',\n",
       " 'i',\n",
       " 'path',\n",
       " 'py_utils',\n",
       " 'float32',\n",
       " 'Variable',\n",
       " 'inputs',\n",
       " 'len',\n",
       " 'tensorflow',\n",
       " 'layers',\n",
       " 'data_format',\n",
       " '_',\n",
       " 'summary',\n",
       " 'net',\n",
       " 'axis',\n",
       " 'outputs',\n",
       " 'None',\n",
       " 'reshape',\n",
       " 'saver',\n",
       " 'reduce_mean',\n",
       " 'elif',\n",
       " 'time',\n",
       " 'batch_size',\n",
       " 'y',\n",
       " 'zeros',\n",
       " 'in_channels',\n",
       " 'class',\n",
       " 'plt',\n",
       " 'session',\n",
       " 'join',\n",
       " 'variable_scope',\n",
       " 'size',\n",
       " 'Session',\n",
       " 'zip',\n",
       " 'is',\n",
       " '**kwargs',\n",
       " 'global_variables_initializer',\n",
       " 'flags',\n",
       " 'strides',\n",
       " '/',\n",
       " 'matmul',\n",
       " 'graph',\n",
       " 'input_dict',\n",
       " '2',\n",
       " 'learning_rate',\n",
       " 'scope',\n",
       " 'str',\n",
       " 'name_scope',\n",
       " 'FLAGS',\n",
       " 'numpy',\n",
       " 'training',\n",
       " 'and',\n",
       " 'logits',\n",
       " 'utils',\n",
       " '-',\n",
       " 'model_name',\n",
       " 'int',\n",
       " 'parser',\n",
       " '__init__',\n",
       " 'array',\n",
       " 's',\n",
       " 'X',\n",
       " 'labels',\n",
       " 'optimizer',\n",
       " 'v',\n",
       " 'relu',\n",
       " 'out_channels',\n",
       " '%',\n",
       " 'h',\n",
       " 'assertEqual',\n",
       " 'raise',\n",
       " 'dense',\n",
       " 'blocks',\n",
       " 'loss',\n",
       " 'minimize',\n",
       " 'params',\n",
       " 'input_shape',\n",
       " 'padding',\n",
       " 'help',\n",
       " '>',\n",
       " 'data',\n",
       " 'cast',\n",
       " 'constant',\n",
       " 'default',\n",
       " 'a',\n",
       " 'global_step',\n",
       " 'mnist',\n",
       " 'evaluate',\n",
       " 'conv2d',\n",
       " 'dict',\n",
       " 'activation',\n",
       " 'add_argument',\n",
       " 'randint',\n",
       " 'astype',\n",
       " 'config',\n",
       " 'AdamOptimizer',\n",
       " 'args',\n",
       " 'f',\n",
       " 'enumerate',\n",
       " 'type',\n",
       " 't',\n",
       " 'backend',\n",
       " '<',\n",
       " 'seed',\n",
       " 'build_model',\n",
       " 'reduce_sum',\n",
       " 'z',\n",
       " 'use_cpu_only',\n",
       " 'get_variable',\n",
       " 'b',\n",
       " 'Saver',\n",
       " 'slim',\n",
       " 'p',\n",
       " 'random_gen',\n",
       " 'channels',\n",
       " 'math',\n",
       " 'reuse',\n",
       " 'TensorFlowBaseTest',\n",
       " 'run_compare_tf',\n",
       " 'int32',\n",
       " 'var',\n",
       " 'ax',\n",
       " 'argmax',\n",
       " 'xavier_init',\n",
       " 'or',\n",
       " 'input_values',\n",
       " 'restore',\n",
       " 'output',\n",
       " 'ValueError',\n",
       " '!',\n",
       " 'save',\n",
       " 'List',\n",
       " 'step',\n",
       " 'initializer',\n",
       " 'bn',\n",
       " 'Graph',\n",
       " '5',\n",
       " 'eval',\n",
       " 'train_op',\n",
       " 'as_default',\n",
       " 'W',\n",
       " 'high',\n",
       " '__name__',\n",
       " 'list',\n",
       " 'contrib',\n",
       " 'value',\n",
       " 'assertAllClose',\n",
       " 'm',\n",
       " 'out',\n",
       " 'stddev',\n",
       " 'image',\n",
       " '__future__',\n",
       " 'scalar',\n",
       " 'trainable',\n",
       " 'try',\n",
       " 'var_list',\n",
       " 'NestedMap',\n",
       " 'low',\n",
       " 'split',\n",
       " '\"__main__\"',\n",
       " 'keras',\n",
       " 'pretrained',\n",
       " 'open',\n",
       " 'env',\n",
       " 'input_data',\n",
       " 'models',\n",
       " 'activation_fn',\n",
       " 'exists',\n",
       " 'add',\n",
       " 'log',\n",
       " 'lambda',\n",
       " 'isinstance',\n",
       " '3',\n",
       " 'n',\n",
       " 'result',\n",
       " 'except',\n",
       " 'res',\n",
       " 'stack',\n",
       " 'sum',\n",
       " 'while',\n",
       " 'Y',\n",
       " 'main',\n",
       " 'd',\n",
       " 'ct',\n",
       " 'j',\n",
       " 'epoch',\n",
       " '-1',\n",
       " 'DEFINE_integer',\n",
       " 'state',\n",
       " 'values',\n",
       " 'float',\n",
       " 'core',\n",
       " 'keep_prob',\n",
       " 'rate',\n",
       " 'sqrt',\n",
       " 'app',\n",
       " 'expand_dims',\n",
       " 'examples',\n",
       " 'concat',\n",
       " 'tensor',\n",
       " 'mode',\n",
       " 'weights',\n",
       " 'matplotlib',\n",
       " 'img',\n",
       " 'write',\n",
       " 'ops',\n",
       " 'r',\n",
       " 'init_block_channels',\n",
       " 'get_shape',\n",
       " '->',\n",
       " 'rand_max',\n",
       " 'c',\n",
       " 'im',\n",
       " 'assertAllEqual',\n",
       " 'fig',\n",
       " 'rand_min',\n",
       " 'zs',\n",
       " 'accuracy',\n",
       " 'bottleneck',\n",
       " 'rand',\n",
       " 'results',\n",
       " 'init',\n",
       " 'H',\n",
       " 'normal',\n",
       " 'scale',\n",
       " 'as_list',\n",
       " 'dropout',\n",
       " 'sys',\n",
       " 'trainable_variables',\n",
       " 'sample',\n",
       " 'lr',\n",
       " 'one_hot',\n",
       " 'histogram',\n",
       " 'use_gpu',\n",
       " 'transpose',\n",
       " 'gridspec',\n",
       " 'groups',\n",
       " 'val',\n",
       " 'update',\n",
       " 'sigmoid',\n",
       " 'tx',\n",
       " \"'\",\n",
       " 'super',\n",
       " 'gs',\n",
       " 'network',\n",
       " 'g',\n",
       " 'DEFINE_string',\n",
       " 'next_batch',\n",
       " 'features',\n",
       " 'classes',\n",
       " 'load',\n",
       " 'cost',\n",
       " 'get_collection',\n",
       " 'conv_layer',\n",
       " 'FileWriter',\n",
       " 'op',\n",
       " 'device',\n",
       " 'mnist_network',\n",
       " 'flatten',\n",
       " 'mean',\n",
       " 'x_train',\n",
       " 'test',\n",
       " 'identity',\n",
       " 'predict',\n",
       " 'state_dict',\n",
       " 'width_scale',\n",
       " 'gfile',\n",
       " 'is_channels_first',\n",
       " '+\"\"',\n",
       " 'is_training',\n",
       " 'layer',\n",
       " 'images',\n",
       " 'expected',\n",
       " 'frontend_only',\n",
       " 'mask',\n",
       " 'batch',\n",
       " 'predictions',\n",
       " 'python',\n",
       " 'softmax',\n",
       " 'ckpt',\n",
       " 'arange',\n",
       " 'l',\n",
       " 'square',\n",
       " 'w',\n",
       " 'random_normal',\n",
       " 'close',\n",
       " 'variational',\n",
       " 'multiply',\n",
       " 'coord',\n",
       " 'x_test',\n",
       " 'logging',\n",
       " 'linear',\n",
       " 'mlmodel',\n",
       " 'True',\n",
       " 'makedirs',\n",
       " 'input_ids',\n",
       " 'units',\n",
       " 'assign',\n",
       " 'plot',\n",
       " 'input',\n",
       " 'uniform',\n",
       " 'datetime',\n",
       " 'argparse',\n",
       " 'start',\n",
       " 'all_vars',\n",
       " 'dataset',\n",
       " 'x_input',\n",
       " 'keys',\n",
       " 'imshow',\n",
       " 'break',\n",
       " 'compat',\n",
       " 'add_summary',\n",
       " 'pyplot',\n",
       " 'stride',\n",
       " 'e',\n",
       " 'discriminator',\n",
       " 'prediction',\n",
       " 'conv1_stride',\n",
       " '//',\n",
       " 'epsilon',\n",
       " 'xs',\n",
       " 'pool_size',\n",
       " 'boxes',\n",
       " 'indices',\n",
       " 'deeppavlov',\n",
       " 'shuffle',\n",
       " 'lower_bound',\n",
       " 'collections',\n",
       " 'kernel_size',\n",
       " 'gamma',\n",
       " 'v1',\n",
       " '4',\n",
       " 'shapes',\n",
       " 'ConfigProto',\n",
       " 'it',\n",
       " 'torch',\n",
       " 'print_function',\n",
       " 'control_dependencies',\n",
       " 'six',\n",
       " 'group',\n",
       " 'ys',\n",
       " 'samples',\n",
       " 'k',\n",
       " 'num_filters',\n",
       " 'writer',\n",
       " 'cv2',\n",
       " 'ae',\n",
       " 'filter_length',\n",
       " 'squeeze',\n",
       " 'file_path',\n",
       " 'equal',\n",
       " 'read_data_sets',\n",
       " 'GetShape',\n",
       " 'line',\n",
       " 'convert_to_tensor',\n",
       " 'tutorials',\n",
       " 'start_time',\n",
       " 'get_default_graph',\n",
       " 'n_hidden',\n",
       " 'method',\n",
       " 'in_size',\n",
       " '100',\n",
       " 'idx',\n",
       " 'WeightInit',\n",
       " 'kernel_initializer',\n",
       " 'random_uniform',\n",
       " 'prod',\n",
       " 'bias',\n",
       " 'gradients',\n",
       " 'save_path',\n",
       " 'min',\n",
       " 'reset_default_graph',\n",
       " 'model_store',\n",
       " 'decoder',\n",
       " 'y_net',\n",
       " 'init_variables_from_state_dict',\n",
       " 'download_state_dict',\n",
       " 'merge_all',\n",
       " 'Union',\n",
       " '__call__',\n",
       " '_test',\n",
       " 'exp',\n",
       " '-tf',\n",
       " 'index',\n",
       " 'tree',\n",
       " 'gpu_options',\n",
       " 'DEFINE_boolean',\n",
       " 'n_particles',\n",
       " 'key',\n",
       " 'batch_i',\n",
       " 'division',\n",
       " 'biases',\n",
       " 'convert',\n",
       " 'latest_checkpoint',\n",
       " 'newaxis',\n",
       " '_init_graph',\n",
       " 'probs',\n",
       " 'pred',\n",
       " 'opt',\n",
       " '10',\n",
       " 'test_batch_size',\n",
       " 'version',\n",
       " 'hparams',\n",
       " 'set',\n",
       " 'done',\n",
       " 'vstack',\n",
       " 'conv1x1_block',\n",
       " 'group_ndims',\n",
       " 'convolutional',\n",
       " 'fetches',\n",
       " 'get_variable_scope',\n",
       " 'bool',\n",
       " 'filter_sizes',\n",
       " 'encoder',\n",
       " 'raw_ops',\n",
       " 'cls',\n",
       " 'apply_gradients',\n",
       " 'cardinality',\n",
       " 'target',\n",
       " 'wnu',\n",
       " 'random_normal_initializer',\n",
       " 'draw',\n",
       " 'items',\n",
       " 'n_features',\n",
       " 'use_bias',\n",
       " 'glob',\n",
       " 'shutil',\n",
       " 'func',\n",
       " 'y_train',\n",
       " 'absolute_import',\n",
       " 'AC',\n",
       " 'ids',\n",
       " '1000',\n",
       " 'get',\n",
       " 'rnn',\n",
       " 'constant_initializer',\n",
       " 'choice',\n",
       " 's_',\n",
       " 'input_mask',\n",
       " 'disp_console',\n",
       " 'nested_map',\n",
       " 'kwargs',\n",
       " 'False',\n",
       " 'errors',\n",
       " 'action',\n",
       " 'subplot',\n",
       " 'label',\n",
       " 'sequence_length',\n",
       " 'z_dim',\n",
       " 'dropout_rate',\n",
       " 'maximum',\n",
       " 'common',\n",
       " 'testing',\n",
       " 'gym',\n",
       " 'continue',\n",
       " 'max',\n",
       " 'max_pool',\n",
       " 'G_loss',\n",
       " 'Flatten',\n",
       " 'scipy',\n",
       " 'rnn_cell',\n",
       " 'read',\n",
       " 'weight',\n",
       " 'lingvo',\n",
       " 'grad',\n",
       " 'Dict',\n",
       " '\"',\n",
       " 'variables',\n",
       " 'argvs',\n",
       " 'alpha',\n",
       " 'D_b2',\n",
       " 'summary_writer',\n",
       " 're',\n",
       " 'node',\n",
       " 'Exception',\n",
       " 'DEFINE_float',\n",
       " 'D_b1',\n",
       " 'ksize',\n",
       " 'Dense',\n",
       " 'truncated_normal',\n",
       " 'GraphKeys',\n",
       " '32',\n",
       " 'generator',\n",
       " 'n_epochs',\n",
       " 'bottleneck_width',\n",
       " 'paddings',\n",
       " 'Transform',\n",
       " 'json',\n",
       " 'attr',\n",
       " 'x_pl',\n",
       " 'GLOBAL_RUNNING_R',\n",
       " 'ones_like',\n",
       " 'ones',\n",
       " 'conv1d',\n",
       " 'D_loss',\n",
       " '500',\n",
       " 'rank',\n",
       " 'maxval',\n",
       " 'memory',\n",
       " 'global_variables',\n",
       " 'fwd',\n",
       " 'tfs',\n",
       " 'CreateVariable',\n",
       " 'allow_soft_placement',\n",
       " 'print',\n",
       " 'tolist',\n",
       " 'abs',\n",
       " 'choose_action',\n",
       " 'grads',\n",
       " 'startswith',\n",
       " 'end',\n",
       " 'weight_variable',\n",
       " 'z_mean',\n",
       " 'embeddings',\n",
       " 'boxes_filtered',\n",
       " 'OrderedDict',\n",
       " 'parse_args',\n",
       " 'extend',\n",
       " 'bias_variable',\n",
       " 'zeros_like',\n",
       " 'metric',\n",
       " 'copy',\n",
       " 'recon',\n",
       " 'io',\n",
       " 'item',\n",
       " 'cmap',\n",
       " 'framework',\n",
       " 'norm',\n",
       " 'figure',\n",
       " 'G_b1',\n",
       " 'output_node',\n",
       " 'local_variables_initializer',\n",
       " 'del',\n",
       " 'C',\n",
       " 'batch_xs',\n",
       " 'X_train',\n",
       " 'moves',\n",
       " 'set_random_seed',\n",
       " 'feature',\n",
       " 'getLogger',\n",
       " 'is_train',\n",
       " 'D',\n",
       " 'sample_z',\n",
       " 'var_grads',\n",
       " 'Optional',\n",
       " 'conv1',\n",
       " 'tflearn',\n",
       " 'channels_per_layers',\n",
       " 'start_idx',\n",
       " 'conv3x3_block',\n",
       " 'beta',\n",
       " 'Params',\n",
       " 'var_grads_vals',\n",
       " 'G_b2',\n",
       " 'files',\n",
       " 'cond',\n",
       " 'ema',\n",
       " 'current_input',\n",
       " 'capacity',\n",
       " 'allow_growth',\n",
       " 'num_outputs',\n",
       " 'buffer_r',\n",
       " 'filename',\n",
       " 'G',\n",
       " 'eps',\n",
       " 'pc',\n",
       " 'ArgumentParser',\n",
       " 'match',\n",
       " 'hidden_size',\n",
       " 'n_samples',\n",
       " 'prefix',\n",
       " 'assertRaises',\n",
       " 'importlib',\n",
       " 'l2_loss',\n",
       " 'beta1',\n",
       " 'zero_state',\n",
       " 'set_seed',\n",
       " 'embedding',\n",
       " 'minval',\n",
       " 'pts',\n",
       " 'splits',\n",
       " 'hidden',\n",
       " 'facenet',\n",
       " 'figsize',\n",
       " 'threading',\n",
       " 'n_filters',\n",
       " 'G_prob',\n",
       " 'int64',\n",
       " 'id',\n",
       " 'ref',\n",
       " 'B',\n",
       " 'G_sample',\n",
       " 'batchnorm',\n",
       " 'compute_gradients',\n",
       " 'root',\n",
       " 'keepdims',\n",
       " 'assert_allclose',\n",
       " 'reuse_variables',\n",
       " 'atol',\n",
       " 'input_dim',\n",
       " 'A',\n",
       " 'std',\n",
       " 'width',\n",
       " 'yield',\n",
       " 'metrics',\n",
       " 'BayesianNet',\n",
       " 'emb',\n",
       " '256',\n",
       " 'sigmoid_cross_entropy_with_logits',\n",
       " 'stdout',\n",
       " 'verbose',\n",
       " 'y_test',\n",
       " 'encoding',\n",
       " 'bias_add',\n",
       " 'pop',\n",
       " 'assertIsInstance',\n",
       " 'wspace',\n",
       " 'bert_config',\n",
       " 'hspace',\n",
       " 'image_lists',\n",
       " 'buffer_a',\n",
       " 'GridSpec',\n",
       " 'RMSPropOptimizer',\n",
       " 'logger',\n",
       " 'conv',\n",
       " 'offset',\n",
       " 'x_shape',\n",
       " 'depth',\n",
       " 'set_aspect',\n",
       " 'reader',\n",
       " 'init_op',\n",
       " 'buffer_s',\n",
       " 'cell',\n",
       " '__all__',\n",
       " 'pooling_layer',\n",
       " 'pad',\n",
       " 'lx_z',\n",
       " 'GLOBAL_EP',\n",
       " 'TensorType',\n",
       " 'data_type',\n",
       " 'mkdir',\n",
       " 'get_train_op',\n",
       " 'prob',\n",
       " 'sess_config',\n",
       " 'segment_ids',\n",
       " 'batch_memory',\n",
       " 'lines',\n",
       " 'reverse',\n",
       " 'dilations',\n",
       " 'steps',\n",
       " 'show',\n",
       " 'threads',\n",
       " 'GFile',\n",
       " 'SESS',\n",
       " 'cross_entropy',\n",
       " 'sorted',\n",
       " 'q',\n",
       " 'label_ids',\n",
       " 'batch_norm',\n",
       " 'dimensions',\n",
       " 'learn',\n",
       " 'test_freq',\n",
       " 'summaries',\n",
       " 'train_data',\n",
       " 'h_dim',\n",
       " 'bbox_inches',\n",
       " 'cadl',\n",
       " 'Coordinator',\n",
       " 'v_init',\n",
       " 'PoolingStrategy',\n",
       " 'final_block_channels',\n",
       " 'clip',\n",
       " 'less',\n",
       " 'input_masks',\n",
       " 'zfill',\n",
       " 'length',\n",
       " 'savefig',\n",
       " '_build_feed_dict',\n",
       " '_Params',\n",
       " '128',\n",
       " 'keras_model',\n",
       " 'test_mode',\n",
       " 'lz_x',\n",
       " 'probs_filtered',\n",
       " 'dim',\n",
       " 'checkpoint_dir',\n",
       " 'dtypes',\n",
       " 'seq',\n",
       " 'y_shape',\n",
       " 'flush',\n",
       " 'train_step',\n",
       " 'input_node',\n",
       " '_build_net',\n",
       " '50',\n",
       " 'D_real',\n",
       " 'input_',\n",
       " 'total_loss',\n",
       " 'maxpool2d',\n",
       " 'GradientDescentOptimizer',\n",
       " 'reduce_max',\n",
       " 'current_data',\n",
       " 'fp',\n",
       " 'grads_and_vars',\n",
       " 'train_labels',\n",
       " 'WeightParams',\n",
       " 'model_dir',\n",
       " 'xrange',\n",
       " 'pass',\n",
       " 'ndarray',\n",
       " 'load_path',\n",
       " '64',\n",
       " 'perm',\n",
       " 'weight_count',\n",
       " 'cfg',\n",
       " 'concatenate',\n",
       " 'checkpoint_path',\n",
       " 'bernoulli',\n",
       " 'vars',\n",
       " 'in_dim',\n",
       " 'mb_size',\n",
       " 'set_yticklabels',\n",
       " 'tag',\n",
       " '200',\n",
       " 'x_value',\n",
       " 'set_xticklabels',\n",
       " 'vocab',\n",
       " 'Pack',\n",
       " 'basename',\n",
       " 'make',\n",
       " 'pooled',\n",
       " 'replace',\n",
       " 'local_model_store_dir_path',\n",
       " 'get_preresnet',\n",
       " 'softmax_cross_entropy_with_logits',\n",
       " 'reset',\n",
       " 'initial_state',\n",
       " 'xavier_stddev',\n",
       " 'data_flow',\n",
       " 'bias_initializer',\n",
       " 'tempfile',\n",
       " 'phase_train',\n",
       " 'log_dir',\n",
       " 'residual',\n",
       " 'seq_len',\n",
       " 'ndims',\n",
       " 'X_mb',\n",
       " 'stat',\n",
       " 'abspath',\n",
       " 'COORD',\n",
       " 'get_resnet',\n",
       " 'time_epoch',\n",
       " 'log_string',\n",
       " '_init_placeholders',\n",
       " 'downsample',\n",
       " 'tqdm',\n",
       " 'losses',\n",
       " 'debug',\n",
       " 'current_label',\n",
       " 'channels_per_stage',\n",
       " 'conv2',\n",
       " 'train_ops',\n",
       " 'ijk',\n",
       " 'architecture_map',\n",
       " 'x_dim',\n",
       " '-time',\n",
       " 'Z',\n",
       " 'placeholder_with_default',\n",
       " 'n_classes',\n",
       " 'rtol',\n",
       " 'txt',\n",
       " 'epoch_i',\n",
       " 'NotImplementedError',\n",
       " 'summary_op',\n",
       " 'context',\n",
       " 'imread',\n",
       " 'inference',\n",
       " 'no_op',\n",
       " 'initialize_all_variables',\n",
       " 'count',\n",
       " 'D_fake',\n",
       " 'now',\n",
       " 'imgs',\n",
       " 'info',\n",
       " 'X_dim',\n",
       " 'pooling_strategy',\n",
       " 'finally',\n",
       " 'modules',\n",
       " 'tile',\n",
       " '6',\n",
       " 'ceil',\n",
       " 'Tuple',\n",
       " 'crop_shape',\n",
       " 'n_code',\n",
       " 'zeros_initializer',\n",
       " 'AveragePooling2D',\n",
       " 'checkpoint',\n",
       " 'filter_mat_boxes',\n",
       " 'clip_by_value',\n",
       " 'input_size',\n",
       " 'iterator',\n",
       " 'train_writer',\n",
       " 'cumsum',\n",
       " 'theta_D',\n",
       " 'expanduser',\n",
       " 'ckpt_name',\n",
       " 'batch_labels',\n",
       " 'modeling',\n",
       " 'assign_ops',\n",
       " 'ep_r',\n",
       " 'n_layers',\n",
       " 'color',\n",
       " 'distribution',\n",
       " 'exclude_scopes',\n",
       " 'assert_equal',\n",
       " 'lbs',\n",
       " 'display_step',\n",
       " 'get_channel_axis',\n",
       " 'T',\n",
       " 'max_to_keep',\n",
       " 'traced_model',\n",
       " 'randn',\n",
       " 'nni',\n",
       " 'noise',\n",
       " 'dx',\n",
       " \"'z'\",\n",
       " 'it_i',\n",
       " 'where',\n",
       " '16',\n",
       " 'obj',\n",
       " 'conv_layers',\n",
       " 'time_test',\n",
       " 'assertRaisesRegex',\n",
       " 'assertTrue',\n",
       " 'valid_result',\n",
       " 'pretrained_bert',\n",
       " 'global',\n",
       " 'vals',\n",
       " 'upsample_and_concat',\n",
       " 'weight_decay_rate',\n",
       " 'conv2d_transpose',\n",
       " 'HasShape',\n",
       " 'expected_val',\n",
       " 'should_stop',\n",
       " 'n_output',\n",
       " 'features_li',\n",
       " 'acc',\n",
       " 'vocab_size',\n",
       " 'attention_params',\n",
       " 'empty',\n",
       " 'fwd_sig',\n",
       " 'TRAINABLE_VARIABLES',\n",
       " 'G_solver',\n",
       " 'l1',\n",
       " 'D_solver',\n",
       " 'readlines',\n",
       " 'fan_in',\n",
       " 'y_dim',\n",
       " 'embedding_lookup',\n",
       " 'x_target',\n",
       " 'v_s_',\n",
       " '9',\n",
       " 'next',\n",
       " 'axes',\n",
       " 'en',\n",
       " 'file',\n",
       " 'num_classes',\n",
       " 'conv3',\n",
       " 'mu',\n",
       " 'ta',\n",
       " 'output_shape',\n",
       " 'max_outputs',\n",
       " 'dirname',\n",
       " 'slice',\n",
       " 'rmtree',\n",
       " 'FlattenItems',\n",
       " 'ret',\n",
       " 'bert',\n",
       " 'expand_path',\n",
       " 'dims',\n",
       " '**2',\n",
       " 'get_seresnet',\n",
       " 'RuntimeError',\n",
       " 'num_layers',\n",
       " 'log_prob',\n",
       " 'tensors',\n",
       " 'CopyFrom',\n",
       " '255',\n",
       " 'D_W2',\n",
       " 'get_sepreresnet',\n",
       " '\"input\"',\n",
       " 'BATCH_SIZE',\n",
       " 'conv_dim',\n",
       " 'x_data',\n",
       " 'Instantiate',\n",
       " 'audio',\n",
       " 'get_checkpoint_state',\n",
       " 'misc',\n",
       " 'y2',\n",
       " 'staircase',\n",
       " 'emb_array',\n",
       " 'z_mu',\n",
       " 'in_padding',\n",
       " 'sparse_softmax_cross_entropy_with_logits',\n",
       " 'conv1x1',\n",
       " 'y_pred',\n",
       " 'x_logits',\n",
       " 'permutation',\n",
       " 'import_module',\n",
       " 'start_queue_runners',\n",
       " 'BertConfig',\n",
       " 'computation_shape',\n",
       " 'resize_identity',\n",
       " '20',\n",
       " 'lengths',\n",
       " 'classes_num_filtered',\n",
       " 'exponential_decay',\n",
       " 'D_W1',\n",
       " 'conv4',\n",
       " 'R',\n",
       " 'ISWeights',\n",
       " 'with_dependencies',\n",
       " 'strip',\n",
       " 'fn',\n",
       " 'dprep_dict',\n",
       " 'theta_G',\n",
       " 'checkpoint_exists',\n",
       " 'data_path',\n",
       " 'sums',\n",
       " 'el',\n",
       " 'L1',\n",
       " 'util',\n",
       " 'D_loss_curr',\n",
       " 'tfa',\n",
       " 'feat_grid',\n",
       " 'begin',\n",
       " 'filters',\n",
       " 'out_padding',\n",
       " 'sigma',\n",
       " 'GLOBAL_VARIABLES',\n",
       " 'unstack',\n",
       " 'fan_out',\n",
       " 'y_val',\n",
       " 'var_name',\n",
       " 'test_acc',\n",
       " 'If',\n",
       " 'y_batch',\n",
       " 'render',\n",
       " 'bottleneck_values',\n",
       " 'resize',\n",
       " 'idxs',\n",
       " 'hstack',\n",
       " 'epochs',\n",
       " 'all',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = \"../00-data/tf_data.txt\"\n",
    "min_word_freq = 3\n",
    "vocab = get_vocab(input_data,min_word_freq)\n",
    "vocab_file = \"../00-data/vocab\"\n",
    "fout_vocab = codecs.open(vocab_file,\"w\",\"utf-8\")\n",
    "for word in vocab:\n",
    "    fout_vocab.write(word + \"\\n\")\n",
    "fout_vocab.close()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-norfolk",
   "metadata": {},
   "source": [
    "### 【随机】前k句预测下一句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "extensive-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_split(line):\n",
    "    line = ' '.join(re.split(' |\\t|\\v|\\n',line))        \n",
    "    line = re.split('([: ,.(){}\\[\\]=])',line)        \n",
    "    line = list(filter(lambda x: x!=' 'and x!='',line))\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "through-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_newdata_by_random(raw_data,source_train,source_test,target_train,target_test,rand_max=10,duplicate=1):\n",
    "    import codecs\n",
    "    import sys\n",
    "    import re\n",
    "\n",
    "    fout_source_train = codecs.open(source_train,\"w\",\"utf-8\")\n",
    "    fout_source_test = codecs.open(source_test,\"w\",\"utf-8\")\n",
    "    fout_target_train = codecs.open(target_train,\"w\",\"utf-8\")\n",
    "    fout_target_test = codecs.open(target_test,\"w\",\"utf-8\")\n",
    "    \n",
    "    data_lists = []\n",
    "    data_length = 0\n",
    "    with open(raw_data,\"r\") as fin:\n",
    "        lines = fin.readlines()\n",
    "        for i in range(rand_max,len(lines)):\n",
    "            rand_nums = set(random.randint(1,rand_max) for _ in range(duplicate))\n",
    "            for rand_num in rand_nums:\n",
    "                data_line = \"\"\n",
    "                #构造enc_data\n",
    "                words = []\n",
    "                for j in range(i - rand_num,i):\n",
    "                    line = clean_and_split(lines[j])\n",
    "                    words += [\"<GO>\"] + line + [\"<EOS>\"]\n",
    "                data_line += ' '.join(words)\n",
    "\n",
    "                #构造dec_data\n",
    "                words = []\n",
    "                line = clean_and_split(lines[i])\n",
    "                words = line + [\"<EOS>\"]\n",
    "                data_line += \" !@! \" + ' '.join(words)\n",
    "                \n",
    "                data_lists.append(data_line)\n",
    "                data_length += 1\n",
    "    \n",
    "    random.shuffle(data_lists)\n",
    "    test_nums = data_length // 10\n",
    "    \n",
    "    for i in range(test_nums):\n",
    "        line = data_lists[i].split(\"!@!\")\n",
    "        fout_source_test.write(line[0].strip() + \"\\n\")\n",
    "        fout_target_test.write(line[1].strip() + \"\\n\")\n",
    "        \n",
    "    for i in range(test_nums,data_length):\n",
    "        line = data_lists[i].split(\"!@!\")\n",
    "        fout_source_train.write(line[0].strip() + \"\\n\")\n",
    "        fout_target_train.write(line[1].strip() + \"\\n\")\n",
    "        \n",
    "    fout_source_train.close()\n",
    "    fout_source_test.close()\n",
    "    fout_target_train.close()\n",
    "    fout_target_test.close()\n",
    "    \n",
    "    return data_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aquatic-approach",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train = \"../00-data/beam_search_data/source_train\"\n",
    "source_test = \"../00-data/beam_search_data/source_test\"\n",
    "target_train = \"../00-data/beam_search_data/target_train\"\n",
    "target_test = \"../00-data/beam_search_data/target_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "periodic-oxygen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_len: 39003\n"
     ]
    }
   ],
   "source": [
    "data_length = get_newdata_by_random(input_data,source_train,source_test,target_train,target_test)\n",
    "print(\"data_len: \" + str(data_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-theater",
   "metadata": {},
   "source": [
    "### beam_search部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "disturbed-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "base_dir = 'data/beam_search_data'\n",
    "\n",
    "\n",
    "def open_file(filename, mode='r'):\n",
    "    return open(filename, mode, encoding='utf-8', errors='ignore')\n",
    "\n",
    "\n",
    "def process_file(file_dir, letter_to_id):\n",
    "    letter_ids = []\n",
    "    len_ = []\n",
    "    with open_file(file_dir) as f:\n",
    "        for line in f:\n",
    "            letter_id = []\n",
    "            conts = line.strip().split(\" \")\n",
    "            for con in conts:\n",
    "                letter_id.append(letter_to_id.get(con, letter_to_id['<UNK>']))\n",
    "            letter_ids.append(letter_id)\n",
    "            len_.append(len(letter_id))\n",
    "    return np.array(letter_ids), np.array(len_)\n",
    "\n",
    "def read_vocab(vocab_dir):\n",
    "    \"\"\"读取词汇表\"\"\"\n",
    "    words = open_file(vocab_dir).read().strip().split('\\n')\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "\n",
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "\n",
    "def clip_batch(sources_batch, targets_batch, source_pad_int, target_pad_int):\n",
    "    pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "    pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "    # 记录每条记录的长度\n",
    "    targets_lengths = []\n",
    "    for target in targets_batch:\n",
    "        targets_lengths.append(len(target))\n",
    "    source_lengths = []\n",
    "    for source in sources_batch:\n",
    "        source_lengths.append(len(source))\n",
    "    return pad_sources_batch, source_lengths, pad_targets_batch, targets_lengths\n",
    "\n",
    "\n",
    "# def batch_iter(source_train, len_source_train, target_train, len_target_train, batch_size, source_pad_int, target_pad_int):\n",
    "def batch_iter(source_train, target_train, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"生成批次数据\"\"\"\n",
    "    data_len = len(source_train)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield clip_batch(source_train[start_id:end_id], target_train[start_id:end_id], source_pad_int, target_pad_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-lloyd",
   "metadata": {},
   "source": [
    "## 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "light-relation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "\n",
    "class ModelConfig(object):\n",
    "    \"\"\"CNN配置参数\"\"\"\n",
    "    print_per_batch = 10    # 每多少轮输出一次结果\n",
    "    num_epochs = 30\n",
    "    batch_size = 256\n",
    "    rnn_size = 256\n",
    "    num_layers = 2\n",
    "    encoding_embedding_size = 256\n",
    "    decoding_embedding_size = 256\n",
    "    learning_rate_base = 0.003\n",
    "    learning_rate_decay = 0.95  #应该设置成0.9比较好\n",
    "    beam_width = 5\n",
    "\n",
    "\n",
    "def get_multi_rnn_cell(rnn_size, num_layers):\n",
    "    return tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    \"\"\"文本分类，CNN模型\"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        # 输入\n",
    "        self.source = tf.placeholder(tf.int32, [None, None], name='source')\n",
    "        self.target = tf.placeholder(tf.int32, [None, None], name='target')\n",
    "        self.target_sequence_length = tf.placeholder(tf.int32, (None,), name='target_sequence_length')\n",
    "        self.source_sequence_length = tf.placeholder(tf.int32, (None,), name='source_sequence_length')\n",
    "        self.global_step = tf.placeholder(tf.int32, name='global_step')\n",
    "        \n",
    "        # 1. encoder\n",
    "        source_embedding = tf.get_variable('source_embedding', [self.config.source_vocab_size, self.config.encoding_embedding_size])\n",
    "        source_embedding_inputs = tf.nn.embedding_lookup(source_embedding, self.source)\n",
    "\n",
    "        # bi-LSTM\n",
    "        fw_lstm_cell = tf.contrib.rnn.LSTMCell(int(self.config.rnn_size / 2), initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        bw_lstm_cell = tf.contrib.rnn.LSTMCell(int(self.config.rnn_size / 2), initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        (outputs, (fw_state, bw_state)) = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_lstm_cell,\n",
    "                                                                          cell_bw=bw_lstm_cell,\n",
    "                                                                          inputs=source_embedding_inputs,\n",
    "                                                                          sequence_length=self.source_sequence_length,\n",
    "                                                                          dtype=tf.float32)\n",
    "        encoder_output = tf.concat(outputs, -1)\n",
    "        encoder_final_state_c = tf.concat([fw_state.c, bw_state.c], -1)\n",
    "        encoder_final_state_h = tf.concat([fw_state.h, bw_state.h], -1)\n",
    "        encoder_state = tf.contrib.rnn.LSTMStateTuple(\n",
    "            c=encoder_final_state_c,\n",
    "            h=encoder_final_state_h\n",
    "        )\n",
    "\n",
    "        # 2. decoder\n",
    "        ending = tf.strided_slice(self.target, [0, 0], [self.config.batch_size, -1], [1, 1])\n",
    "        decoder_input = tf.concat([tf.fill([tf.shape(self.target)[0], 1], self.config.target_letter_to_id['<GO>']), ending], 1)\n",
    "        target_embedding = tf.get_variable('target_embedding', [self.config.target_vocab_size, self.config.decoding_embedding_size])\n",
    "        target_embedding_inputs = tf.nn.embedding_lookup(target_embedding, decoder_input)\n",
    "\n",
    "        decoder_cell = get_multi_rnn_cell(self.config.rnn_size, self.config.num_layers)\n",
    "        output_layer = Dense(self.config.target_vocab_size, kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "\n",
    "        # 训练阶段\n",
    "        with tf.variable_scope(\"decode\"):\n",
    "            # attention\n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(self.config.rnn_size, encoder_output, memory_sequence_length=self.source_sequence_length)\n",
    "            attention_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism, attention_layer_size=self.config.rnn_size)\n",
    "\n",
    "            # initial_state\n",
    "            initial_state = attention_decoder_cell.zero_state(tf.shape(self.source)[0], tf.float32).clone(cell_state=encoder_state)\n",
    "\n",
    "            training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=target_embedding_inputs, sequence_length=self.target_sequence_length)\n",
    "            training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                attention_decoder_cell,\n",
    "                training_helper,\n",
    "                initial_state,  # 使用encoder模块的输出状态来初始化attention_decoder的初始states，若直接使用encoder_state会报错\n",
    "                output_layer)\n",
    "            training_decoder_output, _, __ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                training_decoder,\n",
    "                impute_finished=True,  # 遇到EOS自动停止解码（EOS之后的所有time step的输出为0，输出状态为最后一个有效time step的输出状态）\n",
    "                maximum_iterations=None)  # 设置最大decoding time steps数量，默认decode until the decoder is fully done，因为训练时会将target序列传入，所以可以为None\n",
    "            self.logits = training_decoder_output.rnn_output\n",
    "            \n",
    "        # 测试阶段\n",
    "        with tf.variable_scope(\"decode\", reuse=True):\n",
    "            # a. decoder_attention\n",
    "            bs_encoder_output = tf.contrib.seq2seq.tile_batch(encoder_output, multiplier=self.config.beam_width)  # tile_batch等价于复制10份，然后concat(..., 0)\n",
    "            bs_sequence_length = tf.contrib.seq2seq.tile_batch(self.source_sequence_length, multiplier=self.config.beam_width)\n",
    "\n",
    "            bs_attention_mechanism = tf.contrib.seq2seq.LuongAttention(self.config.rnn_size, bs_encoder_output, memory_sequence_length=bs_sequence_length)\n",
    "            bs_attention_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, bs_attention_mechanism, attention_layer_size=self.config.rnn_size)\n",
    "\n",
    "            # b. decoder_initial_state\n",
    "            bs_cell_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=self.config.beam_width)\n",
    "            bs_initial_state = bs_attention_decoder_cell.zero_state(tf.shape(self.source)[0] * self.config.beam_width, tf.float32).clone(cell_state=bs_cell_state)\n",
    "\n",
    "            predicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                cell=bs_attention_decoder_cell,\n",
    "                embedding=target_embedding,\n",
    "                start_tokens=tf.fill([tf.shape(self.source)[0]], self.config.target_letter_to_id['<GO>']),\n",
    "                end_token=self.config.target_letter_to_id['<EOS>'],\n",
    "                initial_state=bs_initial_state,\n",
    "                beam_width=self.config.beam_width,\n",
    "                output_layer=output_layer,\n",
    "                length_penalty_weight=0.0)  # 对长度较短的生成结果施加惩罚，0.0表示不惩罚\n",
    "\n",
    "            predicting_decoder_output, _, __ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                predicting_decoder,\n",
    "                impute_finished=False,  # 遇到EOS自动停止解码输出（停止输出，输出状态为最后一个有效time step的输出状态）\n",
    "                maximum_iterations=tf.round(tf.reduce_max(self.source_sequence_length) * 2))  # 预测时不知道什么时候输出EOS，所以要设置最大time step数量\n",
    "            self.result_ids = tf.transpose(predicting_decoder_output.predicted_ids, perm=[0, 2, 1])  # 输出target vocab id：[batch_size, beam_width, max_time_step]\n",
    "\n",
    "        #print(tf.trainable_variables())\n",
    "    \n",
    "        # 3. optimize\n",
    "        masks = tf.sequence_mask(self.target_sequence_length, tf.reduce_max(self.target_sequence_length), dtype=tf.float32)\n",
    "        self.loss = tf.contrib.seq2seq.sequence_loss(self.logits, self.target, masks)\n",
    "        self.learning_rate = tf.train.exponential_decay(\n",
    "            self.config.learning_rate_base,\n",
    "            self.global_step,\n",
    "            data_length / self.config.batch_size,   #data_length是数据集的大小\n",
    "            self.config.learning_rate_decay,\n",
    "            staircase=True)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        # 梯度裁剪\n",
    "        gradients = optimizer.compute_gradients(self.loss)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        self.train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-rebate",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "blocked-repair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring model...\n",
      "Loading data...\n",
      "Training and evaluating...\n",
      "Epoch: 1\n",
      "total_batch:     0, Train Loss: 8.30339, Val Loss: 8.30261, Learning Rate:0.00300, Time: 0:00:06 *\n",
      "total_batch:    10, Train Loss: 4.66083, Val Loss: 4.66200, Learning Rate:0.00300, Time: 0:00:18 *\n",
      "total_batch:    20, Train Loss: 4.21011, Val Loss: 4.15212, Learning Rate:0.00300, Time: 0:00:29 *\n",
      "total_batch:    30, Train Loss: 3.77734, Val Loss: 3.75390, Learning Rate:0.00300, Time: 0:00:39 *\n",
      "total_batch:    40, Train Loss: 3.35191, Val Loss: 3.32229, Learning Rate:0.00300, Time: 0:00:49 *\n",
      "total_batch:    50, Train Loss: 3.02818, Val Loss: 3.08593, Learning Rate:0.00300, Time: 0:00:59 *\n",
      "total_batch:    60, Train Loss: 2.98787, Val Loss: 2.91431, Learning Rate:0.00300, Time: 0:01:10 *\n",
      "total_batch:    70, Train Loss: 2.93914, Val Loss: 2.78485, Learning Rate:0.00300, Time: 0:01:20 *\n",
      "total_batch:    80, Train Loss: 2.67408, Val Loss: 2.66444, Learning Rate:0.00300, Time: 0:01:31 *\n",
      "total_batch:    90, Train Loss: 2.63409, Val Loss: 2.57158, Learning Rate:0.00300, Time: 0:01:42 *\n",
      "total_batch:   100, Train Loss: 2.41069, Val Loss: 2.49713, Learning Rate:0.00300, Time: 0:01:53 *\n",
      "total_batch:   110, Train Loss: 2.33779, Val Loss: 2.40501, Learning Rate:0.00300, Time: 0:02:04 *\n",
      "total_batch:   120, Train Loss: 2.34798, Val Loss: 2.35189, Learning Rate:0.00300, Time: 0:02:15 *\n",
      "total_batch:   130, Train Loss: 2.20545, Val Loss: 2.27600, Learning Rate:0.00300, Time: 0:02:27 *\n",
      "Epoch: 2\n",
      "total_batch:   140, Train Loss: 2.36880, Val Loss: 2.22364, Learning Rate:0.00300, Time: 0:02:37 *\n",
      "total_batch:   150, Train Loss: 2.03925, Val Loss: 2.16265, Learning Rate:0.00300, Time: 0:02:49 *\n",
      "total_batch:   160, Train Loss: 2.13999, Val Loss: 2.10286, Learning Rate:0.00285, Time: 0:03:00 *\n",
      "total_batch:   170, Train Loss: 2.05828, Val Loss: 2.04605, Learning Rate:0.00285, Time: 0:03:11 *\n",
      "total_batch:   180, Train Loss: 1.92413, Val Loss: 1.98845, Learning Rate:0.00285, Time: 0:03:21 *\n",
      "total_batch:   190, Train Loss: 1.90215, Val Loss: 1.97044, Learning Rate:0.00285, Time: 0:03:31 *\n",
      "total_batch:   200, Train Loss: 1.87653, Val Loss: 1.91913, Learning Rate:0.00285, Time: 0:03:42 *\n",
      "total_batch:   210, Train Loss: 1.86140, Val Loss: 1.89045, Learning Rate:0.00285, Time: 0:03:53 *\n",
      "total_batch:   220, Train Loss: 1.83935, Val Loss: 1.84073, Learning Rate:0.00285, Time: 0:04:04 *\n",
      "total_batch:   230, Train Loss: 1.73508, Val Loss: 1.81522, Learning Rate:0.00285, Time: 0:04:16 *\n",
      "total_batch:   240, Train Loss: 1.65124, Val Loss: 1.78119, Learning Rate:0.00285, Time: 0:04:33 *\n",
      "total_batch:   250, Train Loss: 1.74572, Val Loss: 1.74779, Learning Rate:0.00285, Time: 0:04:44 *\n",
      "total_batch:   260, Train Loss: 1.59703, Val Loss: 1.73042, Learning Rate:0.00285, Time: 0:04:55 *\n",
      "total_batch:   270, Train Loss: 1.56722, Val Loss: 1.69857, Learning Rate:0.00285, Time: 0:05:06 *\n",
      "Epoch: 3\n",
      "total_batch:   280, Train Loss: 1.61968, Val Loss: 1.70224, Learning Rate:0.00285, Time: 0:05:16 \n",
      "total_batch:   290, Train Loss: 1.59289, Val Loss: 1.65814, Learning Rate:0.00285, Time: 0:05:28 *\n",
      "total_batch:   300, Train Loss: 1.58691, Val Loss: 1.64047, Learning Rate:0.00285, Time: 0:05:39 *\n",
      "total_batch:   310, Train Loss: 1.55119, Val Loss: 1.61345, Learning Rate:0.00271, Time: 0:05:49 *\n",
      "total_batch:   320, Train Loss: 1.47800, Val Loss: 1.58970, Learning Rate:0.00271, Time: 0:05:59 *\n",
      "total_batch:   330, Train Loss: 1.44481, Val Loss: 1.57275, Learning Rate:0.00271, Time: 0:06:10 *\n",
      "total_batch:   340, Train Loss: 1.33356, Val Loss: 1.56055, Learning Rate:0.00271, Time: 0:06:20 *\n",
      "total_batch:   350, Train Loss: 1.38763, Val Loss: 1.54159, Learning Rate:0.00271, Time: 0:06:31 *\n",
      "total_batch:   360, Train Loss: 1.42066, Val Loss: 1.51574, Learning Rate:0.00271, Time: 0:06:43 *\n",
      "total_batch:   370, Train Loss: 1.47224, Val Loss: 1.51254, Learning Rate:0.00271, Time: 0:06:53 *\n",
      "total_batch:   380, Train Loss: 1.38438, Val Loss: 1.49675, Learning Rate:0.00271, Time: 0:07:05 *\n",
      "total_batch:   390, Train Loss: 1.44157, Val Loss: 1.48615, Learning Rate:0.00271, Time: 0:07:15 *\n",
      "total_batch:   400, Train Loss: 1.38072, Val Loss: 1.46220, Learning Rate:0.00271, Time: 0:07:26 *\n",
      "total_batch:   410, Train Loss: 1.24531, Val Loss: 1.44633, Learning Rate:0.00271, Time: 0:07:36 *\n",
      "Epoch: 4\n",
      "total_batch:   420, Train Loss: 1.28445, Val Loss: 1.47390, Learning Rate:0.00271, Time: 0:07:46 \n",
      "total_batch:   430, Train Loss: 1.38023, Val Loss: 1.43617, Learning Rate:0.00271, Time: 0:07:57 *\n",
      "total_batch:   440, Train Loss: 1.28668, Val Loss: 1.44628, Learning Rate:0.00271, Time: 0:08:06 \n",
      "total_batch:   450, Train Loss: 1.33285, Val Loss: 1.40906, Learning Rate:0.00271, Time: 0:08:16 *\n",
      "total_batch:   460, Train Loss: 1.20359, Val Loss: 1.40350, Learning Rate:0.00257, Time: 0:08:26 *\n",
      "total_batch:   470, Train Loss: 1.27531, Val Loss: 1.39740, Learning Rate:0.00257, Time: 0:08:36 *\n",
      "total_batch:   480, Train Loss: 1.12078, Val Loss: 1.39135, Learning Rate:0.00257, Time: 0:08:47 *\n",
      "total_batch:   490, Train Loss: 1.17220, Val Loss: 1.38083, Learning Rate:0.00257, Time: 0:08:58 *\n",
      "total_batch:   500, Train Loss: 1.17308, Val Loss: 1.37254, Learning Rate:0.00257, Time: 0:09:09 *\n",
      "total_batch:   510, Train Loss: 1.15114, Val Loss: 1.36406, Learning Rate:0.00257, Time: 0:09:19 *\n",
      "total_batch:   520, Train Loss: 1.17935, Val Loss: 1.35239, Learning Rate:0.00257, Time: 0:09:31 *\n",
      "total_batch:   530, Train Loss: 1.18374, Val Loss: 1.35009, Learning Rate:0.00257, Time: 0:09:42 *\n",
      "total_batch:   540, Train Loss: 1.08224, Val Loss: 1.33295, Learning Rate:0.00257, Time: 0:09:51 *\n",
      "total_batch:   550, Train Loss: 1.15330, Val Loss: 1.32307, Learning Rate:0.00257, Time: 0:10:02 *\n",
      "Epoch: 5\n",
      "total_batch:   560, Train Loss: 1.16242, Val Loss: 1.34456, Learning Rate:0.00257, Time: 0:10:11 \n",
      "total_batch:   570, Train Loss: 1.09899, Val Loss: 1.32504, Learning Rate:0.00257, Time: 0:10:21 \n",
      "total_batch:   580, Train Loss: 1.02151, Val Loss: 1.32192, Learning Rate:0.00257, Time: 0:10:32 *\n",
      "total_batch:   590, Train Loss: 1.05392, Val Loss: 1.30426, Learning Rate:0.00257, Time: 0:10:42 *\n",
      "total_batch:   600, Train Loss: 0.99849, Val Loss: 1.29961, Learning Rate:0.00257, Time: 0:10:52 *\n",
      "total_batch:   610, Train Loss: 1.09732, Val Loss: 1.29887, Learning Rate:0.00244, Time: 0:11:02 *\n",
      "total_batch:   620, Train Loss: 0.96444, Val Loss: 1.30534, Learning Rate:0.00244, Time: 0:11:12 \n",
      "total_batch:   630, Train Loss: 1.01400, Val Loss: 1.29520, Learning Rate:0.00244, Time: 0:11:23 *\n",
      "total_batch:   640, Train Loss: 1.04469, Val Loss: 1.29493, Learning Rate:0.00244, Time: 0:11:34 *\n",
      "total_batch:   650, Train Loss: 0.93727, Val Loss: 1.28336, Learning Rate:0.00244, Time: 0:11:45 *\n",
      "total_batch:   660, Train Loss: 1.04977, Val Loss: 1.27829, Learning Rate:0.00244, Time: 0:11:56 *\n",
      "total_batch:   670, Train Loss: 0.82837, Val Loss: 1.27120, Learning Rate:0.00244, Time: 0:12:07 *\n",
      "total_batch:   680, Train Loss: 1.08154, Val Loss: 1.26211, Learning Rate:0.00244, Time: 0:12:17 *\n",
      "Epoch: 6\n",
      "total_batch:   690, Train Loss: 0.97151, Val Loss: 1.27043, Learning Rate:0.00244, Time: 0:12:27 \n",
      "total_batch:   700, Train Loss: 0.98193, Val Loss: 1.28166, Learning Rate:0.00244, Time: 0:12:36 \n",
      "total_batch:   710, Train Loss: 0.86792, Val Loss: 1.26369, Learning Rate:0.00244, Time: 0:12:46 \n",
      "total_batch:   720, Train Loss: 0.91522, Val Loss: 1.25722, Learning Rate:0.00244, Time: 0:12:57 *\n",
      "total_batch:   730, Train Loss: 0.89186, Val Loss: 1.25954, Learning Rate:0.00244, Time: 0:13:06 \n",
      "total_batch:   740, Train Loss: 0.81092, Val Loss: 1.25899, Learning Rate:0.00244, Time: 0:13:15 \n",
      "total_batch:   750, Train Loss: 0.98783, Val Loss: 1.25326, Learning Rate:0.00244, Time: 0:13:26 *\n",
      "total_batch:   760, Train Loss: 0.96097, Val Loss: 1.26136, Learning Rate:0.00244, Time: 0:13:36 \n",
      "total_batch:   770, Train Loss: 0.83442, Val Loss: 1.25227, Learning Rate:0.00232, Time: 0:13:46 *\n",
      "total_batch:   780, Train Loss: 0.81666, Val Loss: 1.25090, Learning Rate:0.00232, Time: 0:13:58 *\n",
      "total_batch:   790, Train Loss: 0.77398, Val Loss: 1.23164, Learning Rate:0.00232, Time: 0:14:09 *\n",
      "total_batch:   800, Train Loss: 0.76169, Val Loss: 1.23423, Learning Rate:0.00232, Time: 0:14:19 \n",
      "total_batch:   810, Train Loss: 0.82799, Val Loss: 1.22431, Learning Rate:0.00232, Time: 0:14:30 *\n",
      "total_batch:   820, Train Loss: 0.79040, Val Loss: 1.22012, Learning Rate:0.00232, Time: 0:14:41 *\n",
      "Epoch: 7\n",
      "total_batch:   830, Train Loss: 0.86337, Val Loss: 1.25058, Learning Rate:0.00232, Time: 0:14:50 \n",
      "total_batch:   840, Train Loss: 0.75170, Val Loss: 1.25011, Learning Rate:0.00232, Time: 0:15:00 \n",
      "total_batch:   850, Train Loss: 0.83312, Val Loss: 1.24585, Learning Rate:0.00232, Time: 0:15:09 \n",
      "total_batch:   860, Train Loss: 0.84753, Val Loss: 1.22574, Learning Rate:0.00232, Time: 0:15:18 \n",
      "total_batch:   870, Train Loss: 0.80337, Val Loss: 1.21980, Learning Rate:0.00232, Time: 0:15:29 *\n",
      "total_batch:   880, Train Loss: 0.76485, Val Loss: 1.23768, Learning Rate:0.00232, Time: 0:15:38 \n",
      "total_batch:   890, Train Loss: 0.79269, Val Loss: 1.23624, Learning Rate:0.00232, Time: 0:15:47 \n",
      "total_batch:   900, Train Loss: 0.79106, Val Loss: 1.23478, Learning Rate:0.00232, Time: 0:15:57 \n",
      "total_batch:   910, Train Loss: 0.80988, Val Loss: 1.23254, Learning Rate:0.00232, Time: 0:16:06 \n",
      "total_batch:   920, Train Loss: 0.73023, Val Loss: 1.23298, Learning Rate:0.00221, Time: 0:16:16 \n",
      "total_batch:   930, Train Loss: 0.64212, Val Loss: 1.22153, Learning Rate:0.00221, Time: 0:16:26 \n",
      "total_batch:   940, Train Loss: 0.74277, Val Loss: 1.23298, Learning Rate:0.00221, Time: 0:16:35 \n",
      "total_batch:   950, Train Loss: 0.64764, Val Loss: 1.20837, Learning Rate:0.00221, Time: 0:16:46 *\n",
      "total_batch:   960, Train Loss: 0.69427, Val Loss: 1.21299, Learning Rate:0.00221, Time: 0:16:55 \n",
      "Epoch: 8\n",
      "total_batch:   970, Train Loss: 0.75401, Val Loss: 1.22532, Learning Rate:0.00221, Time: 0:17:05 \n",
      "total_batch:   980, Train Loss: 0.68255, Val Loss: 1.23299, Learning Rate:0.00221, Time: 0:17:15 \n",
      "total_batch:   990, Train Loss: 0.72039, Val Loss: 1.22117, Learning Rate:0.00221, Time: 0:17:25 \n",
      "total_batch:  1000, Train Loss: 0.70090, Val Loss: 1.21521, Learning Rate:0.00221, Time: 0:17:34 \n",
      "total_batch:  1010, Train Loss: 0.67053, Val Loss: 1.21205, Learning Rate:0.00221, Time: 0:17:43 \n",
      "total_batch:  1020, Train Loss: 0.59514, Val Loss: 1.23339, Learning Rate:0.00221, Time: 0:17:52 \n",
      "total_batch:  1030, Train Loss: 0.60990, Val Loss: 1.23808, Learning Rate:0.00221, Time: 0:18:01 \n",
      "total_batch:  1040, Train Loss: 0.68693, Val Loss: 1.23616, Learning Rate:0.00221, Time: 0:18:11 \n",
      "total_batch:  1050, Train Loss: 0.68470, Val Loss: 1.22889, Learning Rate:0.00221, Time: 0:18:21 \n",
      "total_batch:  1060, Train Loss: 0.73894, Val Loss: 1.23237, Learning Rate:0.00221, Time: 0:18:32 \n",
      "total_batch:  1070, Train Loss: 0.64807, Val Loss: 1.22488, Learning Rate:0.00210, Time: 0:18:42 \n",
      "total_batch:  1080, Train Loss: 0.69654, Val Loss: 1.22887, Learning Rate:0.00210, Time: 0:18:52 \n",
      "total_batch:  1090, Train Loss: 0.69673, Val Loss: 1.21411, Learning Rate:0.00210, Time: 0:19:02 \n",
      "total_batch:  1100, Train Loss: 0.58690, Val Loss: 1.21815, Learning Rate:0.00210, Time: 0:19:12 \n",
      "Epoch: 9\n",
      "total_batch:  1110, Train Loss: 0.59577, Val Loss: 1.23268, Learning Rate:0.00210, Time: 0:19:22 \n",
      "total_batch:  1120, Train Loss: 0.60228, Val Loss: 1.23639, Learning Rate:0.00210, Time: 0:19:32 \n",
      "total_batch:  1130, Train Loss: 0.63388, Val Loss: 1.22845, Learning Rate:0.00210, Time: 0:19:41 \n",
      "total_batch:  1140, Train Loss: 0.61929, Val Loss: 1.22545, Learning Rate:0.00210, Time: 0:19:50 \n",
      "total_batch:  1150, Train Loss: 0.55423, Val Loss: 1.21939, Learning Rate:0.00210, Time: 0:19:59 \n",
      "total_batch:  1160, Train Loss: 0.64560, Val Loss: 1.23847, Learning Rate:0.00210, Time: 0:20:08 \n",
      "total_batch:  1170, Train Loss: 0.52357, Val Loss: 1.25284, Learning Rate:0.00210, Time: 0:20:17 \n",
      "total_batch:  1180, Train Loss: 0.57827, Val Loss: 1.24582, Learning Rate:0.00210, Time: 0:20:27 \n",
      "total_batch:  1190, Train Loss: 0.57923, Val Loss: 1.24342, Learning Rate:0.00210, Time: 0:20:37 \n",
      "total_batch:  1200, Train Loss: 0.60500, Val Loss: 1.24139, Learning Rate:0.00210, Time: 0:20:46 \n",
      "total_batch:  1210, Train Loss: 0.61115, Val Loss: 1.24602, Learning Rate:0.00210, Time: 0:20:56 \n",
      "total_batch:  1220, Train Loss: 0.58384, Val Loss: 1.24723, Learning Rate:0.00199, Time: 0:21:05 \n",
      "total_batch:  1230, Train Loss: 0.53699, Val Loss: 1.22740, Learning Rate:0.00199, Time: 0:21:14 \n",
      "total_batch:  1240, Train Loss: 0.59585, Val Loss: 1.23860, Learning Rate:0.00199, Time: 0:21:23 \n",
      "Epoch: 10\n",
      "total_batch:  1250, Train Loss: 0.57153, Val Loss: 1.24504, Learning Rate:0.00199, Time: 0:21:32 \n",
      "total_batch:  1260, Train Loss: 0.52390, Val Loss: 1.24536, Learning Rate:0.00199, Time: 0:21:42 \n",
      "total_batch:  1270, Train Loss: 0.46618, Val Loss: 1.23977, Learning Rate:0.00199, Time: 0:21:51 \n",
      "total_batch:  1280, Train Loss: 0.50595, Val Loss: 1.25218, Learning Rate:0.00199, Time: 0:22:00 \n",
      "total_batch:  1290, Train Loss: 0.51799, Val Loss: 1.24062, Learning Rate:0.00199, Time: 0:22:09 \n",
      "total_batch:  1300, Train Loss: 0.58523, Val Loss: 1.24861, Learning Rate:0.00199, Time: 0:22:18 \n",
      "total_batch:  1310, Train Loss: 0.50564, Val Loss: 1.26349, Learning Rate:0.00199, Time: 0:22:28 \n",
      "total_batch:  1320, Train Loss: 0.51489, Val Loss: 1.26152, Learning Rate:0.00199, Time: 0:22:37 \n",
      "total_batch:  1330, Train Loss: 0.52965, Val Loss: 1.25572, Learning Rate:0.00199, Time: 0:22:47 \n",
      "total_batch:  1340, Train Loss: 0.50962, Val Loss: 1.26410, Learning Rate:0.00199, Time: 0:22:57 \n",
      "total_batch:  1350, Train Loss: 0.55672, Val Loss: 1.27916, Learning Rate:0.00199, Time: 0:23:07 \n",
      "total_batch:  1360, Train Loss: 0.39786, Val Loss: 1.26693, Learning Rate:0.00199, Time: 0:23:17 \n",
      "total_batch:  1370, Train Loss: 0.59692, Val Loss: 1.25441, Learning Rate:0.00199, Time: 0:23:26 \n",
      "Epoch: 11\n",
      "total_batch:  1380, Train Loss: 0.54581, Val Loss: 1.27919, Learning Rate:0.00189, Time: 0:23:34 \n",
      "total_batch:  1390, Train Loss: 0.53315, Val Loss: 1.28090, Learning Rate:0.00189, Time: 0:23:44 \n",
      "total_batch:  1400, Train Loss: 0.45745, Val Loss: 1.25895, Learning Rate:0.00189, Time: 0:23:53 \n",
      "total_batch:  1410, Train Loss: 0.47019, Val Loss: 1.25300, Learning Rate:0.00189, Time: 0:24:02 \n",
      "total_batch:  1420, Train Loss: 0.44032, Val Loss: 1.28662, Learning Rate:0.00189, Time: 0:24:11 \n",
      "total_batch:  1430, Train Loss: 0.41032, Val Loss: 1.26488, Learning Rate:0.00189, Time: 0:24:20 \n",
      "total_batch:  1440, Train Loss: 0.47186, Val Loss: 1.27144, Learning Rate:0.00189, Time: 0:24:29 \n",
      "total_batch:  1450, Train Loss: 0.51147, Val Loss: 1.29473, Learning Rate:0.00189, Time: 0:24:38 \n",
      "total_batch:  1460, Train Loss: 0.41927, Val Loss: 1.27512, Learning Rate:0.00189, Time: 0:24:48 \n",
      "total_batch:  1470, Train Loss: 0.40490, Val Loss: 1.27388, Learning Rate:0.00189, Time: 0:24:57 \n",
      "total_batch:  1480, Train Loss: 0.41031, Val Loss: 1.27849, Learning Rate:0.00189, Time: 0:25:07 \n",
      "total_batch:  1490, Train Loss: 0.38998, Val Loss: 1.29390, Learning Rate:0.00189, Time: 0:25:17 \n",
      "total_batch:  1500, Train Loss: 0.43991, Val Loss: 1.28011, Learning Rate:0.00189, Time: 0:25:26 \n",
      "total_batch:  1510, Train Loss: 0.43857, Val Loss: 1.27393, Learning Rate:0.00189, Time: 0:25:35 \n",
      "Epoch: 12\n",
      "total_batch:  1520, Train Loss: 0.45776, Val Loss: 1.30707, Learning Rate:0.00189, Time: 0:25:44 \n",
      "total_batch:  1530, Train Loss: 0.43884, Val Loss: 1.30900, Learning Rate:0.00180, Time: 0:25:53 \n",
      "total_batch:  1540, Train Loss: 0.46391, Val Loss: 1.29235, Learning Rate:0.00180, Time: 0:26:03 \n",
      "total_batch:  1550, Train Loss: 0.47379, Val Loss: 1.28396, Learning Rate:0.00180, Time: 0:26:12 \n",
      "total_batch:  1560, Train Loss: 0.42802, Val Loss: 1.31672, Learning Rate:0.00180, Time: 0:26:21 \n",
      "total_batch:  1570, Train Loss: 0.41504, Val Loss: 1.28288, Learning Rate:0.00180, Time: 0:26:30 \n",
      "total_batch:  1580, Train Loss: 0.39528, Val Loss: 1.30364, Learning Rate:0.00180, Time: 0:26:39 \n",
      "total_batch:  1590, Train Loss: 0.41668, Val Loss: 1.32114, Learning Rate:0.00180, Time: 0:26:48 \n",
      "total_batch:  1600, Train Loss: 0.42986, Val Loss: 1.31742, Learning Rate:0.00180, Time: 0:26:57 \n",
      "total_batch:  1610, Train Loss: 0.36750, Val Loss: 1.30147, Learning Rate:0.00180, Time: 0:27:07 \n",
      "total_batch:  1620, Train Loss: 0.33930, Val Loss: 1.31753, Learning Rate:0.00180, Time: 0:27:17 \n",
      "total_batch:  1630, Train Loss: 0.37948, Val Loss: 1.31242, Learning Rate:0.00180, Time: 0:27:27 \n",
      "total_batch:  1640, Train Loss: 0.31442, Val Loss: 1.31122, Learning Rate:0.00180, Time: 0:27:37 \n",
      "total_batch:  1650, Train Loss: 0.37084, Val Loss: 1.30353, Learning Rate:0.00180, Time: 0:27:47 \n",
      "Epoch: 13\n",
      "total_batch:  1660, Train Loss: 0.43598, Val Loss: 1.31546, Learning Rate:0.00180, Time: 0:27:56 \n",
      "total_batch:  1670, Train Loss: 0.36929, Val Loss: 1.33531, Learning Rate:0.00180, Time: 0:28:07 \n",
      "total_batch:  1680, Train Loss: 0.35990, Val Loss: 1.32048, Learning Rate:0.00171, Time: 0:28:17 \n",
      "total_batch:  1690, Train Loss: 0.37856, Val Loss: 1.31752, Learning Rate:0.00171, Time: 0:28:26 \n",
      "total_batch:  1700, Train Loss: 0.35439, Val Loss: 1.34576, Learning Rate:0.00171, Time: 0:28:36 \n",
      "total_batch:  1710, Train Loss: 0.29645, Val Loss: 1.32352, Learning Rate:0.00171, Time: 0:28:45 \n",
      "total_batch:  1720, Train Loss: 0.34193, Val Loss: 1.34443, Learning Rate:0.00171, Time: 0:28:55 \n",
      "total_batch:  1730, Train Loss: 0.34512, Val Loss: 1.34584, Learning Rate:0.00171, Time: 0:29:05 \n",
      "total_batch:  1740, Train Loss: 0.35952, Val Loss: 1.36532, Learning Rate:0.00171, Time: 0:29:15 \n",
      "total_batch:  1750, Train Loss: 0.39160, Val Loss: 1.34056, Learning Rate:0.00171, Time: 0:29:25 \n",
      "total_batch:  1760, Train Loss: 0.35233, Val Loss: 1.37417, Learning Rate:0.00171, Time: 0:29:35 \n",
      "total_batch:  1770, Train Loss: 0.39771, Val Loss: 1.35437, Learning Rate:0.00171, Time: 0:29:45 \n",
      "total_batch:  1780, Train Loss: 0.38251, Val Loss: 1.35244, Learning Rate:0.00171, Time: 0:29:55 \n",
      "total_batch:  1790, Train Loss: 0.32740, Val Loss: 1.34027, Learning Rate:0.00171, Time: 0:30:05 \n",
      "Epoch: 14\n",
      "total_batch:  1800, Train Loss: 0.31522, Val Loss: 1.34102, Learning Rate:0.00171, Time: 0:30:15 \n",
      "total_batch:  1810, Train Loss: 0.30649, Val Loss: 1.36747, Learning Rate:0.00171, Time: 0:30:26 \n",
      "total_batch:  1820, Train Loss: 0.35961, Val Loss: 1.34990, Learning Rate:0.00171, Time: 0:30:35 \n",
      "total_batch:  1830, Train Loss: 0.32562, Val Loss: 1.35232, Learning Rate:0.00162, Time: 0:30:45 \n",
      "total_batch:  1840, Train Loss: 0.28509, Val Loss: 1.38252, Learning Rate:0.00162, Time: 0:30:55 \n",
      "total_batch:  1850, Train Loss: 0.35778, Val Loss: 1.36540, Learning Rate:0.00162, Time: 0:31:05 \n",
      "total_batch:  1860, Train Loss: 0.27033, Val Loss: 1.38553, Learning Rate:0.00162, Time: 0:31:14 \n",
      "total_batch:  1870, Train Loss: 0.31454, Val Loss: 1.38060, Learning Rate:0.00162, Time: 0:31:24 \n",
      "total_batch:  1880, Train Loss: 0.28384, Val Loss: 1.39222, Learning Rate:0.00162, Time: 0:31:34 \n",
      "total_batch:  1890, Train Loss: 0.33560, Val Loss: 1.38018, Learning Rate:0.00162, Time: 0:31:45 \n",
      "total_batch:  1900, Train Loss: 0.33227, Val Loss: 1.41798, Learning Rate:0.00162, Time: 0:31:55 \n",
      "total_batch:  1910, Train Loss: 0.28645, Val Loss: 1.40627, Learning Rate:0.00162, Time: 0:32:06 \n",
      "total_batch:  1920, Train Loss: 0.28271, Val Loss: 1.40956, Learning Rate:0.00162, Time: 0:32:14 \n",
      "total_batch:  1930, Train Loss: 0.31083, Val Loss: 1.37896, Learning Rate:0.00162, Time: 0:32:24 \n",
      "Epoch: 15\n",
      "total_batch:  1940, Train Loss: 0.31968, Val Loss: 1.39148, Learning Rate:0.00162, Time: 0:32:34 \n",
      "total_batch:  1950, Train Loss: 0.26860, Val Loss: 1.39170, Learning Rate:0.00162, Time: 0:32:45 \n",
      "total_batch:  1960, Train Loss: 0.23946, Val Loss: 1.38179, Learning Rate:0.00162, Time: 0:32:54 \n",
      "total_batch:  1970, Train Loss: 0.26165, Val Loss: 1.38682, Learning Rate:0.00162, Time: 0:33:04 \n",
      "total_batch:  1980, Train Loss: 0.28439, Val Loss: 1.40326, Learning Rate:0.00162, Time: 0:33:13 \n",
      "total_batch:  1990, Train Loss: 0.31017, Val Loss: 1.40052, Learning Rate:0.00154, Time: 0:33:22 \n",
      "total_batch:  2000, Train Loss: 0.28236, Val Loss: 1.41069, Learning Rate:0.00154, Time: 0:33:32 \n",
      "total_batch:  2010, Train Loss: 0.28457, Val Loss: 1.41676, Learning Rate:0.00154, Time: 0:33:42 \n",
      "total_batch:  2020, Train Loss: 0.27284, Val Loss: 1.42227, Learning Rate:0.00154, Time: 0:33:53 \n",
      "total_batch:  2030, Train Loss: 0.28081, Val Loss: 1.40905, Learning Rate:0.00154, Time: 0:34:03 \n",
      "total_batch:  2040, Train Loss: 0.32279, Val Loss: 1.44239, Learning Rate:0.00154, Time: 0:34:13 \n",
      "total_batch:  2050, Train Loss: 0.20095, Val Loss: 1.42778, Learning Rate:0.00154, Time: 0:34:24 \n",
      "total_batch:  2060, Train Loss: 0.33231, Val Loss: 1.44366, Learning Rate:0.00154, Time: 0:34:33 \n",
      "Epoch: 16\n",
      "total_batch:  2070, Train Loss: 0.27935, Val Loss: 1.41061, Learning Rate:0.00154, Time: 0:34:43 \n",
      "total_batch:  2080, Train Loss: 0.27494, Val Loss: 1.41865, Learning Rate:0.00154, Time: 0:34:53 \n",
      "total_batch:  2090, Train Loss: 0.22284, Val Loss: 1.43011, Learning Rate:0.00154, Time: 0:35:04 \n",
      "total_batch:  2100, Train Loss: 0.24917, Val Loss: 1.41959, Learning Rate:0.00154, Time: 0:35:13 \n",
      "total_batch:  2110, Train Loss: 0.23481, Val Loss: 1.42726, Learning Rate:0.00154, Time: 0:35:23 \n",
      "total_batch:  2120, Train Loss: 0.20697, Val Loss: 1.43483, Learning Rate:0.00154, Time: 0:35:32 \n",
      "total_batch:  2130, Train Loss: 0.24661, Val Loss: 1.44165, Learning Rate:0.00154, Time: 0:35:42 \n",
      "total_batch:  2140, Train Loss: 0.26706, Val Loss: 1.45632, Learning Rate:0.00146, Time: 0:35:52 \n",
      "total_batch:  2150, Train Loss: 0.20692, Val Loss: 1.46027, Learning Rate:0.00146, Time: 0:36:02 \n",
      "total_batch:  2160, Train Loss: 0.21788, Val Loss: 1.46045, Learning Rate:0.00146, Time: 0:36:13 \n",
      "total_batch:  2170, Train Loss: 0.23328, Val Loss: 1.44272, Learning Rate:0.00146, Time: 0:36:23 \n",
      "total_batch:  2180, Train Loss: 0.20502, Val Loss: 1.48166, Learning Rate:0.00146, Time: 0:36:33 \n",
      "total_batch:  2190, Train Loss: 0.23847, Val Loss: 1.46219, Learning Rate:0.00146, Time: 0:36:44 \n",
      "total_batch:  2200, Train Loss: 0.23157, Val Loss: 1.48273, Learning Rate:0.00146, Time: 0:36:53 \n",
      "Epoch: 17\n",
      "total_batch:  2210, Train Loss: 0.22234, Val Loss: 1.47166, Learning Rate:0.00146, Time: 0:37:03 \n",
      "total_batch:  2220, Train Loss: 0.21497, Val Loss: 1.46533, Learning Rate:0.00146, Time: 0:37:13 \n",
      "total_batch:  2230, Train Loss: 0.24453, Val Loss: 1.47245, Learning Rate:0.00146, Time: 0:37:22 \n",
      "total_batch:  2240, Train Loss: 0.25602, Val Loss: 1.46374, Learning Rate:0.00146, Time: 0:37:32 \n",
      "total_batch:  2250, Train Loss: 0.22830, Val Loss: 1.47159, Learning Rate:0.00146, Time: 0:37:42 \n",
      "total_batch:  2260, Train Loss: 0.23354, Val Loss: 1.46924, Learning Rate:0.00146, Time: 0:37:52 \n",
      "total_batch:  2270, Train Loss: 0.22385, Val Loss: 1.47428, Learning Rate:0.00146, Time: 0:38:02 \n",
      "total_batch:  2280, Train Loss: 0.23364, Val Loss: 1.48648, Learning Rate:0.00146, Time: 0:38:12 \n",
      "total_batch:  2290, Train Loss: 0.23062, Val Loss: 1.50532, Learning Rate:0.00139, Time: 0:38:22 \n",
      "total_batch:  2300, Train Loss: 0.23586, Val Loss: 1.51700, Learning Rate:0.00139, Time: 0:38:33 \n",
      "total_batch:  2310, Train Loss: 0.16470, Val Loss: 1.49336, Learning Rate:0.00139, Time: 0:38:43 \n",
      "total_batch:  2320, Train Loss: 0.20686, Val Loss: 1.51131, Learning Rate:0.00139, Time: 0:38:53 \n",
      "total_batch:  2330, Train Loss: 0.15295, Val Loss: 1.51122, Learning Rate:0.00139, Time: 0:39:04 \n",
      "total_batch:  2340, Train Loss: 0.19816, Val Loss: 1.51456, Learning Rate:0.00139, Time: 0:39:13 \n",
      "Epoch: 18\n",
      "total_batch:  2350, Train Loss: 0.25099, Val Loss: 1.51121, Learning Rate:0.00139, Time: 0:39:23 \n",
      "total_batch:  2360, Train Loss: 0.17817, Val Loss: 1.50468, Learning Rate:0.00139, Time: 0:39:34 \n",
      "total_batch:  2370, Train Loss: 0.18970, Val Loss: 1.51904, Learning Rate:0.00139, Time: 0:39:43 \n",
      "total_batch:  2380, Train Loss: 0.19050, Val Loss: 1.52775, Learning Rate:0.00139, Time: 0:39:53 \n",
      "total_batch:  2390, Train Loss: 0.19073, Val Loss: 1.51316, Learning Rate:0.00139, Time: 0:40:03 \n",
      "total_batch:  2400, Train Loss: 0.17340, Val Loss: 1.50739, Learning Rate:0.00139, Time: 0:40:13 \n",
      "total_batch:  2410, Train Loss: 0.21631, Val Loss: 1.52305, Learning Rate:0.00139, Time: 0:40:23 \n",
      "total_batch:  2420, Train Loss: 0.18608, Val Loss: 1.51693, Learning Rate:0.00139, Time: 0:40:33 \n",
      "total_batch:  2430, Train Loss: 0.18063, Val Loss: 1.53482, Learning Rate:0.00139, Time: 0:40:43 \n",
      "total_batch:  2440, Train Loss: 0.23040, Val Loss: 1.56604, Learning Rate:0.00132, Time: 0:40:53 \n",
      "total_batch:  2450, Train Loss: 0.21628, Val Loss: 1.53817, Learning Rate:0.00132, Time: 0:41:04 \n",
      "total_batch:  2460, Train Loss: 0.21675, Val Loss: 1.54794, Learning Rate:0.00132, Time: 0:41:14 \n",
      "total_batch:  2470, Train Loss: 0.21313, Val Loss: 1.54627, Learning Rate:0.00132, Time: 0:41:24 \n",
      "total_batch:  2480, Train Loss: 0.18866, Val Loss: 1.55984, Learning Rate:0.00132, Time: 0:41:33 \n",
      "Epoch: 19\n",
      "total_batch:  2490, Train Loss: 0.17223, Val Loss: 1.55816, Learning Rate:0.00132, Time: 0:41:43 \n",
      "total_batch:  2500, Train Loss: 0.16454, Val Loss: 1.54760, Learning Rate:0.00132, Time: 0:41:54 \n",
      "total_batch:  2510, Train Loss: 0.19640, Val Loss: 1.53938, Learning Rate:0.00132, Time: 0:42:04 \n",
      "total_batch:  2520, Train Loss: 0.16958, Val Loss: 1.55871, Learning Rate:0.00132, Time: 0:42:13 \n",
      "total_batch:  2530, Train Loss: 0.15216, Val Loss: 1.54723, Learning Rate:0.00132, Time: 0:42:23 \n",
      "total_batch:  2540, Train Loss: 0.19451, Val Loss: 1.54505, Learning Rate:0.00132, Time: 0:42:33 \n",
      "total_batch:  2550, Train Loss: 0.14567, Val Loss: 1.55062, Learning Rate:0.00132, Time: 0:42:42 \n",
      "total_batch:  2560, Train Loss: 0.16625, Val Loss: 1.56003, Learning Rate:0.00132, Time: 0:42:53 \n",
      "total_batch:  2570, Train Loss: 0.16067, Val Loss: 1.56894, Learning Rate:0.00132, Time: 0:43:03 \n",
      "total_batch:  2580, Train Loss: 0.19920, Val Loss: 1.58634, Learning Rate:0.00132, Time: 0:43:13 \n",
      "total_batch:  2590, Train Loss: 0.20573, Val Loss: 1.56937, Learning Rate:0.00132, Time: 0:43:23 \n",
      "total_batch:  2600, Train Loss: 0.14762, Val Loss: 1.56346, Learning Rate:0.00125, Time: 0:43:33 \n",
      "total_batch:  2610, Train Loss: 0.16146, Val Loss: 1.58356, Learning Rate:0.00125, Time: 0:43:42 \n",
      "total_batch:  2620, Train Loss: 0.17901, Val Loss: 1.59092, Learning Rate:0.00125, Time: 0:43:51 \n",
      "Epoch: 20\n",
      "total_batch:  2630, Train Loss: 0.16083, Val Loss: 1.59571, Learning Rate:0.00125, Time: 0:44:02 \n",
      "total_batch:  2640, Train Loss: 0.14459, Val Loss: 1.58882, Learning Rate:0.00125, Time: 0:44:12 \n",
      "total_batch:  2650, Train Loss: 0.13811, Val Loss: 1.57834, Learning Rate:0.00125, Time: 0:44:21 \n",
      "total_batch:  2660, Train Loss: 0.14572, Val Loss: 1.58888, Learning Rate:0.00125, Time: 0:44:31 \n",
      "total_batch:  2670, Train Loss: 0.16015, Val Loss: 1.58154, Learning Rate:0.00125, Time: 0:44:40 \n",
      "total_batch:  2680, Train Loss: 0.17198, Val Loss: 1.59013, Learning Rate:0.00125, Time: 0:44:50 \n",
      "total_batch:  2690, Train Loss: 0.13555, Val Loss: 1.57460, Learning Rate:0.00125, Time: 0:45:00 \n",
      "total_batch:  2700, Train Loss: 0.14932, Val Loss: 1.60199, Learning Rate:0.00125, Time: 0:45:09 \n",
      "total_batch:  2710, Train Loss: 0.14442, Val Loss: 1.60174, Learning Rate:0.00125, Time: 0:45:19 \n",
      "total_batch:  2720, Train Loss: 0.15489, Val Loss: 1.61157, Learning Rate:0.00125, Time: 0:45:29 \n",
      "total_batch:  2730, Train Loss: 0.18110, Val Loss: 1.60610, Learning Rate:0.00125, Time: 0:45:40 \n",
      "total_batch:  2740, Train Loss: 0.12056, Val Loss: 1.59583, Learning Rate:0.00125, Time: 0:45:50 \n",
      "total_batch:  2750, Train Loss: 0.16161, Val Loss: 1.60619, Learning Rate:0.00119, Time: 0:45:59 \n",
      "Epoch: 21\n",
      "total_batch:  2760, Train Loss: 0.16911, Val Loss: 1.63750, Learning Rate:0.00119, Time: 0:46:08 \n",
      "total_batch:  2770, Train Loss: 0.16213, Val Loss: 1.62951, Learning Rate:0.00119, Time: 0:46:19 \n",
      "total_batch:  2780, Train Loss: 0.10756, Val Loss: 1.63471, Learning Rate:0.00119, Time: 0:46:29 \n",
      "total_batch:  2790, Train Loss: 0.12798, Val Loss: 1.61144, Learning Rate:0.00119, Time: 0:46:39 \n",
      "total_batch:  2800, Train Loss: 0.12014, Val Loss: 1.63518, Learning Rate:0.00119, Time: 0:46:49 \n",
      "total_batch:  2810, Train Loss: 0.11112, Val Loss: 1.62961, Learning Rate:0.00119, Time: 0:46:59 \n",
      "total_batch:  2820, Train Loss: 0.13766, Val Loss: 1.62596, Learning Rate:0.00119, Time: 0:47:08 \n",
      "total_batch:  2830, Train Loss: 0.15138, Val Loss: 1.60542, Learning Rate:0.00119, Time: 0:47:19 \n",
      "total_batch:  2840, Train Loss: 0.11219, Val Loss: 1.63816, Learning Rate:0.00119, Time: 0:47:29 \n",
      "total_batch:  2850, Train Loss: 0.12267, Val Loss: 1.62389, Learning Rate:0.00119, Time: 0:47:38 \n",
      "total_batch:  2860, Train Loss: 0.13870, Val Loss: 1.64349, Learning Rate:0.00119, Time: 0:47:49 \n",
      "total_batch:  2870, Train Loss: 0.10723, Val Loss: 1.63596, Learning Rate:0.00119, Time: 0:47:59 \n",
      "total_batch:  2880, Train Loss: 0.11941, Val Loss: 1.62237, Learning Rate:0.00119, Time: 0:48:10 \n",
      "total_batch:  2890, Train Loss: 0.11306, Val Loss: 1.64496, Learning Rate:0.00119, Time: 0:48:19 \n",
      "Epoch: 22\n",
      "total_batch:  2900, Train Loss: 0.11842, Val Loss: 1.65579, Learning Rate:0.00113, Time: 0:48:29 \n",
      "total_batch:  2910, Train Loss: 0.11466, Val Loss: 1.65447, Learning Rate:0.00113, Time: 0:48:39 \n",
      "total_batch:  2920, Train Loss: 0.13103, Val Loss: 1.66403, Learning Rate:0.00113, Time: 0:48:49 \n",
      "total_batch:  2930, Train Loss: 0.14421, Val Loss: 1.65199, Learning Rate:0.00113, Time: 0:48:59 \n",
      "total_batch:  2940, Train Loss: 0.11928, Val Loss: 1.66948, Learning Rate:0.00113, Time: 0:49:09 \n",
      "total_batch:  2950, Train Loss: 0.12665, Val Loss: 1.66417, Learning Rate:0.00113, Time: 0:49:18 \n",
      "total_batch:  2960, Train Loss: 0.11130, Val Loss: 1.66165, Learning Rate:0.00113, Time: 0:49:28 \n",
      "total_batch:  2970, Train Loss: 0.12618, Val Loss: 1.64797, Learning Rate:0.00113, Time: 0:49:38 \n",
      "total_batch:  2980, Train Loss: 0.12058, Val Loss: 1.66346, Learning Rate:0.00113, Time: 0:49:47 \n",
      "total_batch:  2990, Train Loss: 0.11703, Val Loss: 1.66190, Learning Rate:0.00113, Time: 0:49:58 \n",
      "total_batch:  3000, Train Loss: 0.08564, Val Loss: 1.67237, Learning Rate:0.00113, Time: 0:50:07 \n",
      "total_batch:  3010, Train Loss: 0.10021, Val Loss: 1.67161, Learning Rate:0.00113, Time: 0:50:17 \n",
      "total_batch:  3020, Train Loss: 0.08221, Val Loss: 1.64561, Learning Rate:0.00113, Time: 0:50:28 \n",
      "total_batch:  3030, Train Loss: 0.09972, Val Loss: 1.68902, Learning Rate:0.00113, Time: 0:50:37 \n",
      "Epoch: 23\n",
      "total_batch:  3040, Train Loss: 0.16179, Val Loss: 1.69048, Learning Rate:0.00113, Time: 0:50:47 \n",
      "total_batch:  3050, Train Loss: 0.09470, Val Loss: 1.68507, Learning Rate:0.00108, Time: 0:50:59 \n",
      "total_batch:  3060, Train Loss: 0.10253, Val Loss: 1.67990, Learning Rate:0.00108, Time: 0:51:08 \n",
      "total_batch:  3070, Train Loss: 0.09783, Val Loss: 1.66630, Learning Rate:0.00108, Time: 0:51:18 \n",
      "total_batch:  3080, Train Loss: 0.09811, Val Loss: 1.69059, Learning Rate:0.00108, Time: 0:51:27 \n",
      "total_batch:  3090, Train Loss: 0.07934, Val Loss: 1.70416, Learning Rate:0.00108, Time: 0:51:37 \n",
      "total_batch:  3100, Train Loss: 0.11236, Val Loss: 1.69224, Learning Rate:0.00108, Time: 0:51:46 \n",
      "total_batch:  3110, Train Loss: 0.10431, Val Loss: 1.69535, Learning Rate:0.00108, Time: 0:51:56 \n",
      "total_batch:  3120, Train Loss: 0.09280, Val Loss: 1.68273, Learning Rate:0.00108, Time: 0:52:06 \n",
      "total_batch:  3130, Train Loss: 0.11166, Val Loss: 1.69230, Learning Rate:0.00108, Time: 0:52:16 \n",
      "total_batch:  3140, Train Loss: 0.11000, Val Loss: 1.70178, Learning Rate:0.00108, Time: 0:52:27 \n",
      "total_batch:  3150, Train Loss: 0.10628, Val Loss: 1.71127, Learning Rate:0.00108, Time: 0:52:36 \n",
      "total_batch:  3160, Train Loss: 0.11199, Val Loss: 1.67405, Learning Rate:0.00108, Time: 0:52:46 \n",
      "total_batch:  3170, Train Loss: 0.09701, Val Loss: 1.72149, Learning Rate:0.00108, Time: 0:52:56 \n",
      "Epoch: 24\n",
      "total_batch:  3180, Train Loss: 0.08179, Val Loss: 1.70799, Learning Rate:0.00108, Time: 0:53:06 \n",
      "total_batch:  3190, Train Loss: 0.07833, Val Loss: 1.72025, Learning Rate:0.00108, Time: 0:53:16 \n",
      "total_batch:  3200, Train Loss: 0.09891, Val Loss: 1.71693, Learning Rate:0.00102, Time: 0:53:25 \n",
      "total_batch:  3210, Train Loss: 0.08444, Val Loss: 1.69476, Learning Rate:0.00102, Time: 0:53:35 \n",
      "total_batch:  3220, Train Loss: 0.07211, Val Loss: 1.71422, Learning Rate:0.00102, Time: 0:53:44 \n",
      "total_batch:  3230, Train Loss: 0.09464, Val Loss: 1.73443, Learning Rate:0.00102, Time: 0:53:54 \n",
      "total_batch:  3240, Train Loss: 0.08201, Val Loss: 1.72064, Learning Rate:0.00102, Time: 0:54:03 \n",
      "total_batch:  3250, Train Loss: 0.07946, Val Loss: 1.73405, Learning Rate:0.00102, Time: 0:54:13 \n",
      "total_batch:  3260, Train Loss: 0.07032, Val Loss: 1.70949, Learning Rate:0.00102, Time: 0:54:23 \n",
      "total_batch:  3270, Train Loss: 0.09455, Val Loss: 1.71862, Learning Rate:0.00102, Time: 0:54:33 \n",
      "total_batch:  3280, Train Loss: 0.09400, Val Loss: 1.73209, Learning Rate:0.00102, Time: 0:54:44 \n",
      "total_batch:  3290, Train Loss: 0.06165, Val Loss: 1.74171, Learning Rate:0.00102, Time: 0:54:54 \n",
      "total_batch:  3300, Train Loss: 0.07300, Val Loss: 1.70792, Learning Rate:0.00102, Time: 0:55:04 \n",
      "total_batch:  3310, Train Loss: 0.08428, Val Loss: 1.74533, Learning Rate:0.00102, Time: 0:55:13 \n",
      "Epoch: 25\n",
      "total_batch:  3320, Train Loss: 0.07127, Val Loss: 1.73937, Learning Rate:0.00102, Time: 0:55:23 \n",
      "total_batch:  3330, Train Loss: 0.07339, Val Loss: 1.73772, Learning Rate:0.00102, Time: 0:55:33 \n",
      "total_batch:  3340, Train Loss: 0.07190, Val Loss: 1.75904, Learning Rate:0.00102, Time: 0:55:43 \n",
      "total_batch:  3350, Train Loss: 0.07531, Val Loss: 1.73652, Learning Rate:0.00102, Time: 0:55:52 \n",
      "total_batch:  3360, Train Loss: 0.08282, Val Loss: 1.75413, Learning Rate:0.00097, Time: 0:56:02 \n",
      "total_batch:  3370, Train Loss: 0.08948, Val Loss: 1.75994, Learning Rate:0.00097, Time: 0:56:11 \n",
      "total_batch:  3380, Train Loss: 0.06880, Val Loss: 1.74365, Learning Rate:0.00097, Time: 0:56:22 \n",
      "total_batch:  3390, Train Loss: 0.07567, Val Loss: 1.75684, Learning Rate:0.00097, Time: 0:56:32 \n",
      "total_batch:  3400, Train Loss: 0.07114, Val Loss: 1.74701, Learning Rate:0.00097, Time: 0:56:42 \n",
      "total_batch:  3410, Train Loss: 0.07356, Val Loss: 1.74525, Learning Rate:0.00097, Time: 0:56:52 \n",
      "total_batch:  3420, Train Loss: 0.08021, Val Loss: 1.77147, Learning Rate:0.00097, Time: 0:57:03 \n",
      "total_batch:  3430, Train Loss: 0.05887, Val Loss: 1.76406, Learning Rate:0.00097, Time: 0:57:13 \n",
      "total_batch:  3440, Train Loss: 0.07870, Val Loss: 1.75835, Learning Rate:0.00097, Time: 0:57:23 \n",
      "Epoch: 26\n",
      "total_batch:  3450, Train Loss: 0.09035, Val Loss: 1.77226, Learning Rate:0.00097, Time: 0:57:32 \n",
      "total_batch:  3460, Train Loss: 0.08404, Val Loss: 1.79132, Learning Rate:0.00097, Time: 0:57:43 \n",
      "total_batch:  3470, Train Loss: 0.05160, Val Loss: 1.77466, Learning Rate:0.00097, Time: 0:57:53 \n",
      "total_batch:  3480, Train Loss: 0.06582, Val Loss: 1.79218, Learning Rate:0.00097, Time: 0:58:04 \n",
      "total_batch:  3490, Train Loss: 0.05649, Val Loss: 1.76521, Learning Rate:0.00097, Time: 0:58:13 \n",
      "total_batch:  3500, Train Loss: 0.04804, Val Loss: 1.78343, Learning Rate:0.00097, Time: 0:58:22 \n",
      "total_batch:  3510, Train Loss: 0.07292, Val Loss: 1.79728, Learning Rate:0.00092, Time: 0:58:32 \n",
      "total_batch:  3520, Train Loss: 0.07521, Val Loss: 1.78608, Learning Rate:0.00092, Time: 0:58:41 \n",
      "total_batch:  3530, Train Loss: 0.04924, Val Loss: 1.79164, Learning Rate:0.00092, Time: 0:58:52 \n",
      "total_batch:  3540, Train Loss: 0.06125, Val Loss: 1.78366, Learning Rate:0.00092, Time: 0:59:02 \n",
      "total_batch:  3550, Train Loss: 0.06301, Val Loss: 1.77764, Learning Rate:0.00092, Time: 0:59:12 \n",
      "total_batch:  3560, Train Loss: 0.04724, Val Loss: 1.80477, Learning Rate:0.00092, Time: 0:59:22 \n",
      "total_batch:  3570, Train Loss: 0.06277, Val Loss: 1.79061, Learning Rate:0.00092, Time: 0:59:32 \n",
      "total_batch:  3580, Train Loss: 0.04989, Val Loss: 1.78606, Learning Rate:0.00092, Time: 0:59:42 \n",
      "Epoch: 27\n",
      "total_batch:  3590, Train Loss: 0.05960, Val Loss: 1.79498, Learning Rate:0.00092, Time: 0:59:51 \n",
      "total_batch:  3600, Train Loss: 0.05230, Val Loss: 1.81315, Learning Rate:0.00092, Time: 1:00:01 \n",
      "total_batch:  3610, Train Loss: 0.06967, Val Loss: 1.81307, Learning Rate:0.00092, Time: 1:00:11 \n",
      "total_batch:  3620, Train Loss: 0.06653, Val Loss: 1.81336, Learning Rate:0.00092, Time: 1:00:21 \n",
      "total_batch:  3630, Train Loss: 0.05777, Val Loss: 1.79888, Learning Rate:0.00092, Time: 1:00:31 \n",
      "total_batch:  3640, Train Loss: 0.05660, Val Loss: 1.80988, Learning Rate:0.00092, Time: 1:00:40 \n",
      "total_batch:  3650, Train Loss: 0.05876, Val Loss: 1.83444, Learning Rate:0.00092, Time: 1:00:51 \n",
      "total_batch:  3660, Train Loss: 0.06334, Val Loss: 1.81507, Learning Rate:0.00088, Time: 1:01:01 \n",
      "total_batch:  3670, Train Loss: 0.06306, Val Loss: 1.82223, Learning Rate:0.00088, Time: 1:01:11 \n",
      "total_batch:  3680, Train Loss: 0.05947, Val Loss: 1.81253, Learning Rate:0.00088, Time: 1:01:21 \n",
      "total_batch:  3690, Train Loss: 0.04097, Val Loss: 1.80856, Learning Rate:0.00088, Time: 1:01:31 \n",
      "total_batch:  3700, Train Loss: 0.04711, Val Loss: 1.83302, Learning Rate:0.00088, Time: 1:01:41 \n",
      "total_batch:  3710, Train Loss: 0.03837, Val Loss: 1.82497, Learning Rate:0.00088, Time: 1:01:51 \n",
      "total_batch:  3720, Train Loss: 0.05501, Val Loss: 1.81816, Learning Rate:0.00088, Time: 1:02:01 \n",
      "Epoch: 28\n",
      "total_batch:  3730, Train Loss: 0.07959, Val Loss: 1.83766, Learning Rate:0.00088, Time: 1:02:10 \n",
      "total_batch:  3740, Train Loss: 0.04887, Val Loss: 1.84621, Learning Rate:0.00088, Time: 1:02:21 \n",
      "total_batch:  3750, Train Loss: 0.05718, Val Loss: 1.85431, Learning Rate:0.00088, Time: 1:02:31 \n",
      "total_batch:  3760, Train Loss: 0.05252, Val Loss: 1.83740, Learning Rate:0.00088, Time: 1:02:40 \n",
      "total_batch:  3770, Train Loss: 0.04804, Val Loss: 1.82911, Learning Rate:0.00088, Time: 1:02:50 \n",
      "total_batch:  3780, Train Loss: 0.04044, Val Loss: 1.83389, Learning Rate:0.00088, Time: 1:03:00 \n",
      "total_batch:  3790, Train Loss: 0.05267, Val Loss: 1.85871, Learning Rate:0.00088, Time: 1:03:09 \n",
      "total_batch:  3800, Train Loss: 0.04950, Val Loss: 1.83982, Learning Rate:0.00088, Time: 1:03:20 \n",
      "total_batch:  3810, Train Loss: 0.04256, Val Loss: 1.84268, Learning Rate:0.00083, Time: 1:03:30 \n",
      "total_batch:  3820, Train Loss: 0.06219, Val Loss: 1.83943, Learning Rate:0.00083, Time: 1:03:40 \n",
      "total_batch:  3830, Train Loss: 0.06515, Val Loss: 1.83494, Learning Rate:0.00083, Time: 1:03:50 \n",
      "total_batch:  3840, Train Loss: 0.06013, Val Loss: 1.84990, Learning Rate:0.00083, Time: 1:04:00 \n",
      "total_batch:  3850, Train Loss: 0.05855, Val Loss: 1.86183, Learning Rate:0.00083, Time: 1:04:10 \n",
      "total_batch:  3860, Train Loss: 0.04617, Val Loss: 1.84542, Learning Rate:0.00083, Time: 1:04:19 \n",
      "Epoch: 29\n",
      "total_batch:  3870, Train Loss: 0.04454, Val Loss: 1.85400, Learning Rate:0.00083, Time: 1:04:30 \n",
      "total_batch:  3880, Train Loss: 0.03492, Val Loss: 1.87232, Learning Rate:0.00083, Time: 1:04:40 \n",
      "total_batch:  3890, Train Loss: 0.04781, Val Loss: 1.88381, Learning Rate:0.00083, Time: 1:04:49 \n",
      "total_batch:  3900, Train Loss: 0.04194, Val Loss: 1.86806, Learning Rate:0.00083, Time: 1:04:59 \n",
      "total_batch:  3910, Train Loss: 0.03778, Val Loss: 1.86133, Learning Rate:0.00083, Time: 1:05:08 \n",
      "total_batch:  3920, Train Loss: 0.05096, Val Loss: 1.87262, Learning Rate:0.00083, Time: 1:05:18 \n",
      "total_batch:  3930, Train Loss: 0.04587, Val Loss: 1.88240, Learning Rate:0.00083, Time: 1:05:28 \n",
      "total_batch:  3940, Train Loss: 0.04080, Val Loss: 1.87418, Learning Rate:0.00083, Time: 1:05:37 \n",
      "total_batch:  3950, Train Loss: 0.03614, Val Loss: 1.87892, Learning Rate:0.00083, Time: 1:05:48 \n",
      "total_batch:  3960, Train Loss: 0.04695, Val Loss: 1.87728, Learning Rate:0.00083, Time: 1:05:58 \n",
      "total_batch:  3970, Train Loss: 0.04939, Val Loss: 1.87631, Learning Rate:0.00079, Time: 1:06:08 \n",
      "total_batch:  3980, Train Loss: 0.03076, Val Loss: 1.88901, Learning Rate:0.00079, Time: 1:06:19 \n",
      "total_batch:  3990, Train Loss: 0.03790, Val Loss: 1.88924, Learning Rate:0.00079, Time: 1:06:28 \n",
      "total_batch:  4000, Train Loss: 0.04499, Val Loss: 1.88062, Learning Rate:0.00079, Time: 1:06:38 \n",
      "Epoch: 30\n",
      "total_batch:  4010, Train Loss: 0.04354, Val Loss: 1.89107, Learning Rate:0.00079, Time: 1:06:48 \n",
      "total_batch:  4020, Train Loss: 0.04121, Val Loss: 1.90205, Learning Rate:0.00079, Time: 1:06:59 \n",
      "total_batch:  4030, Train Loss: 0.04134, Val Loss: 1.91353, Learning Rate:0.00079, Time: 1:07:08 \n",
      "total_batch:  4040, Train Loss: 0.03914, Val Loss: 1.89887, Learning Rate:0.00079, Time: 1:07:17 \n",
      "total_batch:  4050, Train Loss: 0.05108, Val Loss: 1.88740, Learning Rate:0.00079, Time: 1:07:27 \n",
      "total_batch:  4060, Train Loss: 0.04864, Val Loss: 1.90463, Learning Rate:0.00079, Time: 1:07:36 \n",
      "total_batch:  4070, Train Loss: 0.03606, Val Loss: 1.92396, Learning Rate:0.00079, Time: 1:07:46 \n",
      "total_batch:  4080, Train Loss: 0.04385, Val Loss: 1.90651, Learning Rate:0.00079, Time: 1:07:57 \n",
      "total_batch:  4090, Train Loss: 0.03623, Val Loss: 1.91203, Learning Rate:0.00079, Time: 1:08:07 \n",
      "total_batch:  4100, Train Loss: 0.04554, Val Loss: 1.90193, Learning Rate:0.00079, Time: 1:08:17 \n",
      "total_batch:  4110, Train Loss: 0.04142, Val Loss: 1.90588, Learning Rate:0.00079, Time: 1:08:27 \n",
      "total_batch:  4120, Train Loss: 0.03447, Val Loss: 1.91241, Learning Rate:0.00075, Time: 1:08:38 \n",
      "total_batch:  4130, Train Loss: 0.04232, Val Loss: 1.91816, Learning Rate:0.00075, Time: 1:08:47 \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "import sys\n",
    "\n",
    "base_dir = '../00-data/beam_search_data'\n",
    "source_train_dir = os.path.join(base_dir, 'source_train')\n",
    "source_test_dir = os.path.join(base_dir, 'source_test')\n",
    "target_train_dir = os.path.join(base_dir, 'target_train')\n",
    "target_test_dir = os.path.join(base_dir, 'target_test')\n",
    "source_vocab_dir = os.path.join(base_dir, 'vocab')\n",
    "target_vocab_dir = os.path.join(base_dir, 'vocab')\n",
    "\n",
    "\n",
    "save_dir = '../02-checkpoints/03-0322'\n",
    "save_path = os.path.join(save_dir, 'best_validation')   # 最佳验证结果保存路径\n",
    "\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "\n",
    "def evaluate(sess, source_test, target_test,total_batch):\n",
    "    \"\"\"评估在某一数据上的准确率和损失\"\"\"\n",
    "    data_len = len(source_test)\n",
    "    batch_eval = batch_iter(source_test, target_test, config.batch_size, source_letter_to_id['<PAD>'], target_letter_to_id['<PAD>'])\n",
    "    total_loss = 0.\n",
    "    for source_train_batch, len_source_train_batch, target_train_batch, len_target_train_batch in batch_eval:\n",
    "        batch_len = len(source_train_batch)\n",
    "        feed_dict = {\n",
    "            model.source: source_train_batch,\n",
    "            model.target: target_train_batch,\n",
    "            model.source_sequence_length: len_source_train_batch,\n",
    "            model.target_sequence_length: len_target_train_batch,\n",
    "            model.global_step: total_batch\n",
    "            # model.is_train: True\n",
    "        }\n",
    "        loss = sess.run([model.loss], feed_dict=feed_dict)\n",
    "        loss = np.mean(loss)\n",
    "        total_loss += loss * batch_len\n",
    "    return total_loss / data_len\n",
    "\n",
    "\n",
    "def train():\n",
    "    # 配置 Saver\n",
    "    saver = tf.train.Saver()\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # 载入训练集与验证集\n",
    "    print(\"Loading data...\")\n",
    "    source_train, len_source_train = process_file(source_train_dir, source_letter_to_id)\n",
    "    source_test, len_source_test = process_file(source_test_dir, source_letter_to_id)\n",
    "    target_train, len_target_train = process_file(target_train_dir, target_letter_to_id)\n",
    "    target_test, len_target_test = process_file(target_test_dir, target_letter_to_id)\n",
    "\n",
    "    # 创建session\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    print('Training and evaluating...')\n",
    "    start_time = time.time()\n",
    "    total_batch = 0              # 总批次\n",
    "    best_val_loss = sys.float_info.max           # 最佳验证集效果\n",
    "    last_improved = 0            # 记录上一次提升批次\n",
    "    require_improvement = 100   # 如果超过1000轮未提升，提前结束训练\n",
    "\n",
    "    flag = False\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch:', epoch + 1)\n",
    "        batch_train = batch_iter(source_train, target_train, config.batch_size, source_letter_to_id['<PAD>'], target_letter_to_id['<PAD>'])\n",
    "        for source_train_batch, len_source_train_batch, target_train_batch, len_target_train_batch in batch_train:\n",
    "            feed_dict = {\n",
    "                model.source: source_train_batch,\n",
    "                model.target: target_train_batch,\n",
    "                model.source_sequence_length: len_source_train_batch,\n",
    "                model.target_sequence_length: len_target_train_batch,\n",
    "                model.global_step: total_batch\n",
    "                # model.is_train: True\n",
    "            }\n",
    "\n",
    "            if total_batch % config.print_per_batch == 0:\n",
    "                # 每多少轮次输出在训练集上的性能\n",
    "                loss_train = np.mean(session.run([model.loss], feed_dict=feed_dict))\n",
    "                loss_val = evaluate(session, source_test, target_test,total_batch)\n",
    "\n",
    "                if loss_val < best_val_loss:\n",
    "                    # 保存最好结果\n",
    "                    best_val_loss = loss_val\n",
    "                    last_improved = total_batch\n",
    "                    saver.save(sess=session, save_path=save_path)\n",
    "                    improved_str = '*'\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                lr = session.run([model.learning_rate], feed_dict={model.global_step: total_batch})\n",
    "                #print(lr)\n",
    "                msg = 'total_batch: {0:>5}, Train Loss: {1:.5f}, Val Loss: {2:.5f}, Learning Rate:{3:.5f}, Time: {4} {5}'\n",
    "                print(msg.format(total_batch, loss_train, loss_val, lr[0],time_dif, improved_str))\n",
    "\n",
    "            session.run(model.train_op, feed_dict=feed_dict)  # 运行优化\n",
    "            total_batch += 1\n",
    "\n",
    "#             if total_batch - last_improved > require_improvement:\n",
    "#                 # 验证集正确率长期不提升，提前结束训练\n",
    "#                 print(\"No optimization for a long time, auto-stopping...\")\n",
    "#                 flag = True\n",
    "#                 break  # 跳出循环\n",
    "#         if flag:  # 同上\n",
    "#             break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.reset_default_graph()\n",
    "    print('Configuring model...')\n",
    "    config = ModelConfig()\n",
    "    source_letter_to_id, source_id_to_letter = read_vocab(source_vocab_dir)\n",
    "    target_letter_to_id, target_id_to_letter = read_vocab(target_vocab_dir)\n",
    "    \n",
    "    config.source_vocab_size = len(source_letter_to_id)\n",
    "    config.target_vocab_size = len(target_letter_to_id)\n",
    "    config.target_letter_to_id = target_letter_to_id\n",
    "\n",
    "    model = Model(config)\n",
    "\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-greece",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "biological-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predict_input(test_sentences,letter_to_id):\n",
    "    test_vector = []\n",
    "    test_text = \"\"\n",
    "    for sentence in test_sentences:\n",
    "        line = ' '.join(re.split(' |\\t|\\v|\\n',sentence))        \n",
    "        line = re.split('([: ,.(){}\\[\\]=])',line)        \n",
    "        line = list(filter(lambda x: x!=' 'and x!='',line))\n",
    "        \n",
    "        test_vector.append(letter_to_id[\"<GO>\"])\n",
    "        for word in line:\n",
    "            test_vector.append(letter_to_id.get(word,letter_to_id[\"<UNK>\"]))\n",
    "        test_vector.append(letter_to_id[\"<EOS>\"])\n",
    "    \n",
    "    return [test_vector],[len(test_vector)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "rapid-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    print(\"Testing...\")\n",
    "    test_sentences = [\"logits = tf.matmul(output,self.softmax_weight) + self.softmax_bias\",\n",
    "                  \"loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.reshape(trg_label,[-1]),logits=logits)\",\n",
    "                 \"label_weights = tf.sequence_mask(trg_size,maxlen=tf.shape(trg_label)[1],dtype=tf.float32)\",\n",
    "                 \"label_weights = tf.reshape(label_weights,[-1])\"]\n",
    "    \n",
    "    source_test, len_source_test = process_predict_input(test_sentences, source_letter_to_id)\n",
    "\n",
    "    print(source_test)\n",
    "    \n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess=session, save_path=save_path)  # 读取保存的模型\n",
    "\n",
    "    feed_dict = {\n",
    "        model.source: source_test,\n",
    "        model.source_sequence_length: len_source_test,\n",
    "        # model.target_sequence_length: len_source_test,\n",
    "        # model.is_train: False\n",
    "    }\n",
    "    result_ids = session.run(model.result_ids, feed_dict=feed_dict)\n",
    "    # print('输出: {}'.format(\"\".join([target_id_to_letter[i] for i in result_ids])))\n",
    "    # beam search输出\n",
    "    print(result_ids.shape)\n",
    "    result_ids = np.squeeze(result_ids)\n",
    "    print(result_ids.shape)\n",
    "    for x in result_ids:\n",
    "        res = []\n",
    "        for i in x:\n",
    "            if target_id_to_letter[i] == \"<EOS>\":\n",
    "                break\n",
    "            res.append(target_id_to_letter[i])\n",
    "        print(\" \".join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "rental-regard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "[[1, 99, 4, 12, 5, 87, 7, 174, 8, 13, 5, 0, 6, 27, 13, 5, 0, 2, 1, 120, 4, 12, 5, 38, 5, 949, 7, 109, 4, 12, 5, 64, 7, 0, 8, 10, 233, 11, 6, 8, 99, 4, 99, 6, 2, 1, 0, 4, 12, 5, 2449, 7, 0, 8, 1236, 4, 12, 5, 24, 7, 0, 6, 10, 35, 11, 8, 42, 4, 12, 5, 51, 6, 2, 1, 0, 4, 12, 5, 64, 7, 0, 8, 10, 233, 11, 6, 2]]\n",
      "INFO:tensorflow:Restoring parameters from ../02-checkpoints/03-0322/best_validation\n",
      "(1, 5, 18)\n",
      "(5, 18)\n",
      "<UNK> = tf . reshape ( logits , [ ] )\n",
      "<UNK> = tf . reshape ( <UNK> , [ ] )\n",
      "<UNK> = tf . image . <UNK> ( )\n",
      "<UNK> = tf . nn . sparse_softmax_cross_entropy_with_logits ( logits = , labels = , name = )\n",
      "<UNK> = tf . nn . sparse_softmax_cross_entropy_with_logits ( logits = , labels = )\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-mentor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
