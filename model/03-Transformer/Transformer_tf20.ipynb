{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import io\n",
    "import re\n",
    "import random\n",
    "import collections\n",
    "import codecs\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(line):\n",
    "    line = ' '.join(re.split(' |\\t|\\v|\\n',line))        \n",
    "    line = re.split('([: ,.(){}\\[\\]=])',line)        \n",
    "    line = list(filter(lambda x: x!=' 'and x!='',line))\n",
    "    \n",
    "    new_line = '<start> ' + ' '.join(line) + ' <end>'\n",
    "    return new_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path,num_examples,rand_max=15,duplicate=2):\n",
    "    input_data = []\n",
    "    output_data = []\n",
    "    \n",
    "    lines = io.open(path,encoding='utf-8').read().strip().split('\\n')\n",
    "    if num_examples == None:\n",
    "        num_examples = len(lines)\n",
    "\n",
    "    for i in range(0,num_examples):\n",
    "        \n",
    "        rand_nums = set(random.randint(1,rand_max) for _ in range(duplicate))\n",
    "        for rand_num in rand_nums:\n",
    "            data = ''\n",
    "            for j in range(i - rand_num,i):\n",
    "                line = preprocess_sentence(lines[j].strip()) + ' '\n",
    "                data += line\n",
    "            input_data.append(data.strip())\n",
    "            output_data.append(preprocess_sentence(lines[i].strip()))\n",
    "        \n",
    "    return input_data,output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"01-data/1_final_data_0219.py\"\n",
    "input_data,output_data = create_dataset(path,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def get_num_words(lang):\n",
    "    #获取词典大小,暂时用之前训的模型\n",
    "    #lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='',lower=False)\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    num_words = int(len(lang_tokenizer.word_index))\n",
    "    return num_words\n",
    "    \n",
    "def tokenize(lang):\n",
    "    num_words = get_num_words(lang)\n",
    "#     lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words,\n",
    "#                                                            oov_token='<unk>',filters='',lower=False)\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words,\n",
    "                                                           oov_token='<unk>',filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "    \n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "def load_dataset(path,num_examples=None):\n",
    "    inp_lang,targ_lang = create_dataset(path, num_examples)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试实验不同大小的数据集\n",
    "num_examples = None\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path, num_examples)\n",
    "\n",
    "# 计算目标张量的最大长度 （max_length）\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11567"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inp_lang.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74660 74660 755 755\n"
     ]
    }
   ],
   "source": [
    "\"\"\"目前先不采用\"\"\"\n",
    "# 采用 90 - 10 的比例切分训练集和验证集\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.01)\n",
    "\n",
    "# 显示长度\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建一个tf.data的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 128\n",
    "units = 256\n",
    "input_vocab_size = len(inp_lang.word_index)+1\n",
    "target_vocab_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 606), dtype=int32, numpy=\n",
       " array([[   2,  199,    4, ...,    0,    0,    0],\n",
       "        [   2,   16,   12, ...,    0,    0,    0],\n",
       "        [   2,   23,  133, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   2,  370,    8, ...,    0,    0,    0],\n",
       "        [   2, 4073,    5, ...,    0,    0,    0],\n",
       "        [   2,   26,   13, ...,    0,    0,    0]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(64, 245), dtype=int32, numpy=\n",
       " array([[    2,    16,   117, ...,     0,     0,     0],\n",
       "        [    2,    14, 10326, ...,     0,     0,     0],\n",
       "        [    2,   879,     4, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2,    13,     5, ...,     0,     0,     0],\n",
       "        [    2,    16,    13, ...,     0,     0,     0],\n",
       "        [    2,    14,  2909, ...,     0,     0,     0]], dtype=int32)>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch, example_target_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 位置编码（Positional encoding）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # 将 sin 应用于数组中的偶数索引（indices）；2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # 将 cos 应用于数组中的奇数索引；2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 遮挡（Masking）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # 添加额外的维度来将填充加到\n",
    "    # 注意力对数（logits）。\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按比缩放的点积注意力（Scaled dot product attention）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"计算注意力权重。\n",
    "    q, k, v 必须具有匹配的前置维度。\n",
    "    k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。\n",
    "    虽然 mask 根据其类型（填充或前瞻）有不同的形状，\n",
    "    但是 mask 必须能进行广播转换以便求和。\n",
    "\n",
    "    参数:\n",
    "    q: 请求的形状 == (..., seq_len_q, depth)\n",
    "    k: 主键的形状 == (..., seq_len_k, depth)\n",
    "    v: 数值的形状 == (..., seq_len_v, depth_v)\n",
    "    mask: Float 张量，其形状能转换成\n",
    "          (..., seq_len_q, seq_len_k)。默认为None。\n",
    "\n",
    "    返回值:\n",
    "    输出，注意力权重\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # 缩放 matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # 将 mask 加入到缩放的张量上。\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax 在最后一个轴（seq_len_k）上归一化，因此分数\n",
    "    # 相加等于1。\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print ('Attention weights are:')\n",
    "    print (temp_attn)\n",
    "    print ('Output is:')\n",
    "    print (temp_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多头注意力（Multi-head attention）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"分拆最后一个维度到 (num_heads, depth).\n",
    "        转置结果使得形状为 (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 点式前馈网络（Point wise feed forward network）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码与解码（Encoder and decoder）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解码器层（Decoder layer）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码器（Encoder）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # 将嵌入和位置编码相加。\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解码器（Decoder）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置超参数（hyperparameters）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 6\n",
    "d_model = 256\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器（Optimizer）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数与指标（Loss and metrics）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练与检查点（Training and checkpointing）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # 编码器填充遮挡\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # 在解码器的第二个注意力模块使用。\n",
    "    # 该填充遮挡用于遮挡编码器的输出。\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # 在解码器的第一个注意力模块使用。\n",
    "    # 用于填充（pad）和遮挡（mask）解码器获取到的输入的后续标记（future tokens）。\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"03-checkpoints/03-0402_01\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# 如果检查点存在，则恢复最新的检查点。\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该 @tf.function 将追踪-编译 train_step 到 TF 图中，以便更快地\n",
    "# 执行。该函数专用于参数张量的精确形状。为了避免由于可变序列长度或可变\n",
    "# 批次大小（最后一批次较小）导致的再追踪，使用 input_signature 指定\n",
    "# 更多的通用形状。\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                     True, \n",
    "                                     enc_padding_mask, \n",
    "                                     combined_mask, \n",
    "                                     dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.3642 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 0.3831 Accuracy 0.0010\n",
      "Epoch 1 Batch 100 Loss 0.3719 Accuracy 0.0025\n",
      "Epoch 1 Batch 150 Loss 0.3619 Accuracy 0.0030\n",
      "Epoch 1 Batch 200 Loss 0.3499 Accuracy 0.0032\n",
      "Epoch 1 Batch 250 Loss 0.3364 Accuracy 0.0034\n",
      "Epoch 1 Batch 300 Loss 0.3215 Accuracy 0.0042\n",
      "Epoch 1 Batch 350 Loss 0.3057 Accuracy 0.0054\n",
      "Epoch 1 Batch 400 Loss 0.2905 Accuracy 0.0066\n",
      "Epoch 1 Batch 450 Loss 0.2772 Accuracy 0.0078\n",
      "Epoch 1 Batch 500 Loss 0.2651 Accuracy 0.0088\n",
      "Epoch 1 Batch 550 Loss 0.2540 Accuracy 0.0098\n",
      "Epoch 1 Batch 600 Loss 0.2443 Accuracy 0.0107\n",
      "Epoch 1 Batch 650 Loss 0.2358 Accuracy 0.0115\n",
      "Epoch 1 Batch 700 Loss 0.2279 Accuracy 0.0121\n",
      "Epoch 1 Batch 750 Loss 0.2210 Accuracy 0.0128\n",
      "Epoch 1 Batch 800 Loss 0.2148 Accuracy 0.0133\n",
      "Epoch 1 Batch 850 Loss 0.2089 Accuracy 0.0138\n",
      "Epoch 1 Batch 900 Loss 0.2036 Accuracy 0.0143\n",
      "Epoch 1 Batch 950 Loss 0.1989 Accuracy 0.0148\n",
      "Epoch 1 Batch 1000 Loss 0.1944 Accuracy 0.0152\n",
      "Epoch 1 Batch 1150 Loss 0.1831 Accuracy 0.0163\n",
      "Epoch 1 Loss 0.1814 Accuracy 0.0164\n",
      "Time taken for 1 epoch: 1237.675198316574 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.0872 Accuracy 0.0229\n",
      "Epoch 2 Batch 50 Loss 0.1022 Accuracy 0.0242\n",
      "Epoch 2 Batch 100 Loss 0.1015 Accuracy 0.0244\n",
      "Epoch 2 Batch 150 Loss 0.1005 Accuracy 0.0244\n",
      "Epoch 2 Batch 200 Loss 0.0989 Accuracy 0.0245\n",
      "Epoch 2 Batch 250 Loss 0.0983 Accuracy 0.0247\n",
      "Epoch 2 Batch 300 Loss 0.0976 Accuracy 0.0248\n",
      "Epoch 2 Batch 350 Loss 0.0972 Accuracy 0.0249\n",
      "Epoch 2 Batch 400 Loss 0.0963 Accuracy 0.0248\n",
      "Epoch 2 Batch 450 Loss 0.0958 Accuracy 0.0249\n",
      "Epoch 2 Batch 500 Loss 0.0957 Accuracy 0.0250\n",
      "Epoch 2 Batch 550 Loss 0.0954 Accuracy 0.0250\n",
      "Epoch 2 Batch 600 Loss 0.0948 Accuracy 0.0251\n",
      "Epoch 2 Batch 650 Loss 0.0945 Accuracy 0.0252\n",
      "Epoch 2 Batch 700 Loss 0.0940 Accuracy 0.0253\n",
      "Epoch 2 Batch 750 Loss 0.0934 Accuracy 0.0253\n",
      "Epoch 2 Batch 800 Loss 0.0930 Accuracy 0.0253\n",
      "Epoch 2 Batch 850 Loss 0.0926 Accuracy 0.0254\n",
      "Epoch 2 Batch 900 Loss 0.0921 Accuracy 0.0254\n",
      "Epoch 2 Batch 950 Loss 0.0916 Accuracy 0.0255\n",
      "Epoch 2 Batch 1000 Loss 0.0912 Accuracy 0.0255\n",
      "Epoch 2 Batch 1050 Loss 0.0909 Accuracy 0.0256\n",
      "Epoch 2 Batch 1100 Loss 0.0905 Accuracy 0.0256\n",
      "Epoch 2 Batch 1150 Loss 0.0901 Accuracy 0.0257\n",
      "Epoch 2 Loss 0.0899 Accuracy 0.0257\n",
      "Time taken for 1 epoch: 1218.6869478225708 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.0774 Accuracy 0.0260\n",
      "Epoch 3 Batch 50 Loss 0.0800 Accuracy 0.0272\n",
      "Epoch 3 Batch 100 Loss 0.0777 Accuracy 0.0269\n",
      "Epoch 3 Batch 150 Loss 0.0776 Accuracy 0.0270\n",
      "Epoch 3 Batch 200 Loss 0.0777 Accuracy 0.0272\n",
      "Epoch 3 Batch 250 Loss 0.0776 Accuracy 0.0273\n",
      "Epoch 3 Batch 300 Loss 0.0777 Accuracy 0.0273\n",
      "Epoch 3 Batch 350 Loss 0.0774 Accuracy 0.0272\n",
      "Epoch 3 Batch 400 Loss 0.0772 Accuracy 0.0274\n",
      "Epoch 3 Batch 450 Loss 0.0770 Accuracy 0.0273\n",
      "Epoch 3 Batch 500 Loss 0.0769 Accuracy 0.0273\n",
      "Epoch 3 Batch 550 Loss 0.0770 Accuracy 0.0274\n",
      "Epoch 3 Batch 600 Loss 0.0768 Accuracy 0.0274\n",
      "Epoch 3 Batch 650 Loss 0.0766 Accuracy 0.0275\n",
      "Epoch 3 Batch 700 Loss 0.0763 Accuracy 0.0274\n",
      "Epoch 3 Batch 750 Loss 0.0763 Accuracy 0.0275\n",
      "Epoch 3 Batch 800 Loss 0.0761 Accuracy 0.0275\n",
      "Epoch 3 Batch 850 Loss 0.0758 Accuracy 0.0275\n",
      "Epoch 3 Batch 900 Loss 0.0757 Accuracy 0.0275\n",
      "Epoch 3 Batch 950 Loss 0.0756 Accuracy 0.0275\n",
      "Epoch 3 Batch 1000 Loss 0.0754 Accuracy 0.0275\n",
      "Epoch 3 Batch 1050 Loss 0.0751 Accuracy 0.0275\n",
      "Epoch 3 Batch 1100 Loss 0.0750 Accuracy 0.0275\n",
      "Epoch 3 Batch 1150 Loss 0.0751 Accuracy 0.0276\n",
      "Epoch 3 Loss 0.0750 Accuracy 0.0276\n",
      "Time taken for 1 epoch: 1218.3699414730072 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0800 Accuracy 0.0298\n",
      "Epoch 4 Batch 50 Loss 0.0690 Accuracy 0.0284\n",
      "Epoch 4 Batch 100 Loss 0.0681 Accuracy 0.0282\n",
      "Epoch 4 Batch 150 Loss 0.0687 Accuracy 0.0284\n",
      "Epoch 4 Batch 200 Loss 0.0685 Accuracy 0.0285\n",
      "Epoch 4 Batch 250 Loss 0.0685 Accuracy 0.0285\n",
      "Epoch 4 Batch 300 Loss 0.0686 Accuracy 0.0285\n",
      "Epoch 4 Batch 350 Loss 0.0687 Accuracy 0.0284\n",
      "Epoch 4 Batch 400 Loss 0.0687 Accuracy 0.0284\n",
      "Epoch 4 Batch 450 Loss 0.0687 Accuracy 0.0283\n",
      "Epoch 4 Batch 500 Loss 0.0689 Accuracy 0.0284\n",
      "Epoch 4 Batch 550 Loss 0.0690 Accuracy 0.0284\n",
      "Epoch 4 Batch 600 Loss 0.0689 Accuracy 0.0283\n",
      "Epoch 4 Batch 650 Loss 0.0686 Accuracy 0.0283\n",
      "Epoch 4 Batch 700 Loss 0.0687 Accuracy 0.0283\n",
      "Epoch 4 Batch 750 Loss 0.0686 Accuracy 0.0283\n",
      "Epoch 4 Batch 800 Loss 0.0686 Accuracy 0.0283\n",
      "Epoch 4 Batch 850 Loss 0.0685 Accuracy 0.0283\n",
      "Epoch 4 Batch 900 Loss 0.0683 Accuracy 0.0283\n",
      "Epoch 4 Batch 950 Loss 0.0681 Accuracy 0.0283\n",
      "Epoch 4 Batch 1000 Loss 0.0680 Accuracy 0.0283\n",
      "Epoch 4 Batch 1050 Loss 0.0678 Accuracy 0.0284\n",
      "Epoch 4 Batch 1100 Loss 0.0676 Accuracy 0.0284\n",
      "Epoch 4 Batch 1150 Loss 0.0675 Accuracy 0.0284\n",
      "Epoch 4 Loss 0.0674 Accuracy 0.0284\n",
      "Time taken for 1 epoch: 1213.5650718212128 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0556 Accuracy 0.0292\n",
      "Epoch 5 Batch 50 Loss 0.0590 Accuracy 0.0291\n",
      "Epoch 5 Batch 100 Loss 0.0588 Accuracy 0.0292\n",
      "Epoch 5 Batch 150 Loss 0.0587 Accuracy 0.0293\n",
      "Epoch 5 Batch 200 Loss 0.0589 Accuracy 0.0293\n",
      "Epoch 5 Batch 250 Loss 0.0590 Accuracy 0.0293\n",
      "Epoch 5 Batch 300 Loss 0.0592 Accuracy 0.0293\n",
      "Epoch 5 Batch 350 Loss 0.0593 Accuracy 0.0293\n",
      "Epoch 5 Batch 400 Loss 0.0593 Accuracy 0.0294\n",
      "Epoch 5 Batch 450 Loss 0.0594 Accuracy 0.0294\n",
      "Epoch 5 Batch 500 Loss 0.0593 Accuracy 0.0295\n",
      "Epoch 5 Batch 550 Loss 0.0593 Accuracy 0.0295\n",
      "Epoch 5 Batch 600 Loss 0.0594 Accuracy 0.0295\n",
      "Epoch 5 Batch 650 Loss 0.0592 Accuracy 0.0295\n",
      "Epoch 5 Batch 700 Loss 0.0592 Accuracy 0.0295\n",
      "Epoch 5 Batch 750 Loss 0.0592 Accuracy 0.0295\n",
      "Epoch 5 Batch 800 Loss 0.0593 Accuracy 0.0295\n",
      "Epoch 5 Batch 850 Loss 0.0593 Accuracy 0.0295\n",
      "Epoch 5 Batch 900 Loss 0.0593 Accuracy 0.0296\n",
      "Epoch 5 Batch 950 Loss 0.0592 Accuracy 0.0296\n",
      "Epoch 5 Batch 1000 Loss 0.0592 Accuracy 0.0296\n",
      "Epoch 5 Batch 1050 Loss 0.0592 Accuracy 0.0296\n",
      "Epoch 5 Batch 1100 Loss 0.0591 Accuracy 0.0297\n",
      "Epoch 5 Batch 1150 Loss 0.0589 Accuracy 0.0297\n",
      "Saving checkpoint for epoch 5 at 03-checkpoints/03-0402_01/ckpt-1\n",
      "Epoch 5 Loss 0.0589 Accuracy 0.0297\n",
      "Time taken for 1 epoch: 1204.9799973964691 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0594 Accuracy 0.0304\n",
      "Epoch 6 Batch 50 Loss 0.0523 Accuracy 0.0307\n",
      "Epoch 6 Batch 100 Loss 0.0526 Accuracy 0.0308\n",
      "Epoch 6 Batch 150 Loss 0.0523 Accuracy 0.0308\n",
      "Epoch 6 Batch 200 Loss 0.0524 Accuracy 0.0307\n",
      "Epoch 6 Batch 250 Loss 0.0525 Accuracy 0.0306\n",
      "Epoch 6 Batch 300 Loss 0.0526 Accuracy 0.0306\n",
      "Epoch 6 Batch 350 Loss 0.0526 Accuracy 0.0305\n",
      "Epoch 6 Batch 400 Loss 0.0526 Accuracy 0.0306\n",
      "Epoch 6 Batch 450 Loss 0.0526 Accuracy 0.0306\n",
      "Epoch 6 Batch 500 Loss 0.0528 Accuracy 0.0307\n",
      "Epoch 6 Batch 550 Loss 0.0528 Accuracy 0.0307\n",
      "Epoch 6 Batch 600 Loss 0.0528 Accuracy 0.0307\n",
      "Epoch 6 Batch 650 Loss 0.0529 Accuracy 0.0307\n",
      "Epoch 6 Batch 700 Loss 0.0528 Accuracy 0.0307\n",
      "Epoch 6 Batch 750 Loss 0.0529 Accuracy 0.0307\n",
      "Epoch 6 Batch 800 Loss 0.0530 Accuracy 0.0307\n",
      "Epoch 6 Batch 850 Loss 0.0529 Accuracy 0.0307\n",
      "Epoch 6 Batch 900 Loss 0.0530 Accuracy 0.0307\n",
      "Epoch 6 Batch 950 Loss 0.0529 Accuracy 0.0307\n",
      "Epoch 6 Batch 1000 Loss 0.0529 Accuracy 0.0307\n",
      "Epoch 6 Batch 1050 Loss 0.0529 Accuracy 0.0306\n",
      "Epoch 6 Batch 1100 Loss 0.0529 Accuracy 0.0306\n",
      "Epoch 6 Batch 1150 Loss 0.0529 Accuracy 0.0307\n",
      "Epoch 6 Loss 0.0529 Accuracy 0.0307\n",
      "Time taken for 1 epoch: 1200.0119819641113 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0425 Accuracy 0.0297\n",
      "Epoch 7 Batch 50 Loss 0.0467 Accuracy 0.0317\n",
      "Epoch 7 Batch 100 Loss 0.0463 Accuracy 0.0311\n",
      "Epoch 7 Batch 150 Loss 0.0463 Accuracy 0.0313\n",
      "Epoch 7 Batch 200 Loss 0.0476 Accuracy 0.0316\n",
      "Epoch 7 Batch 250 Loss 0.0475 Accuracy 0.0316\n",
      "Epoch 7 Batch 300 Loss 0.0476 Accuracy 0.0315\n",
      "Epoch 7 Batch 350 Loss 0.0478 Accuracy 0.0315\n",
      "Epoch 7 Batch 400 Loss 0.0479 Accuracy 0.0314\n",
      "Epoch 7 Batch 450 Loss 0.0478 Accuracy 0.0314\n",
      "Epoch 7 Batch 500 Loss 0.0479 Accuracy 0.0314\n",
      "Epoch 7 Batch 550 Loss 0.0481 Accuracy 0.0314\n",
      "Epoch 7 Batch 600 Loss 0.0480 Accuracy 0.0314\n",
      "Epoch 7 Batch 650 Loss 0.0481 Accuracy 0.0314\n",
      "Epoch 7 Batch 700 Loss 0.0481 Accuracy 0.0314\n",
      "Epoch 7 Batch 750 Loss 0.0482 Accuracy 0.0315\n",
      "Epoch 7 Batch 800 Loss 0.0483 Accuracy 0.0315\n",
      "Epoch 7 Batch 850 Loss 0.0482 Accuracy 0.0314\n",
      "Epoch 7 Batch 900 Loss 0.0482 Accuracy 0.0314\n",
      "Epoch 7 Batch 950 Loss 0.0483 Accuracy 0.0315\n",
      "Epoch 7 Batch 1000 Loss 0.0483 Accuracy 0.0315\n",
      "Epoch 7 Batch 1050 Loss 0.0483 Accuracy 0.0315\n",
      "Epoch 7 Batch 1100 Loss 0.0482 Accuracy 0.0315\n",
      "Epoch 7 Batch 1150 Loss 0.0483 Accuracy 0.0315\n",
      "Epoch 7 Loss 0.0483 Accuracy 0.0314\n",
      "Time taken for 1 epoch: 1211.2435009479523 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0355 Accuracy 0.0272\n",
      "Epoch 8 Batch 50 Loss 0.0402 Accuracy 0.0323\n",
      "Epoch 8 Batch 100 Loss 0.0414 Accuracy 0.0323\n",
      "Epoch 8 Batch 150 Loss 0.0422 Accuracy 0.0323\n",
      "Epoch 8 Batch 200 Loss 0.0425 Accuracy 0.0321\n",
      "Epoch 8 Batch 250 Loss 0.0432 Accuracy 0.0321\n",
      "Epoch 8 Batch 300 Loss 0.0433 Accuracy 0.0321\n",
      "Epoch 8 Batch 350 Loss 0.0434 Accuracy 0.0321\n",
      "Epoch 8 Batch 400 Loss 0.0435 Accuracy 0.0321\n",
      "Epoch 8 Batch 450 Loss 0.0436 Accuracy 0.0321\n",
      "Epoch 8 Batch 500 Loss 0.0438 Accuracy 0.0321\n",
      "Epoch 8 Batch 550 Loss 0.0439 Accuracy 0.0320\n",
      "Epoch 8 Batch 600 Loss 0.0439 Accuracy 0.0320\n",
      "Epoch 8 Batch 650 Loss 0.0442 Accuracy 0.0320\n",
      "Epoch 8 Batch 700 Loss 0.0443 Accuracy 0.0320\n",
      "Epoch 8 Batch 750 Loss 0.0443 Accuracy 0.0321\n",
      "Epoch 8 Batch 800 Loss 0.0444 Accuracy 0.0320\n",
      "Epoch 8 Batch 850 Loss 0.0445 Accuracy 0.0320\n",
      "Epoch 8 Batch 900 Loss 0.0445 Accuracy 0.0320\n",
      "Epoch 8 Batch 950 Loss 0.0446 Accuracy 0.0320\n",
      "Epoch 8 Batch 1000 Loss 0.0446 Accuracy 0.0320\n",
      "Epoch 8 Batch 1050 Loss 0.0447 Accuracy 0.0320\n",
      "Epoch 8 Batch 1100 Loss 0.0447 Accuracy 0.0320\n",
      "Epoch 8 Batch 1150 Loss 0.0447 Accuracy 0.0321\n",
      "Epoch 8 Loss 0.0447 Accuracy 0.0321\n",
      "Time taken for 1 epoch: 1217.2958555221558 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0365 Accuracy 0.0322\n",
      "Epoch 9 Batch 50 Loss 0.0386 Accuracy 0.0329\n",
      "Epoch 9 Batch 100 Loss 0.0392 Accuracy 0.0323\n",
      "Epoch 9 Batch 150 Loss 0.0397 Accuracy 0.0324\n",
      "Epoch 9 Batch 200 Loss 0.0397 Accuracy 0.0323\n",
      "Epoch 9 Batch 250 Loss 0.0401 Accuracy 0.0324\n",
      "Epoch 9 Batch 300 Loss 0.0406 Accuracy 0.0325\n",
      "Epoch 9 Batch 350 Loss 0.0408 Accuracy 0.0325\n",
      "Epoch 9 Batch 400 Loss 0.0408 Accuracy 0.0325\n",
      "Epoch 9 Batch 450 Loss 0.0408 Accuracy 0.0325\n",
      "Epoch 9 Batch 500 Loss 0.0408 Accuracy 0.0324\n",
      "Epoch 9 Batch 550 Loss 0.0409 Accuracy 0.0325\n",
      "Epoch 9 Batch 600 Loss 0.0409 Accuracy 0.0324\n",
      "Epoch 9 Batch 650 Loss 0.0410 Accuracy 0.0324\n",
      "Epoch 9 Batch 700 Loss 0.0411 Accuracy 0.0325\n",
      "Epoch 9 Batch 750 Loss 0.0412 Accuracy 0.0325\n",
      "Epoch 9 Batch 800 Loss 0.0414 Accuracy 0.0326\n",
      "Epoch 9 Batch 850 Loss 0.0414 Accuracy 0.0326\n",
      "Epoch 9 Batch 900 Loss 0.0415 Accuracy 0.0325\n",
      "Epoch 9 Batch 950 Loss 0.0415 Accuracy 0.0326\n",
      "Epoch 9 Batch 1000 Loss 0.0416 Accuracy 0.0326\n",
      "Epoch 9 Batch 1050 Loss 0.0416 Accuracy 0.0326\n",
      "Epoch 9 Batch 1100 Loss 0.0416 Accuracy 0.0326\n",
      "Epoch 9 Batch 1150 Loss 0.0417 Accuracy 0.0326\n",
      "Epoch 9 Loss 0.0417 Accuracy 0.0326\n",
      "Time taken for 1 epoch: 1216.3152391910553 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0405 Accuracy 0.0336\n",
      "Epoch 10 Batch 50 Loss 0.0370 Accuracy 0.0333\n",
      "Epoch 10 Batch 100 Loss 0.0366 Accuracy 0.0331\n",
      "Epoch 10 Batch 150 Loss 0.0371 Accuracy 0.0332\n",
      "Epoch 10 Batch 200 Loss 0.0374 Accuracy 0.0332\n",
      "Epoch 10 Batch 250 Loss 0.0376 Accuracy 0.0334\n",
      "Epoch 10 Batch 300 Loss 0.0377 Accuracy 0.0333\n",
      "Epoch 10 Batch 350 Loss 0.0378 Accuracy 0.0332\n",
      "Epoch 10 Batch 400 Loss 0.0380 Accuracy 0.0332\n",
      "Epoch 10 Batch 450 Loss 0.0381 Accuracy 0.0333\n",
      "Epoch 10 Batch 500 Loss 0.0381 Accuracy 0.0331\n",
      "Epoch 10 Batch 550 Loss 0.0382 Accuracy 0.0331\n",
      "Epoch 10 Batch 600 Loss 0.0383 Accuracy 0.0332\n",
      "Epoch 10 Batch 650 Loss 0.0384 Accuracy 0.0331\n",
      "Epoch 10 Batch 700 Loss 0.0385 Accuracy 0.0332\n",
      "Epoch 10 Batch 750 Loss 0.0386 Accuracy 0.0331\n",
      "Epoch 10 Batch 800 Loss 0.0387 Accuracy 0.0331\n",
      "Epoch 10 Batch 850 Loss 0.0388 Accuracy 0.0331\n",
      "Epoch 10 Batch 900 Loss 0.0389 Accuracy 0.0331\n",
      "Epoch 10 Batch 950 Loss 0.0390 Accuracy 0.0331\n",
      "Epoch 10 Batch 1000 Loss 0.0391 Accuracy 0.0331\n",
      "Epoch 10 Batch 1050 Loss 0.0391 Accuracy 0.0331\n",
      "Epoch 10 Batch 1100 Loss 0.0391 Accuracy 0.0331\n",
      "Epoch 10 Batch 1150 Loss 0.0392 Accuracy 0.0331\n",
      "Saving checkpoint for epoch 10 at 03-checkpoints/03-0402_01/ckpt-2\n",
      "Epoch 10 Loss 0.0392 Accuracy 0.0331\n",
      "Time taken for 1 epoch: 1220.3642020225525 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0370 Accuracy 0.0335\n",
      "Epoch 11 Batch 50 Loss 0.0361 Accuracy 0.0349\n",
      "Epoch 11 Batch 100 Loss 0.0357 Accuracy 0.0340\n",
      "Epoch 11 Batch 150 Loss 0.0353 Accuracy 0.0339\n",
      "Epoch 11 Batch 200 Loss 0.0354 Accuracy 0.0339\n",
      "Epoch 11 Batch 250 Loss 0.0353 Accuracy 0.0339\n",
      "Epoch 11 Batch 300 Loss 0.0354 Accuracy 0.0339\n",
      "Epoch 11 Batch 350 Loss 0.0356 Accuracy 0.0337\n",
      "Epoch 11 Batch 400 Loss 0.0357 Accuracy 0.0338\n",
      "Epoch 11 Batch 450 Loss 0.0359 Accuracy 0.0338\n",
      "Epoch 11 Batch 500 Loss 0.0360 Accuracy 0.0338\n",
      "Epoch 11 Batch 550 Loss 0.0362 Accuracy 0.0338\n",
      "Epoch 11 Batch 600 Loss 0.0362 Accuracy 0.0338\n",
      "Epoch 11 Batch 650 Loss 0.0363 Accuracy 0.0337\n",
      "Epoch 11 Batch 700 Loss 0.0364 Accuracy 0.0337\n",
      "Epoch 11 Batch 750 Loss 0.0365 Accuracy 0.0336\n",
      "Epoch 11 Batch 800 Loss 0.0365 Accuracy 0.0336\n",
      "Epoch 11 Batch 850 Loss 0.0366 Accuracy 0.0336\n",
      "Epoch 11 Batch 900 Loss 0.0366 Accuracy 0.0336\n",
      "Epoch 11 Batch 950 Loss 0.0367 Accuracy 0.0335\n",
      "Epoch 11 Batch 1000 Loss 0.0368 Accuracy 0.0335\n",
      "Epoch 11 Batch 1050 Loss 0.0369 Accuracy 0.0335\n",
      "Epoch 11 Batch 1100 Loss 0.0369 Accuracy 0.0335\n",
      "Epoch 11 Batch 1150 Loss 0.0370 Accuracy 0.0335\n",
      "Epoch 11 Loss 0.0370 Accuracy 0.0335\n",
      "Time taken for 1 epoch: 1217.327724456787 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0262 Accuracy 0.0336\n",
      "Epoch 12 Batch 50 Loss 0.0334 Accuracy 0.0340\n",
      "Epoch 12 Batch 100 Loss 0.0330 Accuracy 0.0338\n",
      "Epoch 12 Batch 150 Loss 0.0331 Accuracy 0.0338\n",
      "Epoch 12 Batch 200 Loss 0.0335 Accuracy 0.0339\n",
      "Epoch 12 Batch 250 Loss 0.0336 Accuracy 0.0338\n",
      "Epoch 12 Batch 300 Loss 0.0337 Accuracy 0.0338\n",
      "Epoch 12 Batch 350 Loss 0.0337 Accuracy 0.0337\n",
      "Epoch 12 Batch 400 Loss 0.0340 Accuracy 0.0338\n",
      "Epoch 12 Batch 450 Loss 0.0341 Accuracy 0.0339\n",
      "Epoch 12 Batch 500 Loss 0.0342 Accuracy 0.0339\n",
      "Epoch 12 Batch 550 Loss 0.0343 Accuracy 0.0339\n",
      "Epoch 12 Batch 600 Loss 0.0344 Accuracy 0.0339\n",
      "Epoch 12 Batch 650 Loss 0.0346 Accuracy 0.0339\n",
      "Epoch 12 Batch 700 Loss 0.0347 Accuracy 0.0339\n",
      "Epoch 12 Batch 750 Loss 0.0348 Accuracy 0.0339\n",
      "Epoch 12 Batch 800 Loss 0.0348 Accuracy 0.0339\n",
      "Epoch 12 Batch 850 Loss 0.0349 Accuracy 0.0339\n",
      "Epoch 12 Batch 900 Loss 0.0349 Accuracy 0.0339\n",
      "Epoch 12 Batch 950 Loss 0.0350 Accuracy 0.0339\n",
      "Epoch 12 Batch 1000 Loss 0.0350 Accuracy 0.0339\n",
      "Epoch 12 Batch 1050 Loss 0.0351 Accuracy 0.0339\n",
      "Epoch 12 Batch 1100 Loss 0.0352 Accuracy 0.0338\n",
      "Epoch 12 Batch 1150 Loss 0.0353 Accuracy 0.0339\n",
      "Epoch 12 Loss 0.0353 Accuracy 0.0339\n",
      "Time taken for 1 epoch: 1216.59854722023 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0331 Accuracy 0.0316\n",
      "Epoch 13 Batch 50 Loss 0.0319 Accuracy 0.0342\n",
      "Epoch 13 Batch 100 Loss 0.0318 Accuracy 0.0341\n",
      "Epoch 13 Batch 150 Loss 0.0316 Accuracy 0.0342\n",
      "Epoch 13 Batch 200 Loss 0.0318 Accuracy 0.0344\n",
      "Epoch 13 Batch 250 Loss 0.0321 Accuracy 0.0343\n",
      "Epoch 13 Batch 300 Loss 0.0322 Accuracy 0.0343\n",
      "Epoch 13 Batch 350 Loss 0.0325 Accuracy 0.0343\n",
      "Epoch 13 Batch 400 Loss 0.0326 Accuracy 0.0343\n",
      "Epoch 13 Batch 450 Loss 0.0327 Accuracy 0.0343\n",
      "Epoch 13 Batch 500 Loss 0.0329 Accuracy 0.0343\n",
      "Epoch 13 Batch 550 Loss 0.0329 Accuracy 0.0343\n",
      "Epoch 13 Batch 600 Loss 0.0330 Accuracy 0.0344\n",
      "Epoch 13 Batch 650 Loss 0.0333 Accuracy 0.0344\n",
      "Epoch 13 Batch 700 Loss 0.0333 Accuracy 0.0343\n",
      "Epoch 13 Batch 750 Loss 0.0333 Accuracy 0.0343\n",
      "Epoch 13 Batch 800 Loss 0.0334 Accuracy 0.0343\n",
      "Epoch 13 Batch 850 Loss 0.0334 Accuracy 0.0342\n",
      "Epoch 13 Batch 900 Loss 0.0334 Accuracy 0.0342\n",
      "Epoch 13 Batch 950 Loss 0.0335 Accuracy 0.0342\n",
      "Epoch 13 Batch 1000 Loss 0.0335 Accuracy 0.0342\n",
      "Epoch 13 Batch 1050 Loss 0.0336 Accuracy 0.0342\n",
      "Epoch 13 Batch 1100 Loss 0.0337 Accuracy 0.0342\n",
      "Epoch 13 Batch 1150 Loss 0.0338 Accuracy 0.0342\n",
      "Epoch 13 Loss 0.0338 Accuracy 0.0342\n",
      "Time taken for 1 epoch: 1216.5844781398773 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0307 Accuracy 0.0328\n",
      "Epoch 14 Batch 50 Loss 0.0295 Accuracy 0.0346\n",
      "Epoch 14 Batch 100 Loss 0.0300 Accuracy 0.0350\n",
      "Epoch 14 Batch 150 Loss 0.0300 Accuracy 0.0349\n",
      "Epoch 14 Batch 200 Loss 0.0303 Accuracy 0.0346\n",
      "Epoch 14 Batch 250 Loss 0.0303 Accuracy 0.0346\n",
      "Epoch 14 Batch 300 Loss 0.0304 Accuracy 0.0346\n",
      "Epoch 14 Batch 350 Loss 0.0307 Accuracy 0.0346\n",
      "Epoch 14 Batch 400 Loss 0.0307 Accuracy 0.0347\n",
      "Epoch 14 Batch 450 Loss 0.0309 Accuracy 0.0347\n",
      "Epoch 14 Batch 500 Loss 0.0311 Accuracy 0.0347\n",
      "Epoch 14 Batch 550 Loss 0.0312 Accuracy 0.0346\n",
      "Epoch 14 Batch 600 Loss 0.0313 Accuracy 0.0346\n",
      "Epoch 14 Batch 650 Loss 0.0314 Accuracy 0.0346\n",
      "Epoch 14 Batch 700 Loss 0.0315 Accuracy 0.0346\n",
      "Epoch 14 Batch 750 Loss 0.0317 Accuracy 0.0346\n",
      "Epoch 14 Batch 800 Loss 0.0318 Accuracy 0.0346\n",
      "Epoch 14 Batch 850 Loss 0.0319 Accuracy 0.0345\n",
      "Epoch 14 Batch 900 Loss 0.0320 Accuracy 0.0345\n",
      "Epoch 14 Batch 950 Loss 0.0321 Accuracy 0.0345\n",
      "Epoch 14 Batch 1000 Loss 0.0322 Accuracy 0.0345\n",
      "Epoch 14 Batch 1050 Loss 0.0323 Accuracy 0.0345\n",
      "Epoch 14 Batch 1100 Loss 0.0323 Accuracy 0.0345\n",
      "Epoch 14 Batch 1150 Loss 0.0324 Accuracy 0.0345\n",
      "Epoch 14 Loss 0.0325 Accuracy 0.0345\n",
      "Time taken for 1 epoch: 1216.3677752017975 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0284 Accuracy 0.0355\n",
      "Epoch 15 Batch 50 Loss 0.0280 Accuracy 0.0342\n",
      "Epoch 15 Batch 100 Loss 0.0286 Accuracy 0.0347\n",
      "Epoch 15 Batch 150 Loss 0.0290 Accuracy 0.0347\n",
      "Epoch 15 Batch 200 Loss 0.0293 Accuracy 0.0348\n",
      "Epoch 15 Batch 250 Loss 0.0294 Accuracy 0.0348\n",
      "Epoch 15 Batch 300 Loss 0.0295 Accuracy 0.0347\n",
      "Epoch 15 Batch 350 Loss 0.0297 Accuracy 0.0348\n",
      "Epoch 15 Batch 400 Loss 0.0298 Accuracy 0.0348\n",
      "Epoch 15 Batch 450 Loss 0.0299 Accuracy 0.0348\n",
      "Epoch 15 Batch 500 Loss 0.0300 Accuracy 0.0348\n",
      "Epoch 15 Batch 550 Loss 0.0301 Accuracy 0.0348\n",
      "Epoch 15 Batch 600 Loss 0.0301 Accuracy 0.0348\n",
      "Epoch 15 Batch 650 Loss 0.0302 Accuracy 0.0348\n",
      "Epoch 15 Batch 700 Loss 0.0303 Accuracy 0.0348\n",
      "Epoch 15 Batch 750 Loss 0.0304 Accuracy 0.0347\n",
      "Epoch 15 Batch 800 Loss 0.0305 Accuracy 0.0348\n",
      "Epoch 15 Batch 850 Loss 0.0306 Accuracy 0.0347\n",
      "Epoch 15 Batch 900 Loss 0.0307 Accuracy 0.0347\n",
      "Epoch 15 Batch 950 Loss 0.0308 Accuracy 0.0347\n",
      "Epoch 15 Batch 1000 Loss 0.0310 Accuracy 0.0347\n",
      "Epoch 15 Batch 1050 Loss 0.0310 Accuracy 0.0347\n",
      "Epoch 15 Batch 1100 Loss 0.0311 Accuracy 0.0347\n",
      "Epoch 15 Batch 1150 Loss 0.0311 Accuracy 0.0347\n",
      "Saving checkpoint for epoch 15 at 03-checkpoints/03-0402_01/ckpt-3\n",
      "Epoch 15 Loss 0.0312 Accuracy 0.0347\n",
      "Time taken for 1 epoch: 1221.6760573387146 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0262 Accuracy 0.0393\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-ae3275e5b854>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# inp -> portuguese, tar -> english\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # inp -> portuguese, tar -> english\n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估（Evaluate）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eval_sentence(lines): \n",
    "    res = \"\"\n",
    "    for line in lines:\n",
    "        line = re.split('([: ,.(){}\\[\\]=])',line)        \n",
    "        line = list(filter(lambda x: x!=' 'and x!='',line))\n",
    "\n",
    "        res += '<start> ' + ' '.join(line) + ' <end> '\n",
    "    return res.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence,max_len):\n",
    "    inp_sentence = preprocess_eval_sentence(inp_sentence)\n",
    "    inp_sentence = [inp_lang.word_index.get(word.lower(),inp_lang.word_index['<unk>']) for word in inp_sentence.split(' ')]\n",
    "#     inp_sentence = tf.keras.preprocessing.sequence.pad_sequences([inp_sentence],\n",
    "#                                                            maxlen=max_length_inp,\n",
    "#                                                            padding='post')\n",
    "    print(inp_sentence)\n",
    "    inp_sentence = tf.convert_to_tensor(inp_sentence)\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    decoder_input = [targ_lang.word_index['<start>']]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "\n",
    "        # 从 seq_len 维度选择最后一个词\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "        \n",
    "#         print(\"predicted_id: \")\n",
    "#         print(predicted_id)\n",
    "#         print(\"output: \")\n",
    "#         print(output)\n",
    "        \n",
    "        # 如果 predicted_id 等于结束标记，就返回结果\n",
    "        if predicted_id == targ_lang.word_index['<end>']:\n",
    "            return inp_sentence,tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        # 连接 predicted_id 与输出，作为解码器的输入传递到解码器。\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return inp_sentence,tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(encoder_input,attention, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    #sentence = tokenizer_pt.encode(sentence)\n",
    "\n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "\n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # 画出注意力权重\n",
    "        ax.matshow(attention[head][:, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(encoder_input)))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "\n",
    "        ax.set_ylim(len(result)-0.5, -0.5)\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            [inp_lang.index_word[id.numpy()] for id in encoder_input], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "\n",
    "        ax.set_yticklabels([targ_lang.index_word[id.numpy()] for id in result], \n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, max_len, plot=''):\n",
    "    encoder_input,result, attention_weights = evaluate(sentence,max_len)\n",
    "\n",
    "    predicted_sentence = \"\"\n",
    "    for id in result[1:]:\n",
    "        predicted_sentence += targ_lang.index_word[id.numpy()] + \" \" \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))\n",
    "\n",
    "    if plot:\n",
    "        plot_attention_weights(encoder_input,attention_weights, result, plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence,beam_nums,max_len):\n",
    "    \n",
    "    k = beam_nums = 4\n",
    "    \n",
    "    inp_sentence = preprocess_eval_sentence(inp_sentence)\n",
    "    inp_sentence = [inp_lang.word_index.get(word.lower(),inp_lang.word_index['<unk>']) for word in inp_sentence.split(' ')]\n",
    "\n",
    "    inp_sentence = tf.convert_to_tensor(inp_sentence)\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "    decoder_input = [targ_lang.word_index['<start>']]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    #开始预测\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input, output)\n",
    "\n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "\n",
    "    # 从 seq_len 维度选择最后一个词\n",
    "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "    pre_probs,pre_ids = tf.nn.top_k(predictions[0],k)\n",
    "    pre_probs = tf.nn.softmax(pre_probs)\n",
    "    \n",
    "    #将<start>加入到ids开头\n",
    "    ids = tf.fill((1,k),targ_lang.word_index['<start>'])\n",
    "    k_ids = tf.stack([ids,pre_ids],axis=2)\n",
    "    \n",
    "    #构造中间结果ids、probs\n",
    "    res_ids = tf.squeeze(k_ids).numpy().tolist()\n",
    "    res_probs = tf.squeeze(pre_probs).numpy().tolist()\n",
    "    \n",
    "    result_ids = []\n",
    "    result_probs = []\n",
    "    \n",
    "    for step in range(max_len):\n",
    "        \n",
    "        mid_k_probs = []\n",
    "        mid_k_ids = []\n",
    "        \n",
    "        for i in range(k):\n",
    "            pre_prob = res_probs[i]\n",
    "            pre_id = res_ids[i]\n",
    "    \n",
    "            #预测之前，需要将output抽取出来\n",
    "            output = tf.expand_dims(pre_id, 0)\n",
    "            predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "            predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "            \n",
    "            next_k_scores, next_k_ids = tf.nn.top_k(predictions[0],k)\n",
    "            next_k_scores = tf.nn.softmax(next_k_scores)\n",
    "            next_k_scores *= pre_prob\n",
    "            \n",
    "            #将新的预测结果加到中间结果里去\n",
    "            for j in range(k):\n",
    "                mid_k_ids.append(pre_id + [next_k_ids[0][j].numpy()])\n",
    "            \n",
    "            #将probs加入到中间数组中\n",
    "            mid_k_probs.extend(next_k_scores.numpy()[0].tolist())\n",
    "        \n",
    "        #一轮预测后，整理中间结果\n",
    "        mid_k_probs = {index:prob for index,prob in enumerate(mid_k_probs)}\n",
    "        mid_k_probs = sorted(mid_k_probs.items(), key=lambda x: x[1],reverse=True)\n",
    "        \n",
    "        res_ids = []\n",
    "        res_probs = []\n",
    "        for i in range(k):\n",
    "            res_id = mid_k_ids[mid_k_probs[i][0]]\n",
    "            res_prob = mid_k_probs[i][1]\n",
    "            \n",
    "            if res_id[-1] == targ_lang.word_index['<end>']:\n",
    "                result_ids.append(res_id)\n",
    "                result_probs.append(res_prob)\n",
    "                k -= 1\n",
    "            else:\n",
    "                res_ids.append(res_id)\n",
    "                res_probs.append(res_prob)\n",
    "        \n",
    "        if k < 1:\n",
    "            #返回结果\n",
    "#             print(\"result_ids: \")\n",
    "#             print(result_ids)\n",
    "#             print(\"result_probs: \")\n",
    "#             print(result_probs)\n",
    "            return result_ids,result_probs\n",
    "\n",
    "#     print(\"beam_result: \")\n",
    "#     print(beam_result)\n",
    "    return result_ids,result_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluate(sentence,beam_nums,max_len):\n",
    "    result_ids,result_probs = evaluate(sentence,beam_nums,max_len)\n",
    "    \n",
    "    predicted_sentences = []\n",
    "    for i in range(len(result_ids)):\n",
    "        res = \"\"\n",
    "        for id in result_ids[i][1:-1]:\n",
    "            res += targ_lang.index_word[id] + \" \" \n",
    "        predicted_sentences.append([res.strip(),result_probs[i]])\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "beam_nums = 4\n",
    "sentence = [\"\"]\n",
    "test_evaluate(sentence,beam_nums,max_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
