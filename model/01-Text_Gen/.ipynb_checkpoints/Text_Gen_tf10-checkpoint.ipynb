{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"数据导入\"\"\"\n",
    "\n",
    "import re\n",
    "data_file = \"../00-data/tf_data.txt\"\n",
    "filename =open(data_file,'r',encoding='utf-8')        #打开数据文件\n",
    "\n",
    "text = filename.read()        #将数据读取到字符串text中\n",
    "text = ' '.join(re.split(' |\\t|\\v',text))        #将数据中的空格符统一，便于后期处理(原始数据中空格符包含\\t、\\v等)   \n",
    "text = re.split('([: ,.*\\n(){}\\[\\]=])',text)        #将字符串数据按照括号中的符号进行分割，分割成列表格式，并且在列表中保留分隔符\n",
    "\n",
    "text = list(filter(lambda x: x!=' 'and x!='',text))        #将列表中的空格和非空格筛选掉\n",
    "list_text = text        #保留一份列表格式的数据\n",
    "text = ' '.join(text)        #将列表转换成字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"文本词频统计\"\"\"\n",
    "\n",
    "def word_count(list_text):        #定义计算文本词频的函数，传入list_text列表\n",
    "    import collections\n",
    "    word_freq = collections.defaultdict(int)        #定义一个int型的词频词典，并提供默认值\n",
    "    for w in list_text:        #遍历列表中的元素，元素出现一次，频次加一\n",
    "        word_freq[w] += 1\n",
    "    return word_freq        #返回词频词典\n",
    "    \n",
    "    #return word_freq.items()   该语句返回值的类型为list（这句话有语法问题，不必考虑）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"根据text文本创建代码词词典\"\"\"\n",
    "\n",
    "def build_dict(text, min_word_freq=50):\n",
    "    word_freq = word_count(text)         #文本词频统计，返回一个词频词典\n",
    "    word_freq = filter(lambda x: x[1] > min_word_freq, word_freq.items())          # filter将词频数量低于指定值的代码词删除。\n",
    "    word_freq_sorted = sorted(word_freq, key=lambda x: (-x[1], x[0]))         # key用于指定排序的元素，因为sorted默认使用list中每个item的第一个元素从小到大排列，所以这里通过lambda进行前后元素调序，并对词频去相反数，从而将词频最大的排列在最前面\n",
    "    words, _ = list(zip(*word_freq_sorted))         #获取每一个代码词\n",
    "    words = list(words)\n",
    "    words.append('<unk>')\n",
    "    word_idx = dict(zip(words, range(len(words))))         #构建词典（不包含词频）\n",
    "    return words,word_idx        #这里只返回了words，倒数两行代码还用不上。返回的是一个不含重复的代码词词典，不包含词频。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 79181\n",
      "Unique words: 7865\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"数据预处理-字符串序列向量化\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import json\n",
    "\n",
    "maxlen = 50         #提取50个代码词组成的序列\n",
    "step = 5         #每5个代码词采样一个新序列\n",
    "sentences = []         #保存所提取的序列\n",
    "next_words = []         #保存目标代码词\n",
    "vocab_file = \"../00-data/vocab\"\n",
    "\n",
    "cut_words = list_text         #将列表形式的元数据保存在cut_words中\n",
    "for i in range(0,len(cut_words) - maxlen,step):\n",
    "    sentences.append(cut_words[i:i + maxlen])         #将元数据按照步长来存储在每个序列中       \n",
    "    next_words.append(cut_words[i + maxlen])         #将目标代码词存储在next_words中\n",
    "    \n",
    "    \n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "\n",
    "words,word_idx = list(build_dict(list_text,1))         #创建代码词词典，返回的是一个不含重复的代码词词典，不包含词频。\n",
    "print('Unique words:',len(words))\n",
    "json_dict = json.dumps(word_idx)\n",
    "with open(vocab_file,\"w\") as f:\n",
    "    f.write(json_dict)\n",
    "\n",
    "word_indices = dict((word,words.index(word)) for word in words)         #创建一个包含代码词唯一索引的代码词词典，返回的是一个字典\n",
    "#print(word_indices)\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences),maxlen))         #初始化x\n",
    "y = np.zeros((len(sentences)))         #初始化y\n",
    "for i,sentence in enumerate(sentences):\n",
    "    for t,word in enumerate(sentence):\n",
    "        x[i,t] = word_indices.get(word,word_indices['<unk>'])         #将代码词转换成向量形式的编码\n",
    "    #y[i] = word_indices[next_words[i]]\n",
    "    y[i] = word_indices.get(next_words[i],word_indices['<unk>'])\n",
    "\n",
    "y = keras.utils.to_categorical(y, len(words))         #将int型数组y转换成one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"定义下一个代码词的采样函数---temperature越大，代码生成的随机性越强---\"\"\"\n",
    "\n",
    "def sample(preds,temperature=0.1):\n",
    "    preds = np.asarray(preds).astype('float')\n",
    "    preds = np.log(preds) /temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1,preds,1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"将字符串写到指定文件中\"\"\"\n",
    "\n",
    "def save(filename, contents): \n",
    "      file = open(filename, 'a', encoding='utf-8')\n",
    "      file.write(contents)\n",
    "      file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "def create_model(words,learning_rate):         #定义创建模型的函数\n",
    "    model = keras.models.Sequential()         #模型初始化\n",
    "    model.add(layers.Embedding(len(words),512))         #模型第一层为embedding层\n",
    "    model.add(layers.LSTM(512,return_sequences=True,dropout=0.2,recurrent_dropout=0.2))         #模型第二层为LSTM层，加入dropout减少过拟合\n",
    "    model.add(layers.LSTM(512,dropout=0.2,recurrent_dropout=0.2))         #模型第三层为LSTM层，加入dropout减少过拟合\n",
    "    model.add(layers.Dense(len(words),activation='softmax'))         #模型第三层为全连接层\n",
    "\n",
    "    optimizer = keras.optimizers.RMSprop(lr=learning_rate)         #定义优化器\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizer)         #模型编译\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 512)         4026880   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 512)         2099200   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7865)              4034745   \n",
      "=================================================================\n",
      "Total params: 12,260,025\n",
      "Trainable params: 12,260,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"创建模型实例\"\"\"\n",
    "learning_rate = 0.003\n",
    "model = create_model(words,learning_rate)         #创建模型\n",
    "model.summary()         #打印模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_split(line):\n",
    "    #将数据中的空格符统一，便于后期处理(原始数据中空格符包含\\t、\\v等)  \n",
    "    line = ' '.join(re.split(' |\\t|\\v',line))\n",
    "    #将字符串数据按照括号中的符号进行分割，分割成列表格式，并且在列表中保留分隔符\n",
    "    line = re.split('([: ,.*\\n(){}\\[\\]=])',line)        \n",
    "    line = list(filter(lambda x: x!=' 'and x!='',line))\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"打印生成的结果\"\"\"\n",
    "def print_code_text(code_list):\n",
    "    mark = \".,*()[]:{}\\n\"\n",
    "    \n",
    "    result = \"\"\n",
    "    last_word = \"\"\n",
    "    \n",
    "    for word in code_list:\n",
    "        if last_word not in mark and word not in mark:\n",
    "            result += \" \" + word\n",
    "        else:\n",
    "            result += word\n",
    "        \n",
    "        last_word = word\n",
    "        \n",
    "    print(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generate(model,input_file,maxlen,\n",
    "                  temperatures,save_path,epoch,gen_lines=30):\n",
    "    import random\n",
    "    import codecs\n",
    "    \n",
    "    #从原始数据中随机找一行作为文本生成的起点\n",
    "    with codecs.open(input_file,\"r\",\"utf-8\") as fin:\n",
    "        lines = fin.readlines()\n",
    "        random_line = random.randint(0,len(lines))\n",
    "        start_line = clean_and_split(lines[random_line])\n",
    "        \n",
    "    one_line_max_words = 30\n",
    "    \n",
    "    print(\"===========epoch:%d===========\" % epoch)\n",
    "    \n",
    "    for temperature in temperatures:\n",
    "        generated_text = start_line[:]\n",
    "        print_string = start_line[:]\n",
    "\n",
    "        for i in range(gen_lines):\n",
    "            for j in range(one_line_max_words):\n",
    "                sampled = np.zeros((1,len(generated_text))) \n",
    "                #向量化\n",
    "                for t,word in enumerate(generated_text): \n",
    "                    sampled[0,t] = word_indices.get(word,word_indices['<unk>'])\n",
    "                #预测下一个词\n",
    "                preds = model.predict(sampled,verbose=0)[0]\n",
    "                next_index = sample(preds,temperature)\n",
    "                next_word = words[next_index]\n",
    "\n",
    "                if len(generated_text) == maxlen:\n",
    "                    generated_text = generated_text[1:]\n",
    "                generated_text.append(next_word)\n",
    "                print_string.append(next_word)\n",
    "                if next_word == '\\n':\n",
    "                    break\n",
    "\n",
    "        print(\"-----temperature: {}-----\".format(temperature))\n",
    "        result = print_code_text(print_string)\n",
    "\n",
    "        save_file = save_path + \"/{}_epoch_{}_temperature\".format(epoch,temperature)\n",
    "        with codecs.open(save_file,\"w\",\"utf-8\") as fout:\n",
    "            fout.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"模型保存\"\"\"\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath = \"../02-checkpoints/\"\n",
    "checkpoint = ModelCheckpoint(filepath, save_weights_only=False,verbose=1,save_best_only=False)         #回调函数，实现断点续训功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"学习率随模型效果变小\"\"\"\n",
    "import keras.backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    " \n",
    "def scheduler(epoch):\n",
    "    # 每隔10个epoch，学习率减小为原来的5/10\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.5)\n",
    "        print(\"lr changed to {}\".format(lr * 0.5))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    " \n",
    "reduce_lr = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      " 4096/79181 [>.............................] - ETA: 2:37 - loss: 0.6361"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-709ad713d973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0minit_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m#开始训练模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtext_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprint_save_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"epoch_{}.hdf5\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"训练模型\"\"\"\n",
    "import os\n",
    "print_save_path = \"../00-data/\"\n",
    "model_save_path = \"../02-checkpoints/\"\n",
    "total_epochs = 50  \n",
    "for epoch in range(1,total_epochs):\n",
    "    if os.path.exists(filepath):        #如果模型存在，则从现有模型开始训练\n",
    "        model.load_weights(filepath)\n",
    "    init_epoch = (epoch - 1) * 5\n",
    "    model.fit(x,y,batch_size=1024,epochs=(init_epoch + 5),initial_epoch=init_epoch,callbacks=[reduce_lr,checkpoint])        #开始训练模型\n",
    "    text_generate(model,data_file,maxlen,[0.1,0.4,0.8],print_save_path,epoch * 5)\n",
    "    model.save(model_save_path + \"epoch_{}.hdf5\".format(epoch * 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_sentence(seed_text,model_filename):        #测试代码和上面训练模型的代码基本一样，就不再介绍\n",
    "    model.load_weights(model_filename)\n",
    "    \n",
    "    strings=''\n",
    "    last_word=''\n",
    "    seed_text = re.split('([: ,.\\n(){}\\[\\]=])',seed_text)\n",
    "    seed_text = list(filter(lambda x: x!=' 'and x!='',seed_text))\n",
    "    \n",
    "    generated_text = seed_text[:]\n",
    "    \n",
    "    for temperature in [0.1,0.4,0.8]:\n",
    "        strings += '\\n' + '-------------temperature:' + str(temperature) +'-------------\\n' +'\\n'\n",
    "        \n",
    "        for i in range(50):\n",
    "            if i == 0:\n",
    "                for k in range(len(generated_text)):\n",
    "                    if generated_text[k] not in mark and last_word not in mark:\n",
    "                        strings += ' ' + generated_text[k]\n",
    "                    else:\n",
    "                        strings += generated_text[k]\n",
    "                    last_word = generated_text[k]\n",
    "\n",
    "            sampled = np.zeros((1,len(generated_text)))\n",
    "            for t,word in enumerate(generated_text):\n",
    "                sampled[0,t] = word_indices[word]\n",
    "\n",
    "            preds = model.predict(sampled,verbose=0)[0]\n",
    "            next_index = sample(preds,temperature = 0.3)\n",
    "            next_word = words[next_index]\n",
    "\n",
    "\n",
    "            generated_text.append(next_word)\n",
    "\n",
    "            #if len(generated_text) == maxlen:\n",
    "            #    generated_text = generated_text[1:]\n",
    "\n",
    "            if next_word not in mark and last_word not in mark:\n",
    "                strings += ' ' + next_word\n",
    "            else:\n",
    "                strings +=  next_word\n",
    "\n",
    "            last_word = next_word\n",
    "\n",
    "            if next_word == '\\n':\n",
    "                break\n",
    "        \n",
    "        generated_text = seed_text[:]\n",
    "        \n",
    "    return strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
